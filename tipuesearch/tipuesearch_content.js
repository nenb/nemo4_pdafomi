var tipuesearch = {"pages":[{"title":" NEMO4 PDAFOMI ","text":"NEMO4 PDAFOMI Developer documentation Generating this documentation Developer documentation Insert documentation of the PDAFOMI attachments to the NEMO4 ocean model here. Generating this documentation This documentation is generated by FORD .\nFor more details on the project file and the comment markup in the source code visit the FORD documentation . To regenerate this documentation run: ford docs.md Developer Info Nick Byrne","tags":"home","loc":"index.html"},{"title":"mod_parallel_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Modules mod_parallel_pdaf Source Code mod_parallel_pdaf.F90 Source Code MODULE mod_parallel_pdaf !># Setup parallelisation !>This modules provides variables for the MPI parallelization !>to be shared between model-related routines. There are variables !>that are used in the model even without PDAF, and additional variables !>that are only used if data assimilaion with PDAF is performed. !>The initialization of communicators for execution with PDAF is !>performed in `init_parallel_pdaf`. USE mod_kind_pdaf IMPLICIT NONE SAVE INCLUDE 'mpif.h' !> MPI communicator for model tasks INTEGER :: COMM_model !> Rank and size in COMM_model INTEGER :: mype_model , npes_model !> Communicator for entire ensemble INTEGER :: COMM_ensemble !> Rank and size in COMM_ensemble INTEGER :: mype_ens , npes_ens !> Number of parallel model tasks INTEGER :: n_modeltasks = 1 !> MPI communicator for filter PEs INTEGER :: COMM_filter !> rank and size in COMM_filter INTEGER :: mype_filter , npes_filter !> MPI communicator for coupling filter and model INTEGER :: COMM_couple !> rank and size in COMM_couple INTEGER :: mype_couple , npes_couple !> Whether we are on a PE in a COMM_filter LOGICAL :: filterpe !> Index of my model task (1,...,n_modeltasks) INTEGER :: task_id !> Error flag for MPI INTEGER :: MPIerr !> Status array for MPI INTEGER :: MPIstatus ( MPI_STATUS_SIZE ) !> Option for screen output INTEGER :: screen_parallel = 1 !> # PEs per ensemble INTEGER , ALLOCATABLE :: local_npes_model (:) CONTAINS !> Terminate the MPI execution environment. SUBROUTINE abort_parallel () CALL MPI_Abort ( MPI_COMM_WORLD , 1 , MPIerr ) END SUBROUTINE abort_parallel !> Split the MPI communicator initialised by XIOS into MODEL, !> FILTER and COUPLE communicators, return MODEL communicator. !> !> **Calling Sequence** !> *Called from:* `lib_mpp.F90` !> *Calls:* `MPI_Comm_size` !> *Calls:* `MPI_Comm_rank` !> *Calls:* `MPI_Comm_split` !> *Calls:* `MPI_Barrier` !> SUBROUTINE init_parallel_pdaf ( screen , mpi_comm ) !> Whether screen information is shown INTEGER , INTENT ( in ) :: screen !> Communicator after XIOS splitting INTEGER , INTENT ( inout ) :: mpi_comm !> Counters INTEGER :: i , j !> Index of PE INTEGER :: pe_index !> Variables for communicator-splitting INTEGER :: my_color , color_couple !> Number of model tasks INTEGER :: tasks !> namelist file CHARACTER ( lc ) :: nmlfile ! Number of ensemble members, supplied by PDAF namelist NAMELIST / tasks_nml / tasks ! Read namelist for number of model tasks nmlfile = 'namelist.pdaf' OPEN ( 20 , file = nmlfile ) READ ( 20 , NML = tasks_nml ) CLOSE ( 20 ) n_modeltasks = tasks ! ***              COMM_ENSEMBLE                *** ! *** Generate communicator for ensemble runs   *** ! *** only used to generate model communicators *** COMM_ensemble = mpi_comm CALL MPI_Comm_Size ( COMM_ensemble , npes_ens , MPIerr ) CALL MPI_Comm_Rank ( COMM_ensemble , mype_ens , MPIerr ) ! Initialize communicators for ensemble evaluations IF ( mype_ens == 0 ) THEN WRITE ( * , '(/1x, a)' ) 'Initialize communicators for assimilation with PDAF' END IF ! Store # PEs per ensemble member. Used for info on PE 0 and for ! generation of model communicators on other PEs ALLOCATE ( local_npes_model ( n_modeltasks )) local_npes_model = FLOOR ( REAL ( npes_ens ) / REAL ( n_modeltasks )) DO i = 1 , ( npes_ens - n_modeltasks * local_npes_model ( 1 )) local_npes_model ( i ) = local_npes_model ( i ) + 1 END DO ! ***              COMM_MODEL               *** ! *** Generate communicators for model runs *** pe_index = 0 doens1 : DO i = 1 , n_modeltasks DO j = 1 , local_npes_model ( i ) IF ( mype_ens == pe_index ) THEN task_id = i EXIT doens1 END IF pe_index = pe_index + 1 END DO END DO doens1 CALL MPI_Comm_split ( COMM_ensemble , task_id , mype_ens , & COMM_model , MPIerr ) ! Re-initialize PE information according to model communicator CALL MPI_Comm_Size ( COMM_model , npes_model , MPIerr ) CALL MPI_Comm_Rank ( COMM_model , mype_model , MPIerr ) IF ( screen > 1 ) then WRITE ( * , * ) 'MODEL: mype(w)= ' , mype_ens , '; model task: ' , task_id , & '; mype(m)= ' , mype_model , '; npes(m)= ' , npes_model END IF ! Init flag FILTERPE (all PEs of model task 1) IF ( task_id == 1 ) THEN filterpe = . TRUE . ELSE filterpe = . FALSE . END IF ! ***         COMM_FILTER                 *** ! *** Generate communicator for filter    *** IF ( filterpe ) THEN my_color = task_id ELSE my_color = MPI_UNDEFINED END IF CALL MPI_Comm_split ( COMM_ensemble , my_color , mype_ens , & COMM_filter , MPIerr ) ! Initialize PE information according to filter communicator IF ( filterpe ) THEN CALL MPI_Comm_Size ( COMM_filter , npes_filter , MPIerr ) CALL MPI_Comm_Rank ( COMM_filter , mype_filter , MPIerr ) END IF ! ***              COMM_COUPLE                 *** ! *** Generate communicators for communication *** ! *** between model and filter PEs             *** color_couple = mype_model + 1 CALL MPI_Comm_split ( COMM_ensemble , color_couple , mype_ens , & COMM_couple , MPIerr ) ! Initialize PE information according to coupling communicator CALL MPI_Comm_Size ( COMM_couple , npes_couple , MPIerr ) CALL MPI_Comm_Rank ( COMM_couple , mype_couple , MPIerr ) IF ( screen > 0 ) THEN IF ( mype_ens == 0 ) THEN WRITE ( * , '(/18x, a)' ) 'PE configuration:' WRITE ( * , '(2x, a6, a9, a10, a14, a13, /2x, a5, a9, a7, a7, a7, a7, a7, /2x, a)' ) & 'world' , 'filter' , 'model' , 'couple' , 'filterPE' , & 'rank' , 'rank' , 'task' , 'rank' , 'task' , 'rank' , 'T/F' , & '----------------------------------------------------------' END IF CALL MPI_Barrier ( COMM_ensemble , MPIerr ) IF ( task_id == 1 ) THEN WRITE ( * , '(2x, i4, 4x, i4, 4x, i3, 4x, i3, 4x, i3, 4x, i3, 5x, l3)' ) & mype_ens , mype_filter , task_id , mype_model , color_couple , & mype_couple , filterpe END IF IF ( task_id > 1 ) THEN WRITE ( * , '(2x, i4, 12x, i3, 4x, i3, 4x, i3, 4x, i3, 5x, l3)' ) & mype_ens , task_id , mype_model , color_couple , mype_couple , filterpe END IF CALL MPI_Barrier ( COMM_ensemble , MPIerr ) IF ( mype_ens == 0 ) WRITE ( * , '(/a)' ) '' END IF ! **************************************************** ! *** Re-initialize model equivalent to COMM_model *** ! **************************************************** mpi_comm = COMM_model END SUBROUTINE init_parallel_pdaf END MODULE mod_parallel_pdaf","tags":"","loc":"sourcefile/mod_parallel_pdaf.f90.html"},{"title":"mod_kind_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Modules mod_kind_pdaf Source Code mod_kind_pdaf.F90 Source Code MODULE mod_kind_pdaf !>## Define real precision !> !>This module defines the kind of real and length of character strings !>for the PDAF call-back routines and interfaces. It is based on the NEMO !>module `par_kind.F90`. IMPLICIT NONE SAVE INTEGER , PARAMETER :: pdp = SELECTED_REAL_KIND ( 12 , 307 ) INTEGER , PARAMETER :: pwp = pdp INTEGER , PARAMETER :: lc = 256 END MODULE mod_kind_pdaf","tags":"","loc":"sourcefile/mod_kind_pdaf.f90.html"},{"title":"abort_parallel – NEMO4 PDAFOMI","text":"public subroutine abort_parallel() Terminate the MPI execution environment. Arguments None Contents Source Code abort_parallel Source Code SUBROUTINE abort_parallel () CALL MPI_Abort ( MPI_COMM_WORLD , 1 , MPIerr ) END SUBROUTINE abort_parallel","tags":"","loc":"proc/abort_parallel.html"},{"title":"init_parallel_pdaf – NEMO4 PDAFOMI","text":"public subroutine init_parallel_pdaf(screen, mpi_comm) Split the MPI communicator initialised by XIOS into MODEL,\n FILTER and COUPLE communicators, return MODEL communicator. Calling Sequence Called from: lib_mpp.F90 Calls: MPI_Comm_size Calls: MPI_Comm_rank Calls: MPI_Comm_split Calls: MPI_Barrier Arguments Type Intent Optional Attributes Name integer, intent(in) :: screen Whether screen information is shown integer, intent(inout) :: mpi_comm Communicator after XIOS splitting Contents Variables color_couple i j my_color nmlfile pe_index tasks Source Code init_parallel_pdaf Variables Type Visibility Attributes Name Initial integer, public :: color_couple Variables for communicator-splitting integer, public :: i Counters integer, public :: j Counters integer, public :: my_color Variables for communicator-splitting character(len=lc), public :: nmlfile namelist file integer, public :: pe_index Index of PE integer, public :: tasks Number of model tasks Source Code SUBROUTINE init_parallel_pdaf ( screen , mpi_comm ) !> Whether screen information is shown INTEGER , INTENT ( in ) :: screen !> Communicator after XIOS splitting INTEGER , INTENT ( inout ) :: mpi_comm !> Counters INTEGER :: i , j !> Index of PE INTEGER :: pe_index !> Variables for communicator-splitting INTEGER :: my_color , color_couple !> Number of model tasks INTEGER :: tasks !> namelist file CHARACTER ( lc ) :: nmlfile ! Number of ensemble members, supplied by PDAF namelist NAMELIST / tasks_nml / tasks ! Read namelist for number of model tasks nmlfile = 'namelist.pdaf' OPEN ( 20 , file = nmlfile ) READ ( 20 , NML = tasks_nml ) CLOSE ( 20 ) n_modeltasks = tasks ! ***              COMM_ENSEMBLE                *** ! *** Generate communicator for ensemble runs   *** ! *** only used to generate model communicators *** COMM_ensemble = mpi_comm CALL MPI_Comm_Size ( COMM_ensemble , npes_ens , MPIerr ) CALL MPI_Comm_Rank ( COMM_ensemble , mype_ens , MPIerr ) ! Initialize communicators for ensemble evaluations IF ( mype_ens == 0 ) THEN WRITE ( * , '(/1x, a)' ) 'Initialize communicators for assimilation with PDAF' END IF ! Store # PEs per ensemble member. Used for info on PE 0 and for ! generation of model communicators on other PEs ALLOCATE ( local_npes_model ( n_modeltasks )) local_npes_model = FLOOR ( REAL ( npes_ens ) / REAL ( n_modeltasks )) DO i = 1 , ( npes_ens - n_modeltasks * local_npes_model ( 1 )) local_npes_model ( i ) = local_npes_model ( i ) + 1 END DO ! ***              COMM_MODEL               *** ! *** Generate communicators for model runs *** pe_index = 0 doens1 : DO i = 1 , n_modeltasks DO j = 1 , local_npes_model ( i ) IF ( mype_ens == pe_index ) THEN task_id = i EXIT doens1 END IF pe_index = pe_index + 1 END DO END DO doens1 CALL MPI_Comm_split ( COMM_ensemble , task_id , mype_ens , & COMM_model , MPIerr ) ! Re-initialize PE information according to model communicator CALL MPI_Comm_Size ( COMM_model , npes_model , MPIerr ) CALL MPI_Comm_Rank ( COMM_model , mype_model , MPIerr ) IF ( screen > 1 ) then WRITE ( * , * ) 'MODEL: mype(w)= ' , mype_ens , '; model task: ' , task_id , & '; mype(m)= ' , mype_model , '; npes(m)= ' , npes_model END IF ! Init flag FILTERPE (all PEs of model task 1) IF ( task_id == 1 ) THEN filterpe = . TRUE . ELSE filterpe = . FALSE . END IF ! ***         COMM_FILTER                 *** ! *** Generate communicator for filter    *** IF ( filterpe ) THEN my_color = task_id ELSE my_color = MPI_UNDEFINED END IF CALL MPI_Comm_split ( COMM_ensemble , my_color , mype_ens , & COMM_filter , MPIerr ) ! Initialize PE information according to filter communicator IF ( filterpe ) THEN CALL MPI_Comm_Size ( COMM_filter , npes_filter , MPIerr ) CALL MPI_Comm_Rank ( COMM_filter , mype_filter , MPIerr ) END IF ! ***              COMM_COUPLE                 *** ! *** Generate communicators for communication *** ! *** between model and filter PEs             *** color_couple = mype_model + 1 CALL MPI_Comm_split ( COMM_ensemble , color_couple , mype_ens , & COMM_couple , MPIerr ) ! Initialize PE information according to coupling communicator CALL MPI_Comm_Size ( COMM_couple , npes_couple , MPIerr ) CALL MPI_Comm_Rank ( COMM_couple , mype_couple , MPIerr ) IF ( screen > 0 ) THEN IF ( mype_ens == 0 ) THEN WRITE ( * , '(/18x, a)' ) 'PE configuration:' WRITE ( * , '(2x, a6, a9, a10, a14, a13, /2x, a5, a9, a7, a7, a7, a7, a7, /2x, a)' ) & 'world' , 'filter' , 'model' , 'couple' , 'filterPE' , & 'rank' , 'rank' , 'task' , 'rank' , 'task' , 'rank' , 'T/F' , & '----------------------------------------------------------' END IF CALL MPI_Barrier ( COMM_ensemble , MPIerr ) IF ( task_id == 1 ) THEN WRITE ( * , '(2x, i4, 4x, i4, 4x, i3, 4x, i3, 4x, i3, 4x, i3, 5x, l3)' ) & mype_ens , mype_filter , task_id , mype_model , color_couple , & mype_couple , filterpe END IF IF ( task_id > 1 ) THEN WRITE ( * , '(2x, i4, 12x, i3, 4x, i3, 4x, i3, 4x, i3, 5x, l3)' ) & mype_ens , task_id , mype_model , color_couple , mype_couple , filterpe END IF CALL MPI_Barrier ( COMM_ensemble , MPIerr ) IF ( mype_ens == 0 ) WRITE ( * , '(/a)' ) '' END IF ! **************************************************** ! *** Re-initialize model equivalent to COMM_model *** ! **************************************************** mpi_comm = COMM_model END SUBROUTINE init_parallel_pdaf","tags":"","loc":"proc/init_parallel_pdaf.html"},{"title":"mod_parallel_pdaf – NEMO4 PDAFOMI","text":"Setup parallelisation This modules provides variables for the MPI parallelization\nto be shared between model-related routines. There are variables\nthat are used in the model even without PDAF, and additional variables\nthat are only used if data assimilaion with PDAF is performed.\nThe initialization of communicators for execution with PDAF is\nperformed in init_parallel_pdaf . Uses mod_kind_pdaf Contents Variables COMM_couple COMM_ensemble COMM_filter COMM_model MPIerr MPIstatus filterpe local_npes_model mype_couple mype_ens mype_filter mype_model n_modeltasks npes_couple npes_ens npes_filter npes_model screen_parallel task_id Subroutines abort_parallel init_parallel_pdaf Variables Type Visibility Attributes Name Initial integer, public :: COMM_couple MPI communicator for coupling filter and model integer, public :: COMM_ensemble Communicator for entire ensemble integer, public :: COMM_filter MPI communicator for filter PEs integer, public :: COMM_model MPI communicator for model tasks integer, public :: MPIerr Error flag for MPI integer, public :: MPIstatus (MPI_STATUS_SIZE) Status array for MPI logical, public :: filterpe Whether we are on a PE in a COMM_filter integer, public, ALLOCATABLE :: local_npes_model (:) # PEs per ensemble integer, public :: mype_couple rank and size in COMM_couple integer, public :: mype_ens Rank and size in COMM_ensemble integer, public :: mype_filter rank and size in COMM_filter integer, public :: mype_model Rank and size in COMM_model integer, public :: n_modeltasks = 1 Number of parallel model tasks integer, public :: npes_couple rank and size in COMM_couple integer, public :: npes_ens Rank and size in COMM_ensemble integer, public :: npes_filter rank and size in COMM_filter integer, public :: npes_model Rank and size in COMM_model integer, public :: screen_parallel = 1 Option for screen output integer, public :: task_id Index of my model task (1,…,n_modeltasks) Subroutines public subroutine abort_parallel () Terminate the MPI execution environment. Arguments None public subroutine init_parallel_pdaf (screen, mpi_comm) Split the MPI communicator initialised by XIOS into MODEL,\n FILTER and COUPLE communicators, return MODEL communicator. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: screen Whether screen information is shown integer, intent(inout) :: mpi_comm Communicator after XIOS splitting","tags":"","loc":"module/mod_parallel_pdaf.html"},{"title":"mod_kind_pdaf – NEMO4 PDAFOMI","text":"Define real precision This module defines the kind of real and length of character strings\nfor the PDAF call-back routines and interfaces. It is based on the NEMO\nmodule par_kind.F90 . Contents Variables lc pdp pwp Variables Type Visibility Attributes Name Initial integer, public, parameter :: lc = 256 integer, public, parameter :: pdp = SELECTED_REAL_KIND(12, 307) integer, public, parameter :: pwp = pdp","tags":"","loc":"module/mod_kind_pdaf.html"}]}