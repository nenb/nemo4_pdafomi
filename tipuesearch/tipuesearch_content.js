var tipuesearch = {"pages":[{"title":" NEMO4 PDAFOMI ","text":"NEMO4 PDAFOMI PDAFOMI is a The Parallel Data Assimilation Framework written in Fortran developed by AWI. NEMO4 is a state-of-the-art modelling framework for research activities and forecasting services in ocean and climate sciences, developed in a sustainable way by a European consortium. The goal of this project is to incorporate PDAFOMI into the NEMO4 ocean model such that the NEMO4 can make take advantage of the PDAF to carry out parallel ensemble data assimilations. Due to the complexity of the ocean model, the development at the moment is limited to the NEMO-GCM model without the AGRIF components. Developer documentation Generating this documentation Developer documentation Insert documentation of the PDAFOMI attachments to the NEMO4 ocean model here. Generating this documentation This documentation is generated by FORD .\nFor more details on the project file and the comment markup in the source code visit the FORD documentation . To regenerate this documentation run: ford docs.md Developer Info Nick Byrne","tags":"home","loc":"index.html"},{"title":"mod_init_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Modules mod_init_pdaf Source Code mod_init_pdaf.F90 Source Code !>##Initialise PDAF !>This modules contains the initialisation routine for PDAF !>`init_pdaf`. Here the ensemble is initialised and distributed !>and the statevector and state variable information is computed. !> MODULE mod_init_pdaf USE mod_kind_pdaf IMPLICIT NONE SAVE CONTAINS !>##This routine collects the initialization of variables for PDAF. !> !> The initialization routine `PDAF_init` is called !> such that the internal initialization of PDAF is performed. !> The initialization is used to set-up local domain and filter options !> such as the filter type, inflation, and localization radius. !> This variant is for the online mode of PDAF. !> !> The ensemble is initialised in `init_ens_pdaf`, and is then !> distributed to the model in `distribute_state_pdaf`. The arrays !> for the incremental analysis update (IAU) are initialised in !> `asm_inc_init_pdaf`. !> !> The statevector dimension, and the offset and dimension of the !> statevector variables is calculated in `calc_statevector_dim`. !> !> Much of the initialisation is read from a PDAF-specific namelist. !> This is performed in `read_config_pdaf`. !> !> **Calling Sequence** !> !> - Called from: `nemogcm.F90` !> !> - Calls: `calc_statevector_dim` !>          `read_config_pdaf` !>          `init_pdaf_info` !>          `PDAF_init` !>          `asm_inc_init_pdaf` !>          `PDAF_get_state` !> SUBROUTINE init_pdaf () USE mod_parallel_pdaf , & ONLY : n_modeltasks , task_id , COMM_model , COMM_filter , & COMM_couple , mype_ens , filterpe , abort_parallel USE mod_assimilation_pdaf , & ONLY : dim_state_p , screen , filtertype , subtype , dim_ens , & incremental , covartype , type_forget , forget , rank_analysis_enkf , & type_trans , type_sqrt , delt_obs , locweight , local_range , srange , & salfixmin USE mod_iau_pdaf , & ONLY : asm_inc_init_pdaf USE mod_statevector_pdaf , & ONLY : calc_statevector_dim USE mod_util_pdaf , & ONLY : init_info_pdaf , read_config_pdaf !> Integer parameter array for filter INTEGER :: filter_param_i ( 7 ) !> Real parameter array for filter REAL ( pwp ) :: filter_param_r ( 2 ) !> PDAF status flag INTEGER :: status_pdaf !> Not used in this implementation INTEGER :: doexit , steps !> Not used in this implementation REAL ( pwp ) :: timenow ! Ensemble initialization EXTERNAL :: init_ens_pdaf ! Determine how long until next observation EXTERNAL :: next_observation_pdaf ! Routine to distribute a state vector to model fields EXTERNAL :: distribute_state_pdaf ! User supplied pre/poststep routine EXTERNAL :: prepoststep_ens_pdaf ! *************************** ! ***   Initialize PDAF   *** ! *************************** IF ( mype_ens == 0 ) THEN WRITE ( * , '(/1x,a)' ) 'INITIALIZE PDAF - ONLINE MODE' END IF ! Compute dimension of local statevector. CALL calc_statevector_dim ( dim_state_p ) ! ********************************************************** ! ***   CONTROL OF PDAF - used in call to PDAF_init      *** ! ********************************************************** ! *** IO options *** screen = 2 ! Write screen output (1) for output, (2) add timings ! *** Filter specific variables filtertype = 5 ! Type of filter !   (1) SEIK !   (2) EnKF !   (3) LSEIK !   (4) ETKF !   (5) LETKF !   (6) ESTKF !   (7) LESTKF dim_ens = n_modeltasks ! Size of ensemble for all ensemble filters !   We use n_modeltasks here, initialized in init_parallel_pdaf subtype = 0 ! subtype of filter: !   ESTKF: !     (0) Standard form of ESTKF !   LESTKF: !     (0) Standard form of LESTKF type_trans = 0 ! Type of ensemble transformation !   SEIK/LSEIK and ESTKF/LESTKF: !     (0) use deterministic omega !     (1) use random orthonormal omega orthogonal to (1,...,1)&#94;T !     (2) use product of (0) with random orthonormal matrix with !         eigenvector (1,...,1)&#94;T !   ETKF/LETKF: !     (0) use deterministic symmetric transformation !     (2) use product of (0) with random orthonormal matrix with !         eigenvector (1,...,1)&#94;T type_forget = 0 ! Type of forgetting factor in SEIK/LSEIK/ETKF/LETKF/ESTKF/LESTKF !   (0) fixed !   (1) global adaptive !   (2) local adaptive for LSEIK/LETKF/LESTKF forget = 1.0 ! Forgetting factor type_sqrt = 0 ! Type of transform matrix square-root !   (0) symmetric square root, (1) Cholesky decomposition incremental = 0 ! (1) to perform incremental updating (only in SEIK/LSEIK!) covartype = 1 ! Definition of factor in covar. matrix used in SEIK !   (0) for dim_ens&#94;-1 (old SEIK) !   (1) for (dim_ens-1)&#94;-1 (real ensemble covariance matrix) !   This parameter has also to be set internally in PDAF_init. rank_analysis_enkf = 0 ! rank to be considered for inversion of HPH ! in analysis of EnKF; (0) for analysis w/o eigendecomposition ! ********************************************************************* ! ***   Settings for analysis steps  - used in call-back routines   *** ! ********************************************************************* ! *** Forecast length (time interval between analysis steps) *** delt_obs = 24 ! Number of time steps between analysis/assimilation steps ! *** Localization settings locweight = 0 ! Type of localizating weighting !   (0) constant weight of 1 !   (1) exponentially decreasing with SRANGE !   (2) use 5th-order polynomial !   (3) regulated localization of R with mean error variance !   (4) regulated localization of R with single-point error variance local_range = 0 ! Range in grid points for observation domain in local filters srange = local_range ! Support range for 5th-order polynomial ! or range for 1/e for exponential weighting ! *************************************************************** ! *** Settings for analysis increments - used in IAU routines *** ! *************************************************************** ! Minimum value for salinity to allow IAU update salfixmin = 0 ! ************************** ! Namelist and screen output ! ************************** ! Read namelist file for PDAF CALL read_config_pdaf () ! Screen output for PDAF parameters IF ( mype_ens == 0 ) CALL init_info_pdaf () ! ***************************************************** ! *** Call PDAF initialization routine on all PEs.  *** ! ***                                               *** ! *** Here, the full selection of filters is        *** ! *** implemented. In a real implementation, one    *** ! *** reduce this to selected filters.              *** ! ***                                               *** ! *** For all filters, first the arrays of integer  *** ! *** and real number parameters are initialized.   *** ! *** Subsequently, PDAF_init is called.            *** ! ***************************************************** whichinit : IF ( filtertype == 2 ) THEN ! *** EnKF with Monte Carlo init *** filter_param_i ( 1 ) = dim_state_p ! State dimension filter_param_i ( 2 ) = dim_ens ! Size of ensemble filter_param_i ( 3 ) = rank_analysis_enkf ! Rank of speudo-inverse in analysis filter_param_i ( 4 ) = incremental ! Whether to perform incremental analysis filter_param_i ( 5 ) = 0 ! Smoother lag (not implemented here) filter_param_r ( 1 ) = forget ! Forgetting factor CALL PDAF_init ( filtertype , subtype , 0 , & filter_param_i , 6 , & filter_param_r , 2 , & COMM_model , COMM_filter , COMM_couple , & task_id , n_modeltasks , filterpe , init_ens_pdaf , & screen , status_pdaf ) ELSE ! *** All other filters                       *** ! *** SEIK, LSEIK, ETKF, LETKF, ESTKF, LESTKF *** filter_param_i ( 1 ) = dim_state_p ! State dimension filter_param_i ( 2 ) = dim_ens ! Size of ensemble filter_param_i ( 3 ) = 0 ! Smoother lag (not implemented here) filter_param_i ( 4 ) = incremental ! Whether to perform incremental analysis filter_param_i ( 5 ) = type_forget ! Type of forgetting factor filter_param_i ( 6 ) = type_trans ! Type of ensemble transformation filter_param_i ( 7 ) = type_sqrt ! Type of transform square-root (SEIK-sub4/ESTKF) filter_param_r ( 1 ) = forget ! Forgetting factor CALL PDAF_init ( filtertype , subtype , 0 , & filter_param_i , 7 , & filter_param_r , 2 , & COMM_model , COMM_filter , COMM_couple , & task_id , n_modeltasks , filterpe , init_ens_pdaf , & screen , status_pdaf ) END IF whichinit ! *** Check whether initialization of PDAF was successful *** IF ( status_pdaf /= 0 ) THEN WRITE ( * , '(/1x,a6,i3,a43,i4,a1/)' ) & 'ERROR ' , status_pdaf , & ' in initialization of PDAF - stopping! (PE ' , mype_ens , ')' CALL abort_parallel () END IF ! ************************************** ! *** Initialise PDAF arrays for IAU *** ! ************************************** CALL asm_inc_init_PDAF () ! ********************************** ! *** Prepare ensemble forecasts *** ! ********************************** CALL PDAF_get_state ( steps , timenow , doexit , next_observation_pdaf , & distribute_state_pdaf , prepoststep_ens_pdaf , status_pdaf ) END SUBROUTINE init_pdaf END MODULE mod_init_pdaf","tags":"","loc":"sourcefile/mod_init_pdaf.f90.html"},{"title":"prepoststep_ens_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Subroutines prepoststep_ens_pdaf Source Code prepoststep_ens_pdaf.F90 Source Code !>##Controlling Pre- and Post-Processing of the PDAF output !> !> - For global filters (e.g. SEIK), the routine is called !>before the analysis and after the ensemble transformation. !> - For local filters (e.g. LSEIK), the routine is called !>before and after the loop over all local analysis !>domains. !> !>The routine provides full access to the state !>estimate and the state ensemble to the user. !>Thus, user-controlled pre- and poststep !>operations can be performed here. For example !>the forecast and the analysis states and ensemble !>covariance matrix can be analyzed, e.g. by !>computing the estimated variances. !>For the offline mode, this routine is the place !>in which the writing of the analysis ensemble !>can be performed. !> !>If a user considers to perform adjustments to the !>estimates (e.g. for balances), this routine is !>the right place for it. !> !>**Calling Sequence** !> !> - Called by: `PDAF_get_state` (as U_prepoststep) `PDAF_X_update` (as U_prepoststep) !> SUBROUTINE prepoststep_ens_pdaf ( step , dim_p , dim_ens , dim_ens_p , dim_obs_p , & state_p , Uinv , ens_p , flag ) USE mod_kind_pdaf IMPLICIT NONE !> Current time step (negative for call after forecast) INTEGER , INTENT ( in ) :: step !> PE-local state dimension INTEGER , INTENT ( in ) :: dim_p !> Size of state ensemble INTEGER , INTENT ( in ) :: dim_ens !> PE-local size of ensemble INTEGER , INTENT ( in ) :: dim_ens_p !> PE-local dimension of observation vector INTEGER , INTENT ( in ) :: dim_obs_p !> PE-local forecast/analysis state !> The array 'state_p' is already initialised and can be used !> freely here (not for SEEK!) REAL ( pwp ), INTENT ( inout ) :: state_p ( dim_p ) !> Inverse of matrix U REAL ( pwp ), INTENT ( inout ) :: Uinv ( dim_ens - 1 , dim_ens - 1 ) !> PE-local state ensemble REAL ( pwp ), INTENT ( inout ) :: ens_p ( dim_p , dim_ens ) !> PDAF status flag INTEGER , INTENT ( in ) :: flag ! Deallocate observation arrays EXTERNAL :: deallocate_obs_pdafomi WRITE ( * , '(/1x,a)' ) 'Insert prepoststep routines here. ' ! Deallocate observation arrays - DO NOT REMOVE CALL deallocate_obs_pdafomi ( step ) END SUBROUTINE prepoststep_ens_pdaf","tags":"","loc":"sourcefile/prepoststep_ens_pdaf.f90.html"},{"title":"l2g_state_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Subroutines l2g_state_pdaf Source Code l2g_state_pdaf.F90 Source Code !>##Initialize full state from local analysis !> !>The routine is called during the loop over all !>local analysis domains in `PDAF_X_update` !>after the analysis and ensemble transformation !>on a single local analysis domain. It has to !>initialize elements of the PE-local full state !>vector from the provided analysis state vector !>on the local analysis domain. !> SUBROUTINE l2g_state_pdaf ( step , domain_p , dim_l , state_l , dim_p , state_p ) USE mod_kind_pdaf USE mod_parallel_pdaf , & ONLY : abort_parallel USE mod_assimilation_pdaf , & ONLY : indx_dom_l USE mod_statevector_pdaf , & ONLY : mpi_subd_vert , mpi_subd_lon , mpi_subd_lat , & var2d_p_offset , var3d_p_offset USE dom_oce , & ONLY : nldj , nldi , tmask IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step !> Current local analysis domain INTEGER , INTENT ( in ) :: domain_p !> PE-local full state dimension INTEGER , INTENT ( in ) :: dim_l !> Local state dimension INTEGER , INTENT ( in ) :: dim_p !> PE-local full state vector REAL ( pwp ), INTENT ( in ) :: state_l ( dim_l ) !> State vector on local analysis domain REAL ( pwp ), INTENT ( out ) :: state_p ( dim_p ) !> Counter INTEGER :: idx !> Halo offset for local PE INTEGER :: i0 , j0 !> Grid coordinates for local analysis domain INTEGER :: i , j !> Array for converting vertical coordinate in local statevector INTEGER :: vcoord_conv ( 100 ) !> Number of vertical ocean points in local statevector INTEGER :: dim_vert_l !> 2D state variable coordinate in statevector INTEGER :: loc_2dvar !> Variables for 3D state variable index INTEGER :: a , b , c ! ******************************************************* ! *** Initialise vertical coordinate conversion array *** ! ******************************************************* IF ( SIZE ( vcoord_conv ) < mpi_subd_vert ) THEN WRITE ( * , '(/1x,a59/)' ) & 'ERROR: automatic array v_coord in l2g_state_pdaf too small ' CALL abort_parallel () END IF vcoord_conv = 0 dim_vert_l = 0 ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 ! Compute coordinates i = indx_dom_l ( 1 , domain_p ) j = indx_dom_l ( 2 , domain_p ) ! Count number of ocean vertical points DO idx = 1 , mpi_subd_vert IF ( tmask ( i + i0 , j + j0 , idx ) == 1.0_pwp ) THEN dim_vert_l = dim_vert_l + 1 vcoord_conv ( dim_vert_l ) = idx END IF END DO ! ************************************* ! *** Initialize local state vector *** ! ************************************* ! ********************************************************** ! A local domain consists of all ocean points in a vertical ! column. Such a domain will have coordinates (x,y,:). ! The 2d state variables in the global statevector will be ! located at ( (y-1)*dim_longitude ) + x + 2d_offset. ! The 3d state variables in the global statevector will be ! located at ( (z-1)*dim_longitude*dim_latitude ) + ! ( (y-1)*dim_longitude ) + x + 3d_offset, where z can vary ! over all *ocean* points in the vertical column. ! ********************************************************** ! Compute location of 2d variables in statevector loc_2dvar = ( j - 1 ) * ( mpi_subd_lon ) + i DO idx = 1 , dim_l ! Compute 2d state variables IF ( idx <= SIZE ( var2d_p_offset )) THEN state_p ( loc_2dvar + var2d_p_offset ( idx )) = state_l ( idx ) ELSE ! Compute 3d state variables a = idx - SIZE ( var2d_p_offset ) b = MOD ( a - 1 , dim_vert_l ) + 1 c = ( a - b ) / dim_vert_l + 1 state_p ( mpi_subd_lon * mpi_subd_lat * ( vcoord_conv ( b ) - 1 ) & + loc_2dvar + var3d_p_offset ( c )) = state_l ( idx ) END IF END DO END SUBROUTINE l2g_state_pdaf","tags":"","loc":"sourcefile/l2g_state_pdaf.f90.html"},{"title":"callback_obs_pdafomi.F90 – NEMO4 PDAFOMI","text":"Contents Subroutines init_dim_obs_pdafomi obs_op_pdafomi init_dim_obs_l_pdafomi deallocate_obs_pdafomi Source Code callback_obs_pdafomi.F90 Source Code !>##PDAFOMI interface routines !> !>This file provides interface routines between the call-back routines !>of PDAF and the observation-specific routines in PDAFOMI. This structure !>collects all calls to observation-specifc routines in this single file !>to make it easier to find the routines that need to be adapted. !> !>The routines here are mainly pure pass-through routines. Thus they !>simply call one of the routines from PDAF-OMI. !> !>##Initialise the dimension of observations !> !>**Calling Sequence** !> !> - Called by: `mod_assimilation_pdaf` (as `U_prepoststep`) !> !> - Calls: `init_dim_obs_ssh_mgrid` !> SUBROUTINE init_dim_obs_pdafomi ( step , dim_obs ) USE mod_kind_pdaf USE mod_obs_ssh_mgrid_pdafomi , & ONLY : assim_ssh_mgrid , init_dim_obs_ssh_mgrid IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step !> Dimension of full observation vector INTEGER , INTENT ( out ) :: dim_obs !> Observation dimensions INTEGER :: dim_obs_ssh_mgrid dim_obs_ssh_mgrid = 0 IF ( assim_ssh_mgrid ) CALL init_dim_obs_ssh_mgrid ( step , dim_obs_ssh_mgrid ) dim_obs = dim_obs_ssh_mgrid END SUBROUTINE init_dim_obs_pdafomi !>##Define observation operator !> !>**Calling Sequence** !> !> - Called by: `mod_assimilation_pdaf` (as `U_prepoststep`) !> !> - Calls: `obs_op_ssh_mgrid` !> SUBROUTINE obs_op_pdafomi ( step , dim_p , dim_obs , state_p , ostate ) USE mod_kind_pdaf USE mod_obs_ssh_mgrid_pdafomi , & ONLY : obs_op_ssh_mgrid IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step !> PE-local state dimension INTEGER , INTENT ( in ) :: dim_p !> Dimension of full observed state INTEGER , INTENT ( in ) :: dim_obs !> PE-local model state REAL ( pwp ), INTENT ( in ) :: state_p ( dim_p ) !> PE-local full observed state REAL ( pwp ), INTENT ( inout ) :: ostate ( dim_obs ) CALL obs_op_ssh_mgrid ( dim_p , dim_obs , state_p , ostate ) END SUBROUTINE obs_op_pdafomi !>##Initialise the dimension of local observations !> !>**Calling Sequence** !> !> - Called by: `mod_assimilation_pdaf` (as `U_prepoststep`) !> !> - Calls: `init_dim_obs_l_ssh_mgrid` !> SUBROUTINE init_dim_obs_l_pdafomi ( domain_p , step , dim_obs , dim_obs_l ) USE mod_kind_pdaf USE mod_assimilation_pdaf , & ONLY : indx_dom_l USE mod_obs_ssh_mgrid_pdafomi , & ONLY : init_dim_obs_l_ssh_mgrid USE dom_oce , & ONLY : nldi , nldj , tmask IMPLICIT NONE !> Index of current local analysis domain INTEGER , INTENT ( in ) :: domain_p !> Current time step INTEGER , INTENT ( in ) :: step !> Full dimension of observation vector INTEGER , INTENT ( in ) :: dim_obs !> Local dimension of observation vector INTEGER , INTENT ( out ) :: dim_obs_l !> Halo offset for local PE INTEGER :: i0 , j0 !> Grid coordinates for local analysis domain INTEGER :: i , j ! Indicate whether surface gridpoint land LOGICAL :: land ! Hack for dealing with case when no valid local domains on PE. ! See init_n_domains for details. land = . FALSE . IF ( domain_p == 1 ) THEN ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 ! Compute i,j indices i = indx_dom_l ( 1 , domain_p ) j = indx_dom_l ( 2 , domain_p ) ! Check whether the local domain is actually a land point ! (and hence not a valid local domain). IF ( tmask ( i + i0 , j + j0 , 1 ) == 0.0_pwp ) land = . TRUE . END IF IF ( land ) THEN dim_obs_l = 0 ELSE CALL init_dim_obs_l_ssh_mgrid ( domain_p , step , dim_obs , dim_obs_l ) END IF END SUBROUTINE init_dim_obs_l_pdafomi !>##Deallocate observation arrays !>This routine calls the routine `PDAFomi_deallocate_obs` !>for each observation type. !> SUBROUTINE deallocate_obs_pdafomi ( step ) USE mod_kind_pdaf USE PDAFomi , & ONLY : PDAFomi_deallocate_obs USE mod_obs_ssh_mgrid_pdafomi , & ONLY : obs_ssh_mgrid => thisobs USE mod_assimilation_pdaf , & ONLY : indx_dom_l IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step CALL PDAFomi_deallocate_obs ( obs_ssh_mgrid ) ! Tidy-up from init_n_domains_pdaf IF ( ALLOCATED ( indx_dom_l )) DEALLOCATE ( indx_dom_l ) END SUBROUTINE deallocate_obs_pdafomi","tags":"","loc":"sourcefile/callback_obs_pdafomi.f90.html"},{"title":"mod_kind_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Modules mod_kind_pdaf Source Code mod_kind_pdaf.F90 Source Code MODULE mod_kind_pdaf !>## Define real precision !> !>This module defines the kind of real and length of character strings !>for the PDAF call-back routines and interfaces. It is based on the NEMO !>module `par_kind.F90`. IMPLICIT NONE SAVE !> double precision INTEGER , PARAMETER :: pdp = SELECTED_REAL_KIND ( 12 , 307 ) !> double precision INTEGER , PARAMETER :: pwp = pdp INTEGER , PARAMETER :: lc = 256 END MODULE mod_kind_pdaf","tags":"","loc":"sourcefile/mod_kind_pdaf.f90.html"},{"title":"mod_util_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Modules mod_util_pdaf Source Code mod_util_pdaf.F90 Source Code !>##Utility Routines !>This module contains several routines useful for common !>model tasks. The initial routines included output configuration !>information about the PDAF library, and configuration information !>about the assimilation parameters. !> MODULE mod_util_pdaf USE mod_kind_pdaf IMPLICIT NONE SAVE CONTAINS !> This routine performs a model-sided screen output about !> the coniguration of the data assimilation system. !> !> **Calling Sequence** !> !> - Called from: `init_pdaf` SUBROUTINE init_info_pdaf () USE mod_assimilation_pdaf , & ! Variables for assimilation ONLY : filtertype , subtype , dim_ens , delt_obs , model_error , & model_err_amp , forget , rank_analysis_enkf , int_rediag ! ***************************** ! *** Initial Screen output *** ! ***************************** IF ( filtertype == 0 ) THEN WRITE ( * , '(/21x, a)' ) 'Filter: SEEK' IF ( subtype == 2 ) THEN WRITE ( * , '(6x, a)' ) '-- fixed basis filter with update of matrix U' WRITE ( * , '(6x, a)' ) '-- no re-diagonalization of VUV&#94;T' ELSE IF ( subtype == 3 ) THEN WRITE ( * , '(6x, a)' ) '-- fixed basis filter & no update of matrix U' WRITE ( * , '(6x, a)' ) '-- no re-diagonalization of VUV&#94;T' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(13x, a, i5)' ) 'number of EOFs:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( subtype /= 5 ) THEN IF (( int_rediag > 0 ) . AND . (( subtype /= 2 ) . OR . ( subtype /= 3 ))) & WRITE ( * , '(10x, a, i4, a)' ) & 'Re-diag each ' , int_rediag , '-th analysis step' ELSE IF ( int_rediag == 1 ) THEN WRITE ( * , '(10x, a)' ) 'Perform re-diagonalization' ELSE WRITE ( * , '(10x, a)' ) 'No re-diagonalization' END IF END IF ELSE IF ( filtertype == 1 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: SEIK' IF ( subtype == 2 ) THEN WRITE ( * , '(6x, a)' ) '-- fixed error-space basis' ELSE IF ( subtype == 3 ) THEN WRITE ( * , '(6x, a)' ) '-- fixed state covariance matrix' ELSE IF ( subtype == 4 ) THEN WRITE ( * , '(6x, a)' ) '-- use ensemble transformation' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF ELSE IF ( filtertype == 2 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: EnKF' IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF IF ( rank_analysis_enkf > 0 ) THEN WRITE ( * , '(6x, a, i5)' ) & 'analysis with pseudo-inverse of HPH, rank:' , rank_analysis_enkf END IF ELSE IF ( filtertype == 3 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: LSEIK' IF ( subtype == 2 ) THEN WRITE ( * , '(6x, a)' ) '-- fixed error-space basis' ELSE IF ( subtype == 3 ) THEN WRITE ( * , '(6x, a)' ) '-- fixed state covariance matrix' ELSE IF ( subtype == 4 ) THEN WRITE ( * , '(6x, a)' ) '-- use ensemble transformation' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF ELSE IF ( filtertype == 4 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: ETKF' IF ( subtype == 0 ) THEN WRITE ( * , '(6x, a)' ) '-- Variant using T-matrix' ELSE IF ( subtype == 1 ) THEN WRITE ( * , '(6x, a)' ) '-- Variant following Hunt et al. (2007)' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF ELSE IF ( filtertype == 5 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: LETKF' IF ( subtype == 0 ) THEN WRITE ( * , '(6x, a)' ) '-- Variant using T-matrix' ELSE IF ( subtype == 1 ) THEN WRITE ( * , '(6x, a)' ) '-- Variant following Hunt et al. (2007)' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF ELSE IF ( filtertype == 6 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: ESTKF' IF ( subtype == 0 ) THEN WRITE ( * , '(6x, a)' ) '-- Standard mode' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF ELSE IF ( filtertype == 7 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: LESTKF' IF ( subtype == 0 ) THEN WRITE ( * , '(6x, a)' ) '-- Standard mode' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF END IF END SUBROUTINE init_info_pdaf !> This routine reads the namelist file with parameters !> controlling data assimilation with PDAF and outputs to !> screen. !> !> **Calling Sequence** !> !> - Called from: `init_pdaf` SUBROUTINE read_config_pdaf () USE mod_parallel_pdaf , & ONLY : mype_ens USE mod_assimilation_pdaf , & ONLY : filtertype , subtype , dim_ens , delt_obs , & screen , forget , local_range , locweight , srange , istate_t , & istate_s , istate_u , istate_v , istate_ssh , salfixmin !> Namelist file CHARACTER ( lc ) :: nmlfile NAMELIST / pdaf_nml / filtertype , subtype , dim_ens , & delt_obs , screen , forget , local_range , locweight , & srange , istate_s , istate_t , istate_u , istate_v , istate_ssh , & salfixmin ! **************************************************** ! ***   Initialize PDAF parameters from namelist   *** ! **************************************************** nmlfile = 'namelist.pdaf' OPEN ( 20 , file = nmlfile ) READ ( 20 , NML = pdaf_nml ) CLOSE ( 20 ) ! Print PDAF parameters to screen showconf : IF ( mype_ens == 0 ) THEN WRITE ( * , '(/1x,a)' ) '-- Overview of PDAF configuration --' WRITE ( * , '(3x,a)' ) 'PDAF [pdaf_nml]:' WRITE ( * , '(5x,a,i10)' ) 'filtertype   ' , filtertype WRITE ( * , '(5x,a,i10)' ) 'subtype      ' , subtype WRITE ( * , '(5x,a,i10)' ) 'dim_ens      ' , dim_ens WRITE ( * , '(5x,a,i10)' ) 'delt_obs     ' , delt_obs WRITE ( * , '(5x,a,i10)' ) 'screen       ' , screen WRITE ( * , '(5x,a,f10.2)' ) 'forget       ' , forget WRITE ( * , '(5x,a,es10.2)' ) 'local_range  ' , local_range WRITE ( * , '(5x,a,i10)' ) 'locweight    ' , locweight WRITE ( * , '(5x,a,es10.2)' ) 'srange       ' , srange WRITE ( * , '(5x,a,a)' ) 'istate_t   ' , istate_t WRITE ( * , '(5x,a,a)' ) 'istate_s   ' , istate_s WRITE ( * , '(5x,a,a)' ) 'istate_u   ' , istate_u WRITE ( * , '(5x,a,a)' ) 'istate_v   ' , istate_v WRITE ( * , '(5x,a,a)' ) 'istate_ssh ' , istate_ssh WRITE ( * , '(5x,a,es10.2)' ) 'salfixmin  ' , salfixmin WRITE ( * , '(1x,a)' ) '-- End of PDAF configuration overview --' END IF showconf END SUBROUTINE read_config_pdaf SUBROUTINE finalize_pdaf () !>Timing and clean-up of PDAF !> !> **Calling Sequence** !> !> - Called from: `nemogcm` !> !> - Calls: `PDAF_deallocate` USE mod_parallel_pdaf , & ONLY : mype_ens USE mod_iau_pdaf , & ONLY : ssh_iau_pdaf , t_iau_pdaf , s_iau_pdaf , u_iau_pdaf , v_iau_pdaf ! Show allocated memory for PDAF ! DOES NOT CURRENTLY WORK WITH XIOS CONFIGURATION - TBD WITH LARS !IF (mype_ens==0) CALL PDAF_print_info(2) ! Print PDAF timings onto screen ! DOES NOT CURRENTLY WORK WITH XIOS CONFIGURATION - TBD WITH LARS !IF (mype_ens==0) CALL PDAF_print_info(1) ! Deallocate PDAF arrays CALL PDAF_deallocate () ! Deallocaite IAU arrays ! 2D variables DEALLOCATE ( ssh_iau_pdaf ) ! 3D variables DEALLOCATE ( t_iau_pdaf , s_iau_pdaf , u_iau_pdaf , v_iau_pdaf ) END SUBROUTINE finalize_pdaf END MODULE mod_util_pdaf","tags":"","loc":"sourcefile/mod_util_pdaf.f90.html"},{"title":"init_ens_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Subroutines init_ens_pdaf Source Code init_ens_pdaf.F90 Source Code !>##Ensemble Initialisation !>This routine calls the routines for initialising the ensemble. !> !>Separate calls are made for the 2D and 3D state variables to !>allow for differences in how these variables are initialised. !> !>The routine is called when the filter is initialized in !>`PDAF_filter_init`. !> !>The routine is called by all filter processes and !>initializes the ensemble for the *PE-local domain*. !> !> **Calling Sequence** !> !> - Called from: `init_pdaf/PDAF_init` (PDAF module) !> !> - Calls: `fill2d_ensarray` `fill3d_ensarray` SUBROUTINE init_ens_pdaf ( filtertype , dim_p , dim_ens , state_p , Uinv , & ens_p , flag ) USE mod_kind_pdaf USE mod_parallel_pdaf , & ONLY : mype_ens USE mod_assimilation_pdaf , & ONLY : istate_ssh , istate_s , istate_t , istate_u , istate_v , & screen USE mod_statevector_pdaf , & ONLY : fill2d_ensarray , fill3d_ensarray IMPLICIT NONE !> Type of filter to initialize INTEGER , INTENT ( in ) :: filtertype !> PE-local state dimension INTEGER , INTENT ( in ) :: dim_p !> Size of ensemble INTEGER , INTENT ( in ) :: dim_ens !> PE-local model state !> It is not necessary to initialize the array 'state_p' for SEIK. !> It is available here only for convenience and can be used freely. REAL ( pwp ), INTENT ( inout ) :: state_p ( dim_p ) !> Array not referenced for SEIK REAL ( pwp ), INTENT ( inout ) :: Uinv ( dim_ens - 1 , dim_ens - 1 ) !> PE-local state ensemble REAL ( pwp ), INTENT ( out ) :: ens_p ( dim_p , dim_ens ) !> PDAF status flag INTEGER , INTENT ( inout ) :: flag IF ( screen > 0 ) THEN IF ( mype_ens == 0 ) THEN WRITE ( * , '(/1x,a)' ) '------- Reading Initial State --------' WRITE ( * , '(/1x,a)' ) 'Calling fill2d_ensarray' WRITE ( * , '(/9x, a, 3x, a,)' ) \"Initial state file:\" , TRIM ( istate_ssh ) END IF END IF CALL fill2d_ensarray ( istate_ssh , ens_p ) IF ( screen > 0 ) THEN IF ( mype_ens == 0 ) THEN WRITE ( * , '(/1x,a)' ) '------- Reading Initial State --------' WRITE ( * , '(/1x,a)' ) 'Calling fill3d_ensarray' WRITE ( * , '(/9x, a, 3x, a, 3x, a, 3x, a, 3x, a)' ) & \"Initial state files:\" , TRIM ( istate_t ), TRIM ( istate_s ), & TRIM ( istate_u ), TRIM ( istate_v ) END IF END IF CALL fill3d_ensarray ( istate_t , 'T' , ens_p ) CALL fill3d_ensarray ( istate_s , 'S' , ens_p ) CALL fill3d_ensarray ( istate_u , 'U' , ens_p ) CALL fill3d_ensarray ( istate_v , 'V' , ens_p ) END SUBROUTINE init_ens_pdaf","tags":"","loc":"sourcefile/init_ens_pdaf.f90.html"},{"title":"mod_obs_ssh_mgrid_pdafomi.F90 – NEMO4 PDAFOMI","text":"Contents Modules mod_obs_ssh_mgrid_pdafomi Source Code mod_obs_ssh_mgrid_pdafomi.F90 Source Code !>##PDAF-OMI observation module for ssh observations (on model grid) !> !>The subroutines in this module are for the particular handling of !>ssh observations available on the *model* grid. !> !>The routines are called by the different call-back routines of PDAF. !>Most of the routines are generic so that in practice only 2 routines !>need to be adapted for a particular data type. These are the routines !>for the initialization of the observation information (`init_dim_obs`) !>and for the observation operator (`obs_op`). !> !> !>The module uses two derived data type (obs_f and obs_l), which contain !>all information about the full and local observations. Only variables !>of the type obs_f need to be initialized in this module. The variables !>in the type obs_l are initialized by the generic routines from `PDAFomi`. !> MODULE mod_obs_ssh_mgrid_pdafomi USE mod_kind_pdaf USE mod_parallel_pdaf , & ONLY : mype_filter , abort_parallel USE PDAFomi , & ONLY : obs_f , obs_l USE netcdf IMPLICIT NONE SAVE !> Whether to assimilate this data type LOGICAL :: assim_ssh_mgrid = . TRUE . !> Observation error standard deviation (for constant errors) REAL ( pwp ) :: rms_ssh_mgrid = 1 !> Whether to perform an identical twin experiment LOGICAL :: twin_exp_ssh_mgrid = . FALSE . !> Standard deviation for Gaussian noise in twin experiment REAL ( pwp ) :: noise_amp_ssh_mgrid = 1 ! NetCDF file holding observations CHARACTER ( lc ) :: file_ssh_mgrid = 'my_nemo_ssh_file.nc' !> Instance of full observation data type - see `PDAFomi` for details. TYPE ( obs_f ), TARGET , PUBLIC :: thisobs !> Instance of local observation data type - see `PDAFomi` for details. TYPE ( obs_l ), TARGET , PUBLIC :: thisobs_l !$OMP THREADPRIVATE(thisobs_l) CONTAINS !>###Initialize information on the observation !> !> The routine is called by each filter process. !> at the beginning of the analysis step before !> the loop through all local analysis domains. !> !> It has to count the number of process-local and full !> observations, initialize the vector of observations !> and their inverse variances, initialize the coordinate !> array and index array for indices of observed elements !> of the state vector. !> !> The following four variables have to be initialized in this routine: !> !> - **thisobs%doassim** - Whether to assimilate ssh !> - **thisobs%disttype** - type of distance computation for localization !> with ssh !> - **thisobs%ncoord** - number of coordinates used for distance !> computation !> - **thisobs%id_obs_p** - index of module-type observation in PE-local state !> vector !> !> !> Optional is the use of: !> !> - **thisobs%icoeff_p** - Interpolation coefficients for obs. operator !> (only if interpolation is used) !> - **thisobs%domainsize** - Size of domain for periodicity for *disttype=1* !> (<0 for no periodicity) !> - **thisobs%obs_err_type** - Type of observation errors for particle filter !> and NETF !> - **thisobs%use_global_obs** - Whether to use global observations or !> restrict the observations to the relevant ones (default: *.true.* i.e use !> global full observations) !> !> The following variables are set in the routine gather_obs: !> !> - **thisobs%dim_obs_p** - PE-local number of ssh observations !> - **thisobs%dim_obs** - full number of ssh observations !> - **thisobs%obs_f** - full vector of ssh observations !> - **thisobs%ocoord_f** - coordinates of observations in OBS_MOD_F !> -  **thisobs%ivar_obs_f** - full vector of inverse obs. error variances of !> module-type !> - **thisobs%dim_obs_g** - Number of global observations (only if !>*use_global_obs=.false*) !> - **thisobs%id_obs_f_lim** - Ids of full observations in global observations !> (if *use_global_obs=.false*) !> SUBROUTINE init_dim_obs_ssh_mgrid ( step , dim_obs ) USE PDAFomi , & ONLY : PDAFomi_gather_obs USE mod_assimilation_pdaf , & ONLY : filtertype , local_range , delt_obs USE mod_statevector_pdaf , & ONLY : mpi_subd_lon , mpi_subd_lat USE mod_parallel_pdaf , & ONLY : COMM_filter USE par_oce , & ONLY : jpiglo , jpjglo USE dom_oce , & ONLY : nldj , nldi , glamt , gphit , nimpp , njmpp , ndastp !> Current time step INTEGER , INTENT ( in ) :: step !> Dimension of full observation vector INTEGER , INTENT ( inout ) :: dim_obs !> Counters INTEGER :: i , j , s !> Step for observations in NetCDF file INTEGER :: nc_step = 0 !> Status array for NetCDF operations INTEGER :: stat ( 50 ) !> ID for NetCDF file INTEGER :: ncid_in !> IDs for fields INTEGER :: id_var !> NetCDF position arrays for 3D field INTEGER :: pos ( 3 ), cnt ( 3 ) !> Global observation field REAL ( pwp ), ALLOCATABLE :: obs (:, :, :) !> Number of process-local observations INTEGER :: dim_obs_p !> Counters INTEGER :: cnt_p , cnt0_p !> Global gridbox coordinates of observations INTEGER :: i_obs , j_obs !> Halo offset for local PE INTEGER :: i0 , j0 !> PE-local observation vector REAL ( pwp ), ALLOCATABLE :: obs_p (:) !> PE-local inverse observation error variance REAL ( pwp ), ALLOCATABLE :: ivar_obs_p (:) !> PE-local observation coordinates REAL ( pwp ), ALLOCATABLE :: ocoord_p (:, :) !> Degree to radian conversion REAL ( pwp ) :: rad_conv = 3.141592653589793 / 18 0.0 ! ***************************** ! *** Global setting config *** ! ***************************** IF ( mype_filter == 0 ) & WRITE ( * , '(8x,a)' ) 'Assimilate observations - obs_ssh_mgrid' ! Store whether to assimilate this observation type (used in routines ! below) IF ( assim_ssh_mgrid ) thisobs % doassim = 1 ! Specify type of distance computation thisobs % disttype = 3 ! 3=Haversine ! Number of coordinates used for distance computation. ! The distance compution starts from the first row thisobs % ncoord = 2 ! ********************************** ! *** Read PE-local observations *** ! ********************************** ! Format of ndastp is YYYYMMDD IF ( mype_filter == 0 ) WRITE ( * , '(/9x, a, i8)' ) & 'obs_ssh_mgrid current date:' , ndastp s = 1 stat ( s ) = NF90_OPEN ( file_ssh_mgrid , NF90_NOWRITE , ncid_in ) s = s + 1 stat ( s ) = NF90_INQ_VARID ( ncid_in , 'sossheig' , id_var ) s = s + 1 ALLOCATE ( obs ( jpiglo , jpjglo , 1 )) ! Increment time in NetCDF file so correct obs read nc_step = nc_step + delt_obs pos = ( / 1 , 1 , nc_step / ) cnt = ( / jpiglo , jpjglo , 1 / ) stat ( s ) = NF90_GET_VAR ( ncid_in , id_var , obs , start = pos , count = cnt ) s = s + 1 stat ( s ) = NF90_CLOSE ( ncid_in ) s = s + 1 DO j = 1 , s - 1 IF ( stat ( j ) . NE . NF90_NOERR ) THEN WRITE ( * , '(/9x, a, 3x, a, 3x, a, i2)' ) & 'NetCDF error in reading obs file:' , file_ssh_mgrid , & 'status array, j=' , j CALL abort_parallel () END IF END DO ! *********************************************************** ! *** Count available observations for the process domain *** ! *** and initialize index and coordinate arrays.         *** ! *********************************************************** ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 cnt_p = 0 DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon ! Convert to global coordinates i_obs = nimpp + i0 + i - 1 j_obs = njmpp + j0 + j - 1 cnt_p = cnt_p + 1 END DO END DO ! Set number of local observations dim_obs_p = cnt_p IF ( cnt_p == 0 ) WRITE ( * , '(/9x, a, i3, 3x, a, i4)' ) & 'WARNING: No ssh_mgrid observations on PE:' , mype_filter , & 'NetCDF file step=' , nc_step obs_nonzero : IF ( dim_obs_p > 0 ) THEN ! Vector of observations on the process sub-domain ALLOCATE ( obs_p ( dim_obs_p )) ! Coordinate array of observations on the process sub-domain ALLOCATE ( ocoord_p ( 2 , dim_obs_p )) ! Coordinate array for observation operator ALLOCATE ( thisobs % id_obs_p ( 1 , dim_obs_p )) ALLOCATE ( ivar_obs_p ( dim_obs_p )) ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 cnt_p = 0 cnt0_p = 0 DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon ! State vector index counter for observation operator. cnt0_p = cnt0_p + 1 ! Convert to global coordinates. i_obs = nimpp + i0 + i - 1 j_obs = njmpp + j0 + j - 1 cnt_p = cnt_p + 1 obs_p ( cnt_p ) = obs ( i_obs , j_obs , 1 ) ! Observation coordinates - must be in radians for PDAFOMI ocoord_p ( 1 , cnt_p ) = glamt ( i + i0 , j + j0 ) * rad_conv ocoord_p ( 2 , cnt_p ) = gphit ( i + i0 , j + j0 ) * rad_conv ! Coordinates for observation operator (gridpoint) thisobs % id_obs_p ( 1 , cnt_p ) = cnt0_p END DO END DO ELSE ! No observations on PE, create dummy arrays to pass to PDAFOMI ALLOCATE ( obs_p ( 1 )) ALLOCATE ( ivar_obs_p ( 1 )) ALLOCATE ( ocoord_p ( 2 , 1 )) ALLOCATE ( thisobs % id_obs_p ( 1 , 1 )) obs_p = - 99999 9.0 ivar_obs_p = EPSILON ( ivar_obs_p ) ocoord_p = 0 thisobs % id_obs_p = 1 END IF obs_nonzero ! **************************************************************** ! *** Define observation errors for process-local observations *** ! **************************************************************** ! Set inverse observation error variances ivar_obs_p (:) = 1.0 / ( rms_ssh_mgrid * rms_ssh_mgrid ) ! ********************************************************* ! *** For twin experiment: Read synthetic observations  *** ! ********************************************************* IF ( twin_exp_ssh_mgrid ) THEN IF ( dim_obs_p > 0 ) CALL add_noise ( dim_obs_p , obs_p ) END IF ! **************************************** ! *** Gather global observation arrays *** ! **************************************** CALL PDAFomi_gather_obs ( thisobs , dim_obs_p , obs_p , ivar_obs_p , ocoord_p , & thisobs % ncoord , local_range , dim_obs ) ! ******************** ! *** Finishing up *** ! ******************** ! Deallocate all local arrays DEALLOCATE ( obs ) DEALLOCATE ( obs_p , ocoord_p , ivar_obs_p ) ! Arrays in THISOBS have to be deallocated after the analysis step ! by a call to deallocate_obs() in prepoststep_pdaf. END SUBROUTINE init_dim_obs_ssh_mgrid !>###Implementation of observation operator !> !>This routine applies the full observation operator !>for the ssh observations. !> !>The routine is called by all filter processes. !> SUBROUTINE obs_op_ssh_mgrid ( dim_p , dim_obs , state_p , ostate ) USE PDAFomi , & ONLY : PDAFomi_obs_op_gridpoint !> PE-local state dimension INTEGER , INTENT ( in ) :: dim_p !> Dimension of full observed state (all observed fields) INTEGER , INTENT ( in ) :: dim_obs !> PE-local model state REAL ( pwp ), INTENT ( in ) :: state_p ( dim_p ) !> Full observed state REAL ( pwp ), INTENT ( inout ) :: ostate ( dim_obs ) ! ****************************************************** ! *** Apply observation operator H on a state vector *** ! ****************************************************** IF ( thisobs % doassim == 1 ) THEN CALL PDAFomi_obs_op_gridpoint ( thisobs , state_p , ostate ) END IF END SUBROUTINE obs_op_ssh_mgrid !>###Initialize local information on the module-type observation !> !>The routine is called during the loop over all local !>analysis domains. It has to initialize the information !>about local ssh observations. !> !>This routine calls the routine `PDAFomi_init_dim_obs_l` !>for each observation type. The call allows to specify a !>different localization radius and localization functions !>for each observation type and local analysis domain. !> SUBROUTINE init_dim_obs_l_ssh_mgrid ( domain_p , step , dim_obs , dim_obs_l ) USE PDAFomi , & ONLY : PDAFomi_init_dim_obs_l USE mod_assimilation_pdaf , & ONLY : coords_l , local_range , locweight , srange !> Index of current local analysis domain INTEGER , INTENT ( in ) :: domain_p !> Current time step INTEGER , INTENT ( in ) :: step !> Full dimension of observation vector INTEGER , INTENT ( in ) :: dim_obs !> Local dimension of observation vector INTEGER , INTENT ( out ) :: dim_obs_l ! ********************************************** ! *** Initialize local observation dimension *** ! ********************************************** CALL PDAFomi_init_dim_obs_l ( thisobs_l , thisobs , coords_l , & locweight , local_range , srange , dim_obs_l ) END SUBROUTINE init_dim_obs_l_ssh_mgrid !>###Routine to add model error. !> SUBROUTINE add_noise ( dim_obs_p , obs ) !> Number of process-local observations INTEGER , INTENT ( in ) :: dim_obs_p !> Process-local observations REAL , INTENT ( inout ) :: obs ( dim_obs_p ) !> Random noise REAL , ALLOCATABLE :: noise (:) !> Seed for random number generator INTEGER , SAVE :: iseed ( 4 ) !> Flag for first call LOGICAL , SAVE :: firststep = . TRUE . ! Seeds taken from PDAF Lorenz96 routine IF ( firststep ) THEN WRITE ( * , '(9x, a)' ) '--- Initialize seed for ssh_mgrid noise' iseed ( 1 ) = 2 * 220 + 1 iseed ( 2 ) = 2 * 100 + 5 iseed ( 3 ) = 2 * 10 + 7 iseed ( 4 ) = 2 * 30 + 9 firststep = . FALSE . END IF ! Generate random Gaussian noise ALLOCATE ( noise ( dim_obs_p )) CALL dlarnv ( 3 , iseed , dim_obs_p , noise ) obs = obs + ( noise_amp_ssh_mgrid * noise ) DEALLOCATE ( noise ) END SUBROUTINE add_noise END MODULE mod_obs_ssh_mgrid_pdafomi","tags":"","loc":"sourcefile/mod_obs_ssh_mgrid_pdafomi.f90.html"},{"title":"collect_state_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Subroutines collect_state_pdaf Source Code collect_state_pdaf.F90 Source Code !>##Collecting the statevector variables !>The routine has to initialize the statevector of PDAF !>from the fields of the model. !> !>The routine is executed by each process that is !>participating in the model integrations. !> !>**Calling Sequence** !> !> - Called from:* `PDAFomi_assimilate_local`/`mod_assimilation_pdaf` (as U_coll_state) !> SUBROUTINE collect_state_pdaf ( dim_p , state_p ) USE mod_kind_pdaf USE mod_statevector_pdaf , & ONLY : mpi_subd_lat , mpi_subd_lon , mpi_subd_vert , ssh_p_offset , & t_p_offset , s_p_offset , u_p_offset , v_p_offset USE dom_oce , & ONLY : nldj , nldi USE oce , & ONLY : sshb , tsb , ub , vb USE par_oce , & ONLY : jp_tem , jp_sal , jpi , jpj , jpk IMPLICIT NONE !> PE-local state dimension INTEGER , INTENT ( in ) :: dim_p !> PE-local state vector REAL ( pwp ), INTENT ( inout ) :: state_p ( dim_p ) !> Counters INTEGER :: i , j , k !> Start index for MPI subdomain INTEGER :: i0 , j0 ! Set the starting index after the halo region j0 = nldj - 1 i0 = nldi - 1 ! ********************************* ! Collect state vector 2d variables ! ********************************* ! SSH DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon state_p ( i + ( j - 1 ) * mpi_subd_lon + ssh_p_offset ) = & sshb ( i + i0 , j + j0 ) END DO END DO ! ********************************* ! Collect state vector 3d variables ! ********************************* ! T DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + t_p_offset ) = tsb ( i + i0 , & j + j0 , k , jp_tem ) END DO END DO END DO ! S DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + s_p_offset ) = tsb ( i + i0 , & j + j0 , k , jp_sal ) END DO END DO END DO ! U DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + u_p_offset ) = ub ( i + i0 , & j + j0 , k ) END DO END DO END DO ! V DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + v_p_offset ) = vb ( i + i0 , & j + j0 , k ) END DO END DO END DO END SUBROUTINE collect_state_pdaf","tags":"","loc":"sourcefile/collect_state_pdaf.f90.html"},{"title":"distribute_state_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Subroutines distribute_state_pdaf Source Code distribute_state_pdaf.F90 Source Code !>##Distributing the statevector variables, computing the !>##statevector increments !>This routine either initializes the full fields of !>the model from the statevector of PDAF (first timestep), !>or computes the statevector increments (all other timesteps). !>For all other timesteps, the increments are added to the !>model during the NEMO timestepping routine. See `mod_iau_pdaf` for details. !> !>The routine is executed by each process that is !>participating in the model integrations. !> !>**Calling Sequence** !> !> - Called from: `PDAF_get_state` (as U_dist_state) !> !> - Called from: `PDAFomi_assimilate_local` (as U_dist_state) !> SUBROUTINE distribute_state_pdaf ( dim_p , state_p ) USE mod_kind_pdaf USE mod_iau_pdaf , & ONLY : ssh_iau_pdaf , u_iau_pdaf , v_iau_pdaf , t_iau_pdaf , & s_iau_pdaf USE mod_statevector_pdaf , & ONLY : mpi_subd_lat , mpi_subd_lon , mpi_subd_vert , ssh_p_offset , & t_p_offset , s_p_offset , u_p_offset , v_p_offset USE dom_oce , & ONLY : nldj , nldi USE oce , & ONLY : sshb , tsb , ub , vb USE par_oce , & ONLY : jp_tem , jp_sal , jpi , jpj , jpk USE lbclnk , & ONLY : lbc_lnk , lbc_lnk_multi IMPLICIT NONE !> PE-local state dimension INTEGER , INTENT ( in ) :: dim_p !> PE-local state vector REAL ( pwp ), INTENT ( inout ) :: state_p ( dim_p ) !> Counters INTEGER :: i , j , k !> Start index for MPI subdomain INTEGER :: i0 , j0 !> Flag for first timestep LOGICAL :: firststep = . TRUE . ! ********************************************** ! Only distribute full state on first time step. ! Otherwise compute increment. ! ********************************************** ! Set the starting index after the halo region j0 = nldj - 1 i0 = nldi - 1 first : IF ( firststep ) THEN ! ************************************ ! Distribute state vector 2d variables ! ************************************ ! SSH DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon sshb ( i + i0 , j + j0 ) = state_p ( i + ( j - 1 ) * mpi_subd_lon & + ssh_p_offset ) END DO END DO ! Fill halo regions CALL lbc_lnk ( 'distribute_state_pdaf' , sshb , 'T' , 1. ) ! ************************************ ! Distribute state vector 3d variables ! ************************************ ! T DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon tsb ( i + i0 , j + j0 , k , jp_tem ) = state_p ( i + ( j - 1 ) * mpi_subd_lon & + ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + t_p_offset ) END DO END DO END DO ! S DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon tsb ( i + i0 , j + j0 , k , jp_sal ) = state_p ( i + ( j - 1 ) * mpi_subd_lon & + ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + s_p_offset ) END DO END DO END DO ! Fill halo regions CALL lbc_lnk_multi ( 'distribute_state_pdaf' , tsb (:, :, :, jp_tem ), 'T' , & 1. , tsb (:, :, :, jp_sal ), 'T' , 1. ) ! U DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon ub ( i + i0 , j + j0 , k ) = state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + u_p_offset ) END DO END DO END DO ! V DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon vb ( i + i0 , j + j0 , k ) = state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + v_p_offset ) END DO END DO END DO ! Fill halo regions CALL lbc_lnk_multi ( 'distribute_state_pdaf' , ub , 'U' , - 1. , vb , 'V' , - 1. ) firststep = . FALSE . ELSE first ! *********************************************** ! Compute increment for state vector 2d variables ! *********************************************** ! SSH DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon ssh_iau_pdaf ( i + i0 , j + j0 ) = & state_p ( i + ( j - 1 ) * mpi_subd_lon + ssh_p_offset ) - & sshb ( i + i0 , j + j0 ) END DO END DO ! Fill halo regions CALL lbc_lnk ( 'distribute_state_pdaf' , ssh_iau_pdaf , 'T' , 1. ) ! *********************************************** ! Compute increment for state vector 3d variables ! *********************************************** ! T DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon t_iau_pdaf ( i + i0 , j + j0 , k ) = & state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + t_p_offset ) - & tsb ( i + i0 , j + j0 , k , jp_tem ) END DO END DO END DO ! S DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon s_iau_pdaf ( i + i0 , j + j0 , k ) = & state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + s_p_offset ) - & tsb ( i + i0 , j + j0 , k , jp_sal ) END DO END DO END DO ! Fill halo regions CALL lbc_lnk_multi ( 'distribute_state_pdaf' , t_iau_pdaf (:, :, :), 'T' , & 1. , s_iau_pdaf (:, :, :), 'T' , 1. ) ! U DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon u_iau_pdaf ( i + i0 , j + j0 , k ) = & state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + u_p_offset ) - & ub ( i + i0 , j + j0 , k ) END DO END DO END DO ! V DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon v_iau_pdaf ( i + i0 , j + j0 , k ) = & state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + v_p_offset ) - & vb ( i + i0 , j + j0 , k ) END DO END DO END DO ! Fill halo regions CALL lbc_lnk_multi ( 'distribute_state_pdaf' , u_iau_pdaf , 'U' , - 1. , & v_iau_pdaf , 'V' , - 1. ) END IF first END SUBROUTINE distribute_state_pdaf","tags":"","loc":"sourcefile/distribute_state_pdaf.f90.html"},{"title":"mod_assimilation_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Modules mod_assimilation_pdaf Source Code mod_assimilation_pdaf.F90 Source Code !>##Assimilation Parameters !>This module provides variables needed for the !>assimilation. !> !>See `mod_init_pdaf` for where many of these !>variables are initialised. !> MODULE mod_assimilation_pdaf USE mod_kind_pdaf IMPLICIT NONE SAVE !> Global model state dimension INTEGER :: dim_state !> Model state dimension for PE-local domain INTEGER :: dim_state_p !> Process-local number of observations INTEGER :: dim_obs_p !> Vector holding process-local observations REAL ( pwp ), ALLOCATABLE :: obs_p (:) !> Vector holding state-vector indices of observations INTEGER , ALLOCATABLE :: obs_index_p (:) !> Vector holding full vector of observations REAL ( pwp ), ALLOCATABLE :: obs_f (:) !> Array for full observation coordinates REAL ( pwp ), ALLOCATABLE :: coords_obs_f (:, :) !> Vector holding local state-vector indices of observations INTEGER , ALLOCATABLE :: obs_index_l (:) !> Vector holding distances of local observations REAL ( pwp ), ALLOCATABLE :: distance_l (:) !> Control application of model error LOGICAL :: model_error !> Amplitude for model error REAL ( pwp ) :: model_err_amp !> time step interval between assimilation steps INTEGER :: delt_obs !> Number of observations INTEGER :: dim_obs !> Control verbosity of PDAF !> (0) no outputs, (1) progess info, (2) add timings !> (3) debugging output INTEGER :: screen !> Size of ensemble for SEIK/LSEIK/EnKF/ETKF !> Number of EOFs to be used for SEEK INTEGER :: dim_ens !> Select filter algorithm: !> SEEK (0), SEIK (1), EnKF (2), LSEIK (3), ETKF (4) !> LETKF (5), ESTKF (6), LESTKF (7), NETF (9), LNETF (10) INTEGER :: filtertype !> Subtype of filter algorithm !>   SEEK: !>     (0) evolve normalized modes !>     (1) evolve scaled modes with unit U !>     (2) fixed basis (V); variable U matrix !>     (3) fixed covar matrix (V,U kept static) !>   SEIK: !>     (0) ensemble forecast; new formulation !>     (1) ensemble forecast; old formulation !>     (2) fixed error space basis !>     (3) fixed state covariance matrix !>     (4) SEIK with ensemble transformation !>   EnKF: !>     (0) analysis for large observation dimension !>     (1) analysis for small observation dimension !>   LSEIK: !>     (0) ensemble forecast; !>     (2) fixed error space basis !>     (3) fixed state covariance matrix !>     (4) LSEIK with ensemble transformation !>   ETKF: !>     (0) ETKF using T-matrix like SEIK !>     (1) ETKF following Hunt et al. (2007) !>       There are no fixed basis/covariance cases, as !>       these are equivalent to SEIK subtypes 2/3 !>   LETKF: !>     (0) LETKF using T-matrix like SEIK !>     (1) LETKF following Hunt et al. (2007) !>       There are no fixed basis/covariance cases, as !>       these are equivalent to LSEIK subtypes 2/3 !>   ESTKF: !>     (0) standard ESTKF !>       There are no fixed basis/covariance cases, as !>       these are equivalent to SEIK subtypes 2/3 !>   LESTKF: !>     (0) standard LESTKF !>       There are no fixed basis/covariance cases, as !>       these are equivalent to LSEIK subtypes 2/3 !>   NETF: !>     (0) standard NETF !>   LNETF: !>     (0) standard LNETF INTEGER :: subtype !> Perform incremental updating in LSEIK INTEGER :: incremental !> Number of time instances for smoother INTEGER :: dim_lag !> Type of forgetting factor INTEGER :: type_forget !> Forgetting factor for filter analysis REAL ( pwp ) :: forget !> dimension of bias vector INTEGER :: dim_bias !> Interval to perform re-diagonalization in SEEK INTEGER :: int_rediag !> Epsilon for gradient approx. in SEEK forecast REAL ( pwp ) :: epsilon !> Rank to be considered for inversion of HPH INTEGER :: rank_analysis_enkf !> Type of ensemble transformation !> SEIK/LSEIK: !> (0) use deterministic omega !> (1) use random orthonormal omega orthogonal to (1,...,1)&#94;T !> (2) use product of (0) with random orthonormal matrix with !>     eigenvector (1,...,1)&#94;T !> ETKF/LETKF with subtype=4: !> (0) use deterministic symmetric transformation !> (2) use product of (0) with random orthonormal matrix with !>     eigenvector (1,...,1)&#94;T !> ESTKF/LESTKF: !> (0) use deterministic omega !> (1) use random orthonormal omega orthogonal to (1,...,1)&#94;T !> (2) use product of (0) with random orthonormal matrix with !>     eigenvector (1,...,1)&#94;T !> NETF/LNETF: !> (0) use random orthonormal transformation orthogonal to (1,...,1)&#94;T !> (1) use identity transformation !>    ! LSEIK/LETKF/LESTKF INTEGER :: type_trans !> Range for local observation domain - NEMO grid REAL ( pwp ) :: local_range !> Type of localizing weighting of observations !>   (0) constant weight of 1 !>   (1) exponentially decreasing with SRANGE !>   (2) use 5th-order polynomial !>   (3) regulated localization of R with mean error variance !>   (4) regulated localization of R with single-point error variance INTEGER :: locweight !> Support range for 5th order polynomial - NEMO grid !>   or radius for 1/e for exponential weighting !>    ! SEIK-subtype4/LSEIK-subtype4/ESTKF/LESTKF REAL ( pwp ) :: srange !> Type of the transform matrix square-root !> (0) symmetric square root, (1) Cholesky decomposition INTEGER :: type_sqrt !> file for ssh initial state estimate CHARACTER ( lc ) :: istate_ssh !> file for t initial state estimate CHARACTER ( lc ) :: istate_t !> file for t initial state estimate CHARACTER ( lc ) :: istate_s !> file for u initial state estimate CHARACTER ( lc ) :: istate_u !> file for v initial state estimate CHARACTER ( lc ) :: istate_v !> For SEIK: Definition of ensemble covar matrix !> (0): Factor (r+1)&#94;-1 (or N&#94;-1) !> (1): Factor r&#94;-1 (or (N-1)&#94;-1) - real ensemble covar. !> This setting is only for the model part; The definition !> of P has also to be specified in PDAF_filter_init. !> Only for upward-compatibility of PDAF INTEGER :: covartype !> model time REAL ( pwp ) :: time !> Coordinates of local analysis domain REAL ( pwp ) :: coords_l ( 2 ) !> Indices of local analysis domain REAL ( pwp ), ALLOCATABLE :: indx_dom_l (:, :) !> Ensure that the salinity is larger than this value REAL ( pwp ) :: salfixmin !$OMP THREADPRIVATE(coords_l) CONTAINS !>##Performing the Assimilation Step !>This routine is called during the model integrations at each timestep. !>It calls PDAF to check whether the forecast phase is completed and if !>so, PDAF will perform the analysis step. !> !>**Calling Sequence** !> !> - Called from: `step.F90' !> !> - Calls: `PDAFomi_assimilate_local` !> SUBROUTINE assimilate_pdaf () USE pdaf_interfaces_module , & ONLY : PDAFomi_assimilate_local , PDAF_get_localfilter USE mod_parallel_pdaf , & ONLY : mype_ens , abort_parallel !> PDAF status flag INTEGER :: status_pdaf !> Flag for domain-localized filter (1=true) INTEGER :: localfilter ! Collect a state vector from model fields EXTERNAL :: collect_state_pdaf ! Distribute a state vector to model fields EXTERNAL :: distribute_state_pdaf ! Provide time step of next observation EXTERNAL :: next_observation_pdaf ! User supplied pre/poststep routine EXTERNAL :: prepoststep_ens_pdaf ! Provide number of local analysis domains EXTERNAL :: init_n_domains_pdaf ! Initialize state dimension for local analysis domain EXTERNAL :: init_dim_l_pdaf ! Get state on local analysis domain from global state EXTERNAL :: g2l_state_pdaf ! Update global state from state on local analysis domain EXTERNAL :: l2g_state_pdaf ! Get dimension of full obs. vector for PE-local domain EXTERNAL :: init_dim_obs_pdafomi ! Obs. operator for full obs. vector for PE-local domain EXTERNAL :: obs_op_pdafomi ! Get dimension of obs. vector for local analysis domain EXTERNAL :: init_dim_obs_l_pdafomi ! ********************************* ! *** Call assimilation routine *** ! ********************************* ! Check  whether the filter is domain-localized CALL PDAF_get_localfilter ( localfilter ) IF ( localfilter == 1 ) THEN CALL PDAFomi_assimilate_local ( collect_state_pdaf , & distribute_state_pdaf , init_dim_obs_pdafomi , obs_op_pdafomi , & prepoststep_ens_pdaf , init_n_domains_pdaf , init_dim_l_pdaf , & init_dim_obs_l_pdafomi , g2l_state_pdaf , l2g_state_pdaf , & next_observation_pdaf , status_pdaf ) ELSE WRITE ( * , '(a)' ) 'ERROR - global filter not implemented, stopping.' CALL abort_parallel () END IF ! Check for errors during execution of PDAF IF ( status_pdaf /= 0 ) THEN WRITE ( * , '(/1x,a6,i3,a43,i4,a1/)' ) & 'ERROR ' , status_pdaf , & ' in PDAF_put_state - stopping! (PE ' , mype_ens , ')' CALL abort_parallel () END IF END SUBROUTINE assimilate_pdaf END MODULE mod_assimilation_pdaf","tags":"","loc":"sourcefile/mod_assimilation_pdaf.f90.html"},{"title":"init_dim_l_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Subroutines init_dim_l_pdaf Source Code init_dim_l_pdaf.F90 Source Code !>##Set dimension of local model state !>The routine is called during analysis step !>in `PDAF_X_update` in the loop over all local !>analysis domains. It has to set the dimension !>of the local model state on the current analysis !>domain. !> !> - Called from: `PDAFomi_assimilate_local`/`mod_assimilation_pdaf` SUBROUTINE init_dim_l_pdaf ( step , domain_p , dim_l ) USE mod_kind_pdaf USE mod_assimilation_pdaf , & ONLY : coords_l , indx_dom_l USE mod_statevector_pdaf , & ONLY : mpi_subd_vert , var2d_p_offset , var3d_p_offset USE mod_parallel_pdaf , & ONLY : abort_parallel USE dom_oce , & ONLY : nldj , nldi , tmask , glamt , gphit IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step !> Current local analysis domain INTEGER , INTENT ( in ) :: domain_p !> Local state dimension INTEGER , INTENT ( out ) :: dim_l !> Counters INTEGER :: idx , cnt !> Grid coordinates for local analysis domain INTEGER :: i , j !> Halo offset for local PE INTEGER :: i0 , j0 !> Longitude, latitude for local analysis domain REAL ( pwp ) :: lon , lat !> Degree to radian conversion REAL ( pwp ) :: rad_conv = 3.141592653589793 / 18 0.0 ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 ! Compute coordinates i = indx_dom_l ( 1 , domain_p ) j = indx_dom_l ( 2 , domain_p ) ! ********************************************** ! *** Initialize coordinates of local domain *** ! ********************************************** ! Use T-values to get local coordinates lat = gphit ( i + i0 , j + j0 ) lon = glamt ( i + i0 , j + j0 ) ! Convert local domain coordinates to radians (as required by PDAFOMI) coords_l ( 1 ) = lon * rad_conv coords_l ( 2 ) = lat * rad_conv ! **************************************** ! *** Initialize local state dimension *** ! **************************************** ! ************************************************************* ! dimension = (number of 2D state variables) + (number of 3D ! variables * number of ocean vertical points). ! ! We need to calculate the number of ocean vertical points ie ! we need to determine how many points in the vertical are ! ocean and how may are land (we do not include land points in ! our local state vector). ! ************************************************************* cnt = 0 DO idx = 1 , mpi_subd_vert IF ( tmask ( i + i0 , j + j0 , idx ) == 1.0_pwp ) cnt = cnt + 1 END DO ! on the local grid point (column), it has number of 2D variables + number of 3D variables dim_l = SIZE ( var2d_p_offset ) + ( SIZE ( var3d_p_offset ) * cnt ) END SUBROUTINE init_dim_l_pdaf","tags":"","loc":"sourcefile/init_dim_l_pdaf.f90.html"},{"title":"mod_iau_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Modules mod_iau_pdaf Source Code mod_iau_pdaf.F90 Source Code !>##Using the Incremental Analysis Method !>The material in this module is **heavily** based on a similar !>implementation for the NEMOVAR data assimilation system. Please !>refer to the `ASM` subdirectory in the `OCE` source code for !>precise details. !> MODULE mod_iau_pdaf USE mod_kind_pdaf USE par_oce , & ONLY : jpi , jpj , jpk , jpkm1 , jp_tem , jp_sal IMPLICIT NONE SAVE !> Array to store the ssh IAU REAL ( pwp ), DIMENSION (:, :), ALLOCATABLE :: ssh_iau_pdaf !> Array to store the T, S IAU REAL ( pwp ), DIMENSION (:, :, :), ALLOCATABLE :: t_iau_pdaf , s_iau_pdaf !> Array to store the U, V IAU REAL ( pwp ), DIMENSION (:, :, :), ALLOCATABLE :: u_iau_pdaf , v_iau_pdaf CONTAINS !>This routine initialises the arrays for the IAU. The arrays !>**must* be initialised to zero, as PDAF does not compute an !>analysis update for all gridpoints. (In this case, no increment !>should be added to the model at this gridpoint.) !> !>The arrays are deallocated in `finalise_pdaf`. SUBROUTINE asm_inc_init_pdaf () ! 2D variables ALLOCATE ( ssh_iau_pdaf ( jpi , jpj )) ssh_iau_pdaf = 0._pwp ! 3D variables ALLOCATE ( t_iau_pdaf ( jpi , jpj , jpk ), s_iau_pdaf ( jpi , jpj , jpk )) t_iau_pdaf = 0._pwp s_iau_pdaf = 0._pwp ALLOCATE ( u_iau_pdaf ( jpi , jpj , jpk ), v_iau_pdaf ( jpi , jpj , jpk )) u_iau_pdaf = 0._pwp v_iau_pdaf = 0._pwp END SUBROUTINE asm_inc_init_pdaf !>This routine is almost identical to a similar routine !>from NEMOVAR. It applies the IAU to the dynamical fields. !> !>*Called from:* `step.F90` SUBROUTINE dyn_asm_inc_pdaf ( kt ) USE mod_assimilation_pdaf , & ONLY : delt_obs USE in_out_manager , & ONLY : nit000 USE oce , & ONLY : ua , va !> Current time step INTEGER , INTENT ( IN ) :: kt !> Counter INTEGER :: jk ! Check whether to update the dynamic tendencies IF ( MOD ( kt - nit000 , delt_obs ) == 0 . AND . kt > nit000 ) THEN DO jk = 1 , jpkm1 ua (:, :, jk ) = ua (:, :, jk ) + u_iau_pdaf (:, :, jk ) va (:, :, jk ) = va (:, :, jk ) + v_iau_pdaf (:, :, jk ) END DO END IF END SUBROUTINE dyn_asm_inc_pdaf !>This routine is almost identical to a similar routine !>from NEMOVAR. It applies the IAU to the tracer fields. !> !>*Called from:* `step.F90` SUBROUTINE tra_asm_inc_pdaf ( kt ) USE mod_assimilation_pdaf , & ONLY : delt_obs , salfixmin USE eosbn2 , & ONLY : eos_fzp USE dom_oce , & ONLY : gdept_n USE in_out_manager , & ONLY : nit000 USE oce , & ONLY : tsn , tsa !> Current time step INTEGER , INTENT ( IN ) :: kt !> Counter INTEGER :: jk !> ! 3d freezing point values !> Nick: Taken from NEMOVAR. Will this lead to stack overflow? REAL ( pwp ), DIMENSION ( jpi , jpj , jpk ) :: fzptnz ! Freezing point calculation taken from oc_fz_pt (but calculated for ! all depths). Used to prevent the applied increments taking the ! temperature below the local freezing point. DO jk = 1 , jpkm1 CALL eos_fzp ( tsn (:, :, jk , jp_sal ), fzptnz (:, :, jk ), gdept_n (:, :, jk )) END DO ! Check whether to update the tracer tendencies IF ( MOD ( kt - nit000 , delt_obs ) == 0 . AND . kt > nit000 ) THEN ! Do not apply nonnegative increments. ! Do not apply increments if the temperature will fall below freezing ! or if the salinity will fall below a specified minimum value. DO jk = 1 , jpkm1 WHERE ( t_iau_pdaf (:, :, jk ) > 0.0_pwp . OR . & tsn (:, :, jk , jp_tem ) + tsa (:, :, jk , jp_tem ) + t_iau_pdaf (:, :, jk ) & > fzptnz (:, :, jk )) tsa (:, :, jk , jp_tem ) = tsa (:, :, jk , jp_tem ) + t_iau_pdaf (:, :, jk ) END WHERE WHERE ( s_iau_pdaf (:, :, jk ) > 0.0_pwp . OR . & tsn (:, :, jk , jp_sal ) + tsa (:, :, jk , jp_sal ) + s_iau_pdaf (:, :, jk ) & > salfixmin ) tsa (:, :, jk , jp_sal ) = tsa (:, :, jk , jp_sal ) + s_iau_pdaf (:, :, jk ) END WHERE END DO END IF END SUBROUTINE tra_asm_inc_pdaf !>This routine is almost identical to a similar routine !>from NEMOVAR. It applies the IAU to the ssh divergence term. !> !>Currently, the implementation is only valid when using the !>linear free surface assumption. !> !>*Called from:* `divhor.F90` SUBROUTINE ssh_asm_div_pdaf ( kt , phdivn ) USE mod_assimilation_pdaf , & ONLY : delt_obs USE mod_parallel_pdaf , & ONLY : abort_parallel USE dom_oce , & ONLY : e3t_n , ln_linssh , tmask USE in_out_manager , & ONLY : nit000 !> Current time step INTEGER , INTENT ( IN ) :: kt !> Horizontal divergence REAL ( pwp ), DIMENSION (:, :, :), INTENT ( inout ) :: phdivn ! Check whether to update the tracer tendencies IF ( MOD ( kt - nit000 , delt_obs ) == 0 . AND . kt > nit000 ) THEN ! NEMO-PDAF currently only implemented for linear free surface. IF ( ln_linssh ) THEN phdivn (:, :, 1 ) = phdivn (:, :, 1 ) - & ssh_iau_pdaf (:, :) / e3t_n (:, :, 1 ) * tmask (:, :, 1 ) ELSE WRITE ( * , '(/9x, a)' ) & 'NEMO-PDAF has not yet been implemented for nonlinear free & &surface (see ssh_asm_div_pdaf).' CALL abort_parallel () END IF END IF END SUBROUTINE ssh_asm_div_pdaf END MODULE mod_iau_pdaf","tags":"","loc":"sourcefile/mod_iau_pdaf.f90.html"},{"title":"g2l_state_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Subroutines g2l_state_pdaf Source Code g2l_state_pdaf.F90 Source Code !>##Restrict a model state to a local analysis domain !> !>The routine is called during the loop over all !>local analysis domains in `PDAF_X_update` !>before the analysis on a single local analysis !>domain. It has to initialize elements of the !>state vector for the local analysis domains from !>the PE-local full state vector. !> SUBROUTINE g2l_state_pdaf ( step , domain_p , dim_p , state_p , dim_l , state_l ) USE mod_kind_pdaf USE mod_parallel_pdaf , & ONLY : abort_parallel USE mod_assimilation_pdaf , & ONLY : indx_dom_l USE mod_statevector_pdaf , & ONLY : mpi_subd_vert , mpi_subd_lon , mpi_subd_lat , & var2d_p_offset , var3d_p_offset USE dom_oce , & ONLY : nldj , nldi , tmask IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step !> Current local analysis domain INTEGER , INTENT ( in ) :: domain_p !> PE-local full state dimension INTEGER , INTENT ( in ) :: dim_p !> Local state dimension INTEGER , INTENT ( in ) :: dim_l !> PE-local full state vector REAL ( pwp ), INTENT ( in ) :: state_p ( dim_p ) !> State vector on local analysis domain REAL ( pwp ), INTENT ( out ) :: state_l ( dim_l ) !> Counter INTEGER :: idx !> Halo offset for local PE INTEGER :: i0 , j0 !> Grid coordinates for local analysis domain INTEGER :: i , j !> Array for converting vertical coordinate in local statevector INTEGER :: vcoord_conv ( 100 ) !> Number of vertical ocean points in local statevector INTEGER :: dim_vert_l !> 2D state variable coordinate in statevector INTEGER :: loc_2dvar !> Variables for 3D state variable index INTEGER :: a , b , c ! ******************************************************* ! *** Initialise vertical coordinate conversion array *** ! ******************************************************* IF ( SIZE ( vcoord_conv ) < mpi_subd_vert ) THEN WRITE ( * , '(/1x,a59/)' ) & 'ERROR: automatic array v_coord in g2l_state_pdaf too small ' CALL abort_parallel () END IF vcoord_conv = 0 dim_vert_l = 0 ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 ! Compute coordinates i = indx_dom_l ( 1 , domain_p ) j = indx_dom_l ( 2 , domain_p ) ! Count number of ocean vertical points DO idx = 1 , mpi_subd_vert IF ( tmask ( i + i0 , j + j0 , idx ) == 1.0_pwp ) THEN dim_vert_l = dim_vert_l + 1 vcoord_conv ( dim_vert_l ) = idx END IF END DO ! ************************************* ! *** Initialize local state vector *** ! ************************************* ! ********************************************************** ! A local domain consists of all ocean points in a vertical ! column. Such a domain will have coordinates (x,y,:). ! The 2d state variables in the global statevector will be ! located at ( (y-1)*dim_longitude ) + x + 2d_offset. ! The 3d state variables in the global statevector will be ! located at ( (z-1)*dim_longitude*dim_latitude ) + ! ( (y-1)*dim_longitude ) + x + 3d_offset, where z can vary ! over all *ocean* points in the vertical column. ! ********************************************************** ! Compute location of 2d variables in statevector loc_2dvar = ( j - 1 ) * ( mpi_subd_lon ) + i DO idx = 1 , dim_l ! Compute 2d state variables IF ( idx <= SIZE ( var2d_p_offset )) THEN state_l ( idx ) = state_p ( loc_2dvar + var2d_p_offset ( idx )) ELSE ! Compute 3d state variables a = idx - SIZE ( var2d_p_offset ) b = MOD ( a - 1 , dim_vert_l ) + 1 c = ( a - b ) / dim_vert_l + 1 state_l ( idx ) = state_p ( mpi_subd_lon * mpi_subd_lat * ( vcoord_conv ( b ) - 1 ) & + loc_2dvar + var3d_p_offset ( c )) END IF END DO END SUBROUTINE g2l_state_pdaf","tags":"","loc":"sourcefile/g2l_state_pdaf.f90.html"},{"title":"init_n_domains_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Subroutines init_n_domains_pdaf Source Code init_n_domains_pdaf.F90 Source Code !>##Set number of local analysis domains !>The routine is called in `PDAF_X_update` !>at the beginning of the analysis step before !>the loop through all local analysis domains. !>It has to set the number of local analysis !>domains for the PE-local domain. !> !> - Called from: `PDAFomi_assimilate_local`/`mod_assimilation_pdaf` SUBROUTINE init_n_domains_pdaf ( step , n_domains_p ) USE mod_kind_pdaf USE mod_assimilation_pdaf , & ONLY : indx_dom_l USE mod_statevector_pdaf , & ONLY : mpi_subd_lon , mpi_subd_lat USE mod_parallel_pdaf , & ONLY : mype_filter USE dom_oce , & ONLY : nldi , nldj , tmask IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step !> PE-local number of analysis domains INTEGER , INTENT ( out ) :: n_domains_p !> Counters INTEGER :: i , j , cnt !> Halo offset INTEGER :: i0 , j0 ! ************************************ ! *** Initialize number of domains *** ! ************************************ ! ******************************************* ! ! The number of local domains is defined as ! the number of grid points at the surface ! where tmask is 1 ie horizontal localization ! is used, and land points are ignored. ! ! ******************************************* n_domains_p = 0 ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon IF ( tmask ( i + i0 , j + j0 , 1 ) == 1.0_pwp ) THEN n_domains_p = n_domains_p + 1 END IF END DO END DO ! ******************************************* ! *** Store local domain i,j index values *** ! ******************************************* ! ****************************************** ! ! Important to consider case where there ! are no valid local domains. In this ! case we use a hack. We set the number ! of local domains (n_domains_p) to 1, ! and set the dimension of the observations ! for this local domain to zero (see ! init_dim_obs_l). This hack means no update ! will be performed on this local domain. ! ! ****************************************** IF ( n_domains_p > 0 ) THEN ! Deallocated in deallocate_obs_pdafomi IF (. NOT . ALLOCATED ( indx_dom_l )) ALLOCATE ( indx_dom_l ( 2 , n_domains_p )) cnt = 0 DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon IF ( tmask ( i + i0 , j + j0 , 1 ) == 1.0_pwp ) THEN cnt = cnt + 1 indx_dom_l ( 1 , cnt ) = i indx_dom_l ( 2 , cnt ) = j END IF END DO END DO ELSE WRITE ( * , '(8x,a,i3)' ) 'WARNING: No valid local domains, PE=' , mype_filter n_domains_p = 1 ! Deallocated in deallocate_obs_pdafomi IF (. NOT . ALLOCATED ( indx_dom_l )) ALLOCATE ( indx_dom_l ( 2 , 1 )) indx_dom_l ( 1 , 1 ) = 1 indx_dom_l ( 2 , 1 ) = 1 END IF END SUBROUTINE init_n_domains_pdaf","tags":"","loc":"sourcefile/init_n_domains_pdaf.f90.html"},{"title":"mod_statevector_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Modules mod_statevector_pdaf Source Code mod_statevector_pdaf.F90 Source Code !>##Building the Statevector !>This module provides variables & routines for !>building the state vector. !> MODULE mod_statevector_pdaf USE mod_kind_pdaf IMPLICIT NONE SAVE !> 2d statevector variables - start index INTEGER :: ssh_p_offset !> Array holding 2d state variable offsets INTEGER :: var2d_p_offset ( 1 ) !> 2d statevector variables - dimension size INTEGER :: ssh_p_dim !> 3d statevector variables - start index for t INTEGER :: t_p_offset !> 3d statevector variable - start index for s INTEGER :: s_p_offset !> 3d statevector variable - start index for u INTEGER :: u_p_offset !> 3d statevector variable - start index for v INTEGER :: v_p_offset !> Array holding 3d state variable offsets !> index i is the i-th variable INTEGER :: var3d_p_offset ( 4 ) !> 3d statevector variables (t) - dimension size INTEGER :: t_p_dim !> 3d statevector variables (s) - dimension size INTEGER :: s_p_dim !> 3d statevector variables (u) - dimension size INTEGER :: u_p_dim !> 3d statevector variables (v) - dimension size INTEGER :: v_p_dim !> Dimensions for MPI subdomain that is included !> in local statevector. Necessary so that halo !> regions are not included in multiple local !> statevectors !> size of local lat domain excluding halo region INTEGER :: mpi_subd_lat !> size of local lon domain excluding halo region INTEGER :: mpi_subd_lon !> size of local vertical domain INTEGER :: mpi_subd_vert CONTAINS !>##This routine calculates the dimensions of the MPI subdomain !> that is used to fill the local statevector. !> !> **Calling Sequence** !> !> - Called from: `calc_statevar_dim` SUBROUTINE calc_mpi_dim () USE par_oce , ONLY : jpk USE dom_oce , ONLY : nldi , nldj , nlei , nlej mpi_subd_lon = nlei - nldi + 1 mpi_subd_lat = nlej - nldj + 1 mpi_subd_vert = jpk END SUBROUTINE calc_mpi_dim !>##This routine calculates the dimension of each of the local !> statevector variables. !> !> **Calling Sequence** !> !> - Called from: `calc_offset` !> !> - Calls: `calc_mpi_dim` SUBROUTINE calc_statevar_dim () ! Compute MPI subdomain dimensions CALL calc_mpi_dim () ssh_p_dim = mpi_subd_lat * mpi_subd_lon t_p_dim = mpi_subd_lat * mpi_subd_lon * mpi_subd_vert s_p_dim = mpi_subd_lat * mpi_subd_lon * mpi_subd_vert u_p_dim = mpi_subd_lat * mpi_subd_lon * mpi_subd_vert v_p_dim = mpi_subd_lat * mpi_subd_lon * mpi_subd_vert END SUBROUTINE calc_statevar_dim !>##This routine calculates the offset values for each of the !> local statevector variables. !> !> It then stores the 2d/3d offset values in separate arrays. !> !> **Calling Sequence** !> !> - Called from: `calc_statevector_dim` !> !> - Calls: `calc_statevar_dim` SUBROUTINE calc_offset () ! Compute local statevector dimensions CALL calc_statevar_dim () ssh_p_offset = 0 t_p_offset = ssh_p_offset + ssh_p_dim s_p_offset = t_p_offset + t_p_dim u_p_offset = s_p_offset + s_p_dim v_p_offset = u_p_offset + u_p_dim ! Fill array of 2D state variable offsets for local PE var2d_p_offset ( 1 ) = ssh_p_offset ! Fill array of 3D state variable offsets for local PE var3d_p_offset ( 1 ) = t_p_offset var3d_p_offset ( 2 ) = s_p_offset var3d_p_offset ( 3 ) = u_p_offset var3d_p_offset ( 4 ) = v_p_offset END SUBROUTINE calc_offset !>##This routine calculates the dimension of the local statevector. !> !> **Calling Sequence** !> !> - Called from: `init_pdaf` !> !> - Calls: `calc_offset` SUBROUTINE calc_statevector_dim ( dim_p ) !> Local statevector dimension INTEGER , INTENT ( inout ) :: dim_p ! Calculate statevector variable offset and dimension. ! *DO NOT REMOVE* as offset is not calculated anywhere else. CALL calc_offset () dim_p = ssh_p_dim + t_p_dim + s_p_dim + u_p_dim + v_p_dim END SUBROUTINE calc_statevector_dim !>##Fill local ensemble array with 2d state variables from !> initial state file. !> !> @todo !> User should give values to the statevector !> for the DA. !> @endtodo SUBROUTINE fill2d_ensarray ( fname , ens_p ) USE netcdf !> Name of netCDF file CHARACTER ( lc ), INTENT ( in ) :: fname !> PE-local state ensemble REAL ( pwp ), INTENT ( inout ) :: ens_p (:, :) WRITE ( * , '(/1x,a,a)' ) '2D initial state file =' , TRIM ( fname ) WRITE ( * , '(/1x,a)' ) 'Insert routine for reading 2D initial state here.' END SUBROUTINE fill2d_ensarray !>##Fill local ensemble array with 3d state variables from !> initial state file. !> !> @todo !> User should give values to the statevector !> for the DA. !> @endtodo SUBROUTINE fill3d_ensarray ( fname , statevar , ens_p ) USE netcdf !> Name of netCDF file CHARACTER ( lc ), INTENT ( in ) :: fname !> Name of state variable CHARACTER ( len = 1 ), INTENT ( in ) :: statevar !> PE-local state ensemble REAL ( pwp ), INTENT ( inout ) :: ens_p (:, :) SELECT CASE ( statevar ) CASE ( 'T' ) WRITE ( * , '(/1x,a,a)' ) 'T initial state file =' , TRIM ( fname ) WRITE ( * , '(/1x,a)' ) 'Insert routine for reading T initial state here.' CASE ( 'S' ) WRITE ( * , '(/1x,a,a)' ) 'S initial state file =' , TRIM ( fname ) WRITE ( * , '(/1x,a)' ) 'Insert routine for reading S initial state here.' CASE ( 'U' ) WRITE ( * , '(/1x,a,a)' ) 'U initial state file =' , TRIM ( fname ) WRITE ( * , '(/1x,a)' ) 'Insert routine for reading U initial state here.' CASE ( 'V' ) WRITE ( * , '(/1x,a,a)' ) 'V initial state file =' , TRIM ( fname ) WRITE ( * , '(/1x,a)' ) 'Insert routine for reading V initial state here.' END SELECT END SUBROUTINE fill3d_ensarray END MODULE mod_statevector_pdaf","tags":"","loc":"sourcefile/mod_statevector_pdaf.f90.html"},{"title":"next_observation_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Subroutines next_observation_pdaf Source Code next_observation_pdaf.F90 Source Code !>##Determining the Next Analysis Step !> !>The subroutine is called before each forecast phase !>by `PDAF_get_state`. It has to initialize the number !>of time steps until the next available observation !>(`nsteps`). It indicates if the data assimilation process !>is completed such that the ensemble loop in the model !>routine can be exited. !> !>The routine is called by all processes. !> !> **Calling Sequence** !> !> - Called from: `init_pdaf/PDAF_get_state` (as U_next_obs) !> SUBROUTINE next_observation_pdaf ( stepnow , nsteps , doexit , time ) USE mod_kind_pdaf USE mod_assimilation_pdaf , & ONLY : delt_obs USE mod_parallel_pdaf , & ONLY : mype_ens USE in_out_manager , & ONLY : nitend IMPLICIT NONE !> Number of the current time step INTEGER , INTENT ( in ) :: stepnow !> Number of time steps until next obs INTEGER , INTENT ( out ) :: nsteps !> Whether to exit forecasting (1 for exit) INTEGER , INTENT ( out ) :: doexit !> Current model (physical) time REAL ( pwp ), INTENT ( out ) :: time ! ******************************************************* ! *** Set number of time steps until next observation *** ! ******************************************************* ! Not used in this implementation time = 0.0 IF ( stepnow + delt_obs <= nitend ) THEN ! *** During the assimilation process *** nsteps = delt_obs ! This assumes a constant time step interval doexit = 0 ! Not used in this implementation IF ( mype_ens == 0 ) WRITE ( * , '(i7, 3x, a, i7)' ) & stepnow , 'Next observation at time step' , stepnow + nsteps ELSE ! *** End of assimilation process *** nsteps = 0 ! No more steps doexit = 1 ! Not used in this implementation IF ( mype_ens == 0 ) WRITE ( * , '(i7, 3x, a)' ) & stepnow , 'No more observations - end assimilation' END IF END SUBROUTINE next_observation_pdaf","tags":"","loc":"sourcefile/next_observation_pdaf.f90.html"},{"title":"mod_parallel_pdaf.F90 – NEMO4 PDAFOMI","text":"Contents Modules mod_parallel_pdaf Source Code mod_parallel_pdaf.F90 Source Code !>##Setup parallelisation !>This modules provides variables for the MPI parallelization !>to be shared between model-related routines. There are variables !>that are used in the model even without PDAF, and additional variables !>that are only used if data assimilaion with PDAF is performed. !>The initialization of communicators for execution with PDAF is !>performed in `init_parallel_pdaf`. !> MODULE mod_parallel_pdaf USE mod_kind_pdaf IMPLICIT NONE SAVE INCLUDE 'mpif.h' !> MPI communicator for model tasks INTEGER :: COMM_model !> Rank and size in COMM_model INTEGER :: mype_model , npes_model !> Communicator for entire ensemble INTEGER :: COMM_ensemble !> Rank and size in COMM_ensemble INTEGER :: mype_ens , npes_ens !> Number of parallel model tasks INTEGER :: n_modeltasks = 1 !> MPI communicator for filter PEs INTEGER :: COMM_filter !> rank and size in COMM_filter INTEGER :: mype_filter , npes_filter !> MPI communicator for coupling filter and model INTEGER :: COMM_couple !> rank and size in COMM_couple INTEGER :: mype_couple , npes_couple !> Whether we are on a PE in a COMM_filter LOGICAL :: filterpe !> Index of my model task (1,...,n_modeltasks) INTEGER :: task_id !> Error flag for MPI INTEGER :: MPIerr !> Status array for MPI INTEGER :: MPIstatus ( MPI_STATUS_SIZE ) !> Option for screen output INTEGER :: screen_parallel = 1 !> # PEs per ensemble INTEGER , ALLOCATABLE :: local_npes_model (:) CONTAINS !>##Terminate the MPI execution environment. SUBROUTINE abort_parallel () CALL MPI_Abort ( MPI_COMM_WORLD , 1 , MPIerr ) END SUBROUTINE abort_parallel !>##Split the MPI communicator initialised by XIOS into MODEL, !> FILTER and COUPLE communicators, return MODEL communicator. !> !> **Calling Sequence** !> !>  - Called from `lib_mpp.F90` !> !>  - Calls:  `MPI_Comm_size`, `MPI_Comm_rank` !> `MPI_Comm_split`, `MPI_Barrier` !> SUBROUTINE init_parallel_pdaf ( screen , mpi_comm ) !> Whether screen information is shown INTEGER , INTENT ( in ) :: screen !> Communicator after XIOS splitting INTEGER , INTENT ( inout ) :: mpi_comm !> Counters INTEGER :: i , j !> Index of PE INTEGER :: pe_index !> Variables for communicator-splitting INTEGER :: my_color , color_couple !> Number of model tasks INTEGER :: tasks !> namelist file CHARACTER ( lc ) :: nmlfile ! Number of ensemble members, supplied by PDAF namelist NAMELIST / tasks_nml / tasks ! Read namelist for number of model tasks nmlfile = 'namelist.pdaf' OPEN ( 20 , file = nmlfile ) READ ( 20 , NML = tasks_nml ) CLOSE ( 20 ) n_modeltasks = tasks ! ***              COMM_ENSEMBLE                *** ! *** Generate communicator for ensemble runs   *** ! *** only used to generate model communicators *** COMM_ensemble = mpi_comm CALL MPI_Comm_Size ( COMM_ensemble , npes_ens , MPIerr ) CALL MPI_Comm_Rank ( COMM_ensemble , mype_ens , MPIerr ) ! Initialize communicators for ensemble evaluations IF ( mype_ens == 0 ) THEN WRITE ( * , '(/1x, a)' ) 'Initialize communicators for assimilation with PDAF' END IF ! Store # PEs per ensemble member. Used for info on PE 0 and for ! generation of model communicators on other PEs ALLOCATE ( local_npes_model ( n_modeltasks )) local_npes_model = FLOOR ( REAL ( npes_ens ) / REAL ( n_modeltasks )) DO i = 1 , ( npes_ens - n_modeltasks * local_npes_model ( 1 )) local_npes_model ( i ) = local_npes_model ( i ) + 1 END DO ! ***              COMM_MODEL               *** ! *** Generate communicators for model runs *** pe_index = 0 doens1 : DO i = 1 , n_modeltasks DO j = 1 , local_npes_model ( i ) IF ( mype_ens == pe_index ) THEN task_id = i EXIT doens1 END IF pe_index = pe_index + 1 END DO END DO doens1 CALL MPI_Comm_split ( COMM_ensemble , task_id , mype_ens , & COMM_model , MPIerr ) ! Re-initialize PE information according to model communicator CALL MPI_Comm_Size ( COMM_model , npes_model , MPIerr ) CALL MPI_Comm_Rank ( COMM_model , mype_model , MPIerr ) IF ( screen > 1 ) then WRITE ( * , * ) 'MODEL: mype(w)= ' , mype_ens , '; model task: ' , task_id , & '; mype(m)= ' , mype_model , '; npes(m)= ' , npes_model END IF ! Init flag FILTERPE (all PEs of model task 1) IF ( task_id == 1 ) THEN filterpe = . TRUE . ELSE filterpe = . FALSE . END IF ! ***         COMM_FILTER                 *** ! *** Generate communicator for filter    *** IF ( filterpe ) THEN my_color = task_id ELSE my_color = MPI_UNDEFINED END IF CALL MPI_Comm_split ( COMM_ensemble , my_color , mype_ens , & COMM_filter , MPIerr ) ! Initialize PE information according to filter communicator IF ( filterpe ) THEN CALL MPI_Comm_Size ( COMM_filter , npes_filter , MPIerr ) CALL MPI_Comm_Rank ( COMM_filter , mype_filter , MPIerr ) END IF ! ***              COMM_COUPLE                 *** ! *** Generate communicators for communication *** ! *** between model and filter PEs             *** color_couple = mype_model + 1 CALL MPI_Comm_split ( COMM_ensemble , color_couple , mype_ens , & COMM_couple , MPIerr ) ! Initialize PE information according to coupling communicator CALL MPI_Comm_Size ( COMM_couple , npes_couple , MPIerr ) CALL MPI_Comm_Rank ( COMM_couple , mype_couple , MPIerr ) IF ( screen > 0 ) THEN IF ( mype_ens == 0 ) THEN WRITE ( * , '(/18x, a)' ) 'PE configuration:' WRITE ( * , '(2x, a6, a9, a10, a14, a13, /2x, a5, a9, a7, a7, a7, a7, a7, /2x, a)' ) & 'world' , 'filter' , 'model' , 'couple' , 'filterPE' , & 'rank' , 'rank' , 'task' , 'rank' , 'task' , 'rank' , 'T/F' , & '----------------------------------------------------------' END IF CALL MPI_Barrier ( COMM_ensemble , MPIerr ) IF ( task_id == 1 ) THEN WRITE ( * , '(2x, i4, 4x, i4, 4x, i3, 4x, i3, 4x, i3, 4x, i3, 5x, l3)' ) & mype_ens , mype_filter , task_id , mype_model , color_couple , & mype_couple , filterpe END IF IF ( task_id > 1 ) THEN WRITE ( * , '(2x, i4, 12x, i3, 4x, i3, 4x, i3, 4x, i3, 5x, l3)' ) & mype_ens , task_id , mype_model , color_couple , mype_couple , filterpe END IF CALL MPI_Barrier ( COMM_ensemble , MPIerr ) IF ( mype_ens == 0 ) WRITE ( * , '(/a)' ) '' END IF ! **************************************************** ! *** Re-initialize model equivalent to COMM_model *** ! **************************************************** mpi_comm = COMM_model END SUBROUTINE init_parallel_pdaf END MODULE mod_parallel_pdaf","tags":"","loc":"sourcefile/mod_parallel_pdaf.f90.html"},{"title":"prepoststep_ens_pdaf – NEMO4 PDAFOMI","text":"subroutine prepoststep_ens_pdaf(step, dim_p, dim_ens, dim_ens_p, dim_obs_p, state_p, Uinv, ens_p, flag) Uses mod_kind_pdaf Controlling Pre- and Post-Processing of the PDAF output For global filters (e.g. SEIK), the routine is called\nbefore the analysis and after the ensemble transformation. For local filters (e.g. LSEIK), the routine is called\nbefore and after the loop over all local analysis\ndomains. The routine provides full access to the state\nestimate and the state ensemble to the user.\nThus, user-controlled pre- and poststep\noperations can be performed here. For example\nthe forecast and the analysis states and ensemble\ncovariance matrix can be analyzed, e.g. by\ncomputing the estimated variances.\nFor the offline mode, this routine is the place\nin which the writing of the analysis ensemble\ncan be performed. If a user considers to perform adjustments to the\nestimates (e.g. for balances), this routine is\nthe right place for it. Calling Sequence Called by: PDAF_get_state (as U_prepoststep) PDAF_X_update (as U_prepoststep) Arguments Type Intent Optional Attributes Name integer, intent(in) :: step Current time step (negative for call after forecast) integer, intent(in) :: dim_p PE-local state dimension integer, intent(in) :: dim_ens Size of state ensemble integer, intent(in) :: dim_ens_p PE-local size of ensemble integer, intent(in) :: dim_obs_p PE-local dimension of observation vector real(kind=pwp), intent(inout) :: state_p (dim_p) PE-local forecast/analysis state\n The array ‘state_p’ is already initialised and can be used\n freely here (not for SEEK!) real(kind=pwp), intent(inout) :: Uinv (dim_ens-1,dim_ens-1) Inverse of matrix U real(kind=pwp), intent(inout) :: ens_p (dim_p,dim_ens) PE-local state ensemble integer, intent(in) :: flag PDAF status flag Contents Source Code prepoststep_ens_pdaf Source Code SUBROUTINE prepoststep_ens_pdaf ( step , dim_p , dim_ens , dim_ens_p , dim_obs_p , & state_p , Uinv , ens_p , flag ) USE mod_kind_pdaf IMPLICIT NONE !> Current time step (negative for call after forecast) INTEGER , INTENT ( in ) :: step !> PE-local state dimension INTEGER , INTENT ( in ) :: dim_p !> Size of state ensemble INTEGER , INTENT ( in ) :: dim_ens !> PE-local size of ensemble INTEGER , INTENT ( in ) :: dim_ens_p !> PE-local dimension of observation vector INTEGER , INTENT ( in ) :: dim_obs_p !> PE-local forecast/analysis state !> The array 'state_p' is already initialised and can be used !> freely here (not for SEEK!) REAL ( pwp ), INTENT ( inout ) :: state_p ( dim_p ) !> Inverse of matrix U REAL ( pwp ), INTENT ( inout ) :: Uinv ( dim_ens - 1 , dim_ens - 1 ) !> PE-local state ensemble REAL ( pwp ), INTENT ( inout ) :: ens_p ( dim_p , dim_ens ) !> PDAF status flag INTEGER , INTENT ( in ) :: flag ! Deallocate observation arrays EXTERNAL :: deallocate_obs_pdafomi WRITE ( * , '(/1x,a)' ) 'Insert prepoststep routines here. ' ! Deallocate observation arrays - DO NOT REMOVE CALL deallocate_obs_pdafomi ( step ) END SUBROUTINE prepoststep_ens_pdaf","tags":"","loc":"proc/prepoststep_ens_pdaf.html"},{"title":"l2g_state_pdaf – NEMO4 PDAFOMI","text":"subroutine l2g_state_pdaf(step, domain_p, dim_l, state_l, dim_p, state_p) Uses mod_kind_pdaf mod_parallel_pdaf mod_assimilation_pdaf mod_statevector_pdaf dom_oce Initialize full state from local analysis The routine is called during the loop over all\nlocal analysis domains in PDAF_X_update after the analysis and ensemble transformation\non a single local analysis domain. It has to\ninitialize elements of the PE-local full state\nvector from the provided analysis state vector\non the local analysis domain. Arguments Type Intent Optional Attributes Name integer, intent(in) :: step Current time step integer, intent(in) :: domain_p Current local analysis domain integer, intent(in) :: dim_l PE-local full state dimension real(kind=pwp), intent(in) :: state_l (dim_l) PE-local full state vector integer, intent(in) :: dim_p Local state dimension real(kind=pwp), intent(out) :: state_p (dim_p) State vector on local analysis domain Contents Variables a b c dim_vert_l i i0 idx j j0 loc_2dvar vcoord_conv Source Code l2g_state_pdaf Variables Type Visibility Attributes Name Initial integer, public :: a Variables for 3D state variable index integer, public :: b Variables for 3D state variable index integer, public :: c Variables for 3D state variable index integer, public :: dim_vert_l Number of vertical ocean points in local statevector integer, public :: i Grid coordinates for local analysis domain integer, public :: i0 Halo offset for local PE integer, public :: idx Counter integer, public :: j Grid coordinates for local analysis domain integer, public :: j0 Halo offset for local PE integer, public :: loc_2dvar 2D state variable coordinate in statevector integer, public :: vcoord_conv (100) Array for converting vertical coordinate in local statevector Source Code SUBROUTINE l2g_state_pdaf ( step , domain_p , dim_l , state_l , dim_p , state_p ) USE mod_kind_pdaf USE mod_parallel_pdaf , & ONLY : abort_parallel USE mod_assimilation_pdaf , & ONLY : indx_dom_l USE mod_statevector_pdaf , & ONLY : mpi_subd_vert , mpi_subd_lon , mpi_subd_lat , & var2d_p_offset , var3d_p_offset USE dom_oce , & ONLY : nldj , nldi , tmask IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step !> Current local analysis domain INTEGER , INTENT ( in ) :: domain_p !> PE-local full state dimension INTEGER , INTENT ( in ) :: dim_l !> Local state dimension INTEGER , INTENT ( in ) :: dim_p !> PE-local full state vector REAL ( pwp ), INTENT ( in ) :: state_l ( dim_l ) !> State vector on local analysis domain REAL ( pwp ), INTENT ( out ) :: state_p ( dim_p ) !> Counter INTEGER :: idx !> Halo offset for local PE INTEGER :: i0 , j0 !> Grid coordinates for local analysis domain INTEGER :: i , j !> Array for converting vertical coordinate in local statevector INTEGER :: vcoord_conv ( 100 ) !> Number of vertical ocean points in local statevector INTEGER :: dim_vert_l !> 2D state variable coordinate in statevector INTEGER :: loc_2dvar !> Variables for 3D state variable index INTEGER :: a , b , c ! ******************************************************* ! *** Initialise vertical coordinate conversion array *** ! ******************************************************* IF ( SIZE ( vcoord_conv ) < mpi_subd_vert ) THEN WRITE ( * , '(/1x,a59/)' ) & 'ERROR: automatic array v_coord in l2g_state_pdaf too small ' CALL abort_parallel () END IF vcoord_conv = 0 dim_vert_l = 0 ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 ! Compute coordinates i = indx_dom_l ( 1 , domain_p ) j = indx_dom_l ( 2 , domain_p ) ! Count number of ocean vertical points DO idx = 1 , mpi_subd_vert IF ( tmask ( i + i0 , j + j0 , idx ) == 1.0_pwp ) THEN dim_vert_l = dim_vert_l + 1 vcoord_conv ( dim_vert_l ) = idx END IF END DO ! ************************************* ! *** Initialize local state vector *** ! ************************************* ! ********************************************************** ! A local domain consists of all ocean points in a vertical ! column. Such a domain will have coordinates (x,y,:). ! The 2d state variables in the global statevector will be ! located at ( (y-1)*dim_longitude ) + x + 2d_offset. ! The 3d state variables in the global statevector will be ! located at ( (z-1)*dim_longitude*dim_latitude ) + ! ( (y-1)*dim_longitude ) + x + 3d_offset, where z can vary ! over all *ocean* points in the vertical column. ! ********************************************************** ! Compute location of 2d variables in statevector loc_2dvar = ( j - 1 ) * ( mpi_subd_lon ) + i DO idx = 1 , dim_l ! Compute 2d state variables IF ( idx <= SIZE ( var2d_p_offset )) THEN state_p ( loc_2dvar + var2d_p_offset ( idx )) = state_l ( idx ) ELSE ! Compute 3d state variables a = idx - SIZE ( var2d_p_offset ) b = MOD ( a - 1 , dim_vert_l ) + 1 c = ( a - b ) / dim_vert_l + 1 state_p ( mpi_subd_lon * mpi_subd_lat * ( vcoord_conv ( b ) - 1 ) & + loc_2dvar + var3d_p_offset ( c )) = state_l ( idx ) END IF END DO END SUBROUTINE l2g_state_pdaf","tags":"","loc":"proc/l2g_state_pdaf.html"},{"title":"init_dim_obs_pdafomi – NEMO4 PDAFOMI","text":"subroutine init_dim_obs_pdafomi(step, dim_obs) Uses mod_kind_pdaf mod_obs_ssh_mgrid_pdafomi PDAFOMI interface routines This file provides interface routines between the call-back routines\nof PDAF and the observation-specific routines in PDAFOMI. This structure\ncollects all calls to observation-specifc routines in this single file\nto make it easier to find the routines that need to be adapted. The routines here are mainly pure pass-through routines. Thus they\nsimply call one of the routines from PDAF-OMI. Initialise the dimension of observations Calling Sequence Called by: mod_assimilation_pdaf (as U_prepoststep ) Calls: init_dim_obs_ssh_mgrid Arguments Type Intent Optional Attributes Name integer, intent(in) :: step Current time step integer, intent(out) :: dim_obs Dimension of full observation vector Contents Variables dim_obs_ssh_mgrid Source Code init_dim_obs_pdafomi Variables Type Visibility Attributes Name Initial integer, public :: dim_obs_ssh_mgrid Observation dimensions Source Code SUBROUTINE init_dim_obs_pdafomi ( step , dim_obs ) USE mod_kind_pdaf USE mod_obs_ssh_mgrid_pdafomi , & ONLY : assim_ssh_mgrid , init_dim_obs_ssh_mgrid IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step !> Dimension of full observation vector INTEGER , INTENT ( out ) :: dim_obs !> Observation dimensions INTEGER :: dim_obs_ssh_mgrid dim_obs_ssh_mgrid = 0 IF ( assim_ssh_mgrid ) CALL init_dim_obs_ssh_mgrid ( step , dim_obs_ssh_mgrid ) dim_obs = dim_obs_ssh_mgrid END SUBROUTINE init_dim_obs_pdafomi","tags":"","loc":"proc/init_dim_obs_pdafomi.html"},{"title":"obs_op_pdafomi – NEMO4 PDAFOMI","text":"subroutine obs_op_pdafomi(step, dim_p, dim_obs, state_p, ostate) Uses mod_kind_pdaf mod_obs_ssh_mgrid_pdafomi Define observation operator Calling Sequence Called by: mod_assimilation_pdaf (as U_prepoststep ) Calls: obs_op_ssh_mgrid Arguments Type Intent Optional Attributes Name integer, intent(in) :: step Current time step integer, intent(in) :: dim_p PE-local state dimension integer, intent(in) :: dim_obs Dimension of full observed state real(kind=pwp), intent(in) :: state_p (dim_p) PE-local model state real(kind=pwp), intent(inout) :: ostate (dim_obs) PE-local full observed state Contents Source Code obs_op_pdafomi Source Code SUBROUTINE obs_op_pdafomi ( step , dim_p , dim_obs , state_p , ostate ) USE mod_kind_pdaf USE mod_obs_ssh_mgrid_pdafomi , & ONLY : obs_op_ssh_mgrid IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step !> PE-local state dimension INTEGER , INTENT ( in ) :: dim_p !> Dimension of full observed state INTEGER , INTENT ( in ) :: dim_obs !> PE-local model state REAL ( pwp ), INTENT ( in ) :: state_p ( dim_p ) !> PE-local full observed state REAL ( pwp ), INTENT ( inout ) :: ostate ( dim_obs ) CALL obs_op_ssh_mgrid ( dim_p , dim_obs , state_p , ostate ) END SUBROUTINE obs_op_pdafomi","tags":"","loc":"proc/obs_op_pdafomi.html"},{"title":"init_dim_obs_l_pdafomi – NEMO4 PDAFOMI","text":"subroutine init_dim_obs_l_pdafomi(domain_p, step, dim_obs, dim_obs_l) Uses mod_kind_pdaf mod_assimilation_pdaf mod_obs_ssh_mgrid_pdafomi dom_oce Initialise the dimension of local observations Calling Sequence Called by: mod_assimilation_pdaf (as U_prepoststep ) Calls: init_dim_obs_l_ssh_mgrid Arguments Type Intent Optional Attributes Name integer, intent(in) :: domain_p Index of current local analysis domain integer, intent(in) :: step Current time step integer, intent(in) :: dim_obs Full dimension of observation vector integer, intent(out) :: dim_obs_l Local dimension of observation vector Contents Variables i i0 j j0 land Source Code init_dim_obs_l_pdafomi Variables Type Visibility Attributes Name Initial integer, public :: i Grid coordinates for local analysis domain integer, public :: i0 Halo offset for local PE integer, public :: j Grid coordinates for local analysis domain integer, public :: j0 Halo offset for local PE logical, public :: land Source Code SUBROUTINE init_dim_obs_l_pdafomi ( domain_p , step , dim_obs , dim_obs_l ) USE mod_kind_pdaf USE mod_assimilation_pdaf , & ONLY : indx_dom_l USE mod_obs_ssh_mgrid_pdafomi , & ONLY : init_dim_obs_l_ssh_mgrid USE dom_oce , & ONLY : nldi , nldj , tmask IMPLICIT NONE !> Index of current local analysis domain INTEGER , INTENT ( in ) :: domain_p !> Current time step INTEGER , INTENT ( in ) :: step !> Full dimension of observation vector INTEGER , INTENT ( in ) :: dim_obs !> Local dimension of observation vector INTEGER , INTENT ( out ) :: dim_obs_l !> Halo offset for local PE INTEGER :: i0 , j0 !> Grid coordinates for local analysis domain INTEGER :: i , j ! Indicate whether surface gridpoint land LOGICAL :: land ! Hack for dealing with case when no valid local domains on PE. ! See init_n_domains for details. land = . FALSE . IF ( domain_p == 1 ) THEN ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 ! Compute i,j indices i = indx_dom_l ( 1 , domain_p ) j = indx_dom_l ( 2 , domain_p ) ! Check whether the local domain is actually a land point ! (and hence not a valid local domain). IF ( tmask ( i + i0 , j + j0 , 1 ) == 0.0_pwp ) land = . TRUE . END IF IF ( land ) THEN dim_obs_l = 0 ELSE CALL init_dim_obs_l_ssh_mgrid ( domain_p , step , dim_obs , dim_obs_l ) END IF END SUBROUTINE init_dim_obs_l_pdafomi","tags":"","loc":"proc/init_dim_obs_l_pdafomi.html"},{"title":"deallocate_obs_pdafomi – NEMO4 PDAFOMI","text":"subroutine deallocate_obs_pdafomi(step) Uses mod_kind_pdaf PDAFomi mod_obs_ssh_mgrid_pdafomi mod_assimilation_pdaf Deallocate observation arrays This routine calls the routine PDAFomi_deallocate_obs for each observation type. Arguments Type Intent Optional Attributes Name integer, intent(in) :: step Current time step Contents Source Code deallocate_obs_pdafomi Source Code SUBROUTINE deallocate_obs_pdafomi ( step ) USE mod_kind_pdaf USE PDAFomi , & ONLY : PDAFomi_deallocate_obs USE mod_obs_ssh_mgrid_pdafomi , & ONLY : obs_ssh_mgrid => thisobs USE mod_assimilation_pdaf , & ONLY : indx_dom_l IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step CALL PDAFomi_deallocate_obs ( obs_ssh_mgrid ) ! Tidy-up from init_n_domains_pdaf IF ( ALLOCATED ( indx_dom_l )) DEALLOCATE ( indx_dom_l ) END SUBROUTINE deallocate_obs_pdafomi","tags":"","loc":"proc/deallocate_obs_pdafomi.html"},{"title":"init_ens_pdaf – NEMO4 PDAFOMI","text":"subroutine init_ens_pdaf(filtertype, dim_p, dim_ens, state_p, Uinv, ens_p, flag) Uses mod_kind_pdaf mod_parallel_pdaf mod_assimilation_pdaf mod_statevector_pdaf Ensemble Initialisation This routine calls the routines for initialising the ensemble. Separate calls are made for the 2D and 3D state variables to\nallow for differences in how these variables are initialised. The routine is called when the filter is initialized in PDAF_filter_init . The routine is called by all filter processes and\ninitializes the ensemble for the PE-local domain . Calling Sequence Called from: init_pdaf/PDAF_init (PDAF module) Calls: fill2d_ensarray fill3d_ensarray Arguments Type Intent Optional Attributes Name integer, intent(in) :: filtertype Type of filter to initialize integer, intent(in) :: dim_p PE-local state dimension integer, intent(in) :: dim_ens Size of ensemble real(kind=pwp), intent(inout) :: state_p (dim_p) PE-local model state\n It is not necessary to initialize the array ‘state_p’ for SEIK.\n It is available here only for convenience and can be used freely. real(kind=pwp), intent(inout) :: Uinv (dim_ens-1,dim_ens-1) Array not referenced for SEIK real(kind=pwp), intent(out) :: ens_p (dim_p,dim_ens) PE-local state ensemble integer, intent(inout) :: flag PDAF status flag Contents Source Code init_ens_pdaf Source Code SUBROUTINE init_ens_pdaf ( filtertype , dim_p , dim_ens , state_p , Uinv , & ens_p , flag ) USE mod_kind_pdaf USE mod_parallel_pdaf , & ONLY : mype_ens USE mod_assimilation_pdaf , & ONLY : istate_ssh , istate_s , istate_t , istate_u , istate_v , & screen USE mod_statevector_pdaf , & ONLY : fill2d_ensarray , fill3d_ensarray IMPLICIT NONE !> Type of filter to initialize INTEGER , INTENT ( in ) :: filtertype !> PE-local state dimension INTEGER , INTENT ( in ) :: dim_p !> Size of ensemble INTEGER , INTENT ( in ) :: dim_ens !> PE-local model state !> It is not necessary to initialize the array 'state_p' for SEIK. !> It is available here only for convenience and can be used freely. REAL ( pwp ), INTENT ( inout ) :: state_p ( dim_p ) !> Array not referenced for SEIK REAL ( pwp ), INTENT ( inout ) :: Uinv ( dim_ens - 1 , dim_ens - 1 ) !> PE-local state ensemble REAL ( pwp ), INTENT ( out ) :: ens_p ( dim_p , dim_ens ) !> PDAF status flag INTEGER , INTENT ( inout ) :: flag IF ( screen > 0 ) THEN IF ( mype_ens == 0 ) THEN WRITE ( * , '(/1x,a)' ) '------- Reading Initial State --------' WRITE ( * , '(/1x,a)' ) 'Calling fill2d_ensarray' WRITE ( * , '(/9x, a, 3x, a,)' ) \"Initial state file:\" , TRIM ( istate_ssh ) END IF END IF CALL fill2d_ensarray ( istate_ssh , ens_p ) IF ( screen > 0 ) THEN IF ( mype_ens == 0 ) THEN WRITE ( * , '(/1x,a)' ) '------- Reading Initial State --------' WRITE ( * , '(/1x,a)' ) 'Calling fill3d_ensarray' WRITE ( * , '(/9x, a, 3x, a, 3x, a, 3x, a, 3x, a)' ) & \"Initial state files:\" , TRIM ( istate_t ), TRIM ( istate_s ), & TRIM ( istate_u ), TRIM ( istate_v ) END IF END IF CALL fill3d_ensarray ( istate_t , 'T' , ens_p ) CALL fill3d_ensarray ( istate_s , 'S' , ens_p ) CALL fill3d_ensarray ( istate_u , 'U' , ens_p ) CALL fill3d_ensarray ( istate_v , 'V' , ens_p ) END SUBROUTINE init_ens_pdaf","tags":"","loc":"proc/init_ens_pdaf.html"},{"title":"collect_state_pdaf – NEMO4 PDAFOMI","text":"subroutine collect_state_pdaf(dim_p, state_p) Uses mod_kind_pdaf mod_statevector_pdaf dom_oce oce par_oce Collecting the statevector variables The routine has to initialize the statevector of PDAF\nfrom the fields of the model. The routine is executed by each process that is\nparticipating in the model integrations. Calling Sequence Called from:* PDAFomi_assimilate_local / mod_assimilation_pdaf (as U_coll_state) Arguments Type Intent Optional Attributes Name integer, intent(in) :: dim_p PE-local state dimension real(kind=pwp), intent(inout) :: state_p (dim_p) PE-local state vector Contents Variables i i0 j j0 k Source Code collect_state_pdaf Variables Type Visibility Attributes Name Initial integer, public :: i Counters integer, public :: i0 Start index for MPI subdomain integer, public :: j Counters integer, public :: j0 Start index for MPI subdomain integer, public :: k Counters Source Code SUBROUTINE collect_state_pdaf ( dim_p , state_p ) USE mod_kind_pdaf USE mod_statevector_pdaf , & ONLY : mpi_subd_lat , mpi_subd_lon , mpi_subd_vert , ssh_p_offset , & t_p_offset , s_p_offset , u_p_offset , v_p_offset USE dom_oce , & ONLY : nldj , nldi USE oce , & ONLY : sshb , tsb , ub , vb USE par_oce , & ONLY : jp_tem , jp_sal , jpi , jpj , jpk IMPLICIT NONE !> PE-local state dimension INTEGER , INTENT ( in ) :: dim_p !> PE-local state vector REAL ( pwp ), INTENT ( inout ) :: state_p ( dim_p ) !> Counters INTEGER :: i , j , k !> Start index for MPI subdomain INTEGER :: i0 , j0 ! Set the starting index after the halo region j0 = nldj - 1 i0 = nldi - 1 ! ********************************* ! Collect state vector 2d variables ! ********************************* ! SSH DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon state_p ( i + ( j - 1 ) * mpi_subd_lon + ssh_p_offset ) = & sshb ( i + i0 , j + j0 ) END DO END DO ! ********************************* ! Collect state vector 3d variables ! ********************************* ! T DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + t_p_offset ) = tsb ( i + i0 , & j + j0 , k , jp_tem ) END DO END DO END DO ! S DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + s_p_offset ) = tsb ( i + i0 , & j + j0 , k , jp_sal ) END DO END DO END DO ! U DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + u_p_offset ) = ub ( i + i0 , & j + j0 , k ) END DO END DO END DO ! V DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + v_p_offset ) = vb ( i + i0 , & j + j0 , k ) END DO END DO END DO END SUBROUTINE collect_state_pdaf","tags":"","loc":"proc/collect_state_pdaf.html"},{"title":"distribute_state_pdaf – NEMO4 PDAFOMI","text":"subroutine distribute_state_pdaf(dim_p, state_p) Uses mod_kind_pdaf mod_iau_pdaf mod_statevector_pdaf dom_oce oce par_oce lbclnk Distributing the statevector variables, computing the statevector increments This routine either initializes the full fields of\nthe model from the statevector of PDAF (first timestep),\nor computes the statevector increments (all other timesteps).\nFor all other timesteps, the increments are added to the\nmodel during the NEMO timestepping routine. See mod_iau_pdaf for details. The routine is executed by each process that is\nparticipating in the model integrations. Calling Sequence Called from: PDAF_get_state (as U_dist_state) Called from: PDAFomi_assimilate_local (as U_dist_state) Arguments Type Intent Optional Attributes Name integer, intent(in) :: dim_p PE-local state dimension real(kind=pwp), intent(inout) :: state_p (dim_p) PE-local state vector Contents Variables firststep i i0 j j0 k Source Code distribute_state_pdaf Variables Type Visibility Attributes Name Initial logical, public :: firststep = .TRUE. Flag for first timestep integer, public :: i Counters integer, public :: i0 Start index for MPI subdomain integer, public :: j Counters integer, public :: j0 Start index for MPI subdomain integer, public :: k Counters Source Code SUBROUTINE distribute_state_pdaf ( dim_p , state_p ) USE mod_kind_pdaf USE mod_iau_pdaf , & ONLY : ssh_iau_pdaf , u_iau_pdaf , v_iau_pdaf , t_iau_pdaf , & s_iau_pdaf USE mod_statevector_pdaf , & ONLY : mpi_subd_lat , mpi_subd_lon , mpi_subd_vert , ssh_p_offset , & t_p_offset , s_p_offset , u_p_offset , v_p_offset USE dom_oce , & ONLY : nldj , nldi USE oce , & ONLY : sshb , tsb , ub , vb USE par_oce , & ONLY : jp_tem , jp_sal , jpi , jpj , jpk USE lbclnk , & ONLY : lbc_lnk , lbc_lnk_multi IMPLICIT NONE !> PE-local state dimension INTEGER , INTENT ( in ) :: dim_p !> PE-local state vector REAL ( pwp ), INTENT ( inout ) :: state_p ( dim_p ) !> Counters INTEGER :: i , j , k !> Start index for MPI subdomain INTEGER :: i0 , j0 !> Flag for first timestep LOGICAL :: firststep = . TRUE . ! ********************************************** ! Only distribute full state on first time step. ! Otherwise compute increment. ! ********************************************** ! Set the starting index after the halo region j0 = nldj - 1 i0 = nldi - 1 first : IF ( firststep ) THEN ! ************************************ ! Distribute state vector 2d variables ! ************************************ ! SSH DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon sshb ( i + i0 , j + j0 ) = state_p ( i + ( j - 1 ) * mpi_subd_lon & + ssh_p_offset ) END DO END DO ! Fill halo regions CALL lbc_lnk ( 'distribute_state_pdaf' , sshb , 'T' , 1. ) ! ************************************ ! Distribute state vector 3d variables ! ************************************ ! T DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon tsb ( i + i0 , j + j0 , k , jp_tem ) = state_p ( i + ( j - 1 ) * mpi_subd_lon & + ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + t_p_offset ) END DO END DO END DO ! S DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon tsb ( i + i0 , j + j0 , k , jp_sal ) = state_p ( i + ( j - 1 ) * mpi_subd_lon & + ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + s_p_offset ) END DO END DO END DO ! Fill halo regions CALL lbc_lnk_multi ( 'distribute_state_pdaf' , tsb (:, :, :, jp_tem ), 'T' , & 1. , tsb (:, :, :, jp_sal ), 'T' , 1. ) ! U DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon ub ( i + i0 , j + j0 , k ) = state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + u_p_offset ) END DO END DO END DO ! V DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon vb ( i + i0 , j + j0 , k ) = state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + v_p_offset ) END DO END DO END DO ! Fill halo regions CALL lbc_lnk_multi ( 'distribute_state_pdaf' , ub , 'U' , - 1. , vb , 'V' , - 1. ) firststep = . FALSE . ELSE first ! *********************************************** ! Compute increment for state vector 2d variables ! *********************************************** ! SSH DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon ssh_iau_pdaf ( i + i0 , j + j0 ) = & state_p ( i + ( j - 1 ) * mpi_subd_lon + ssh_p_offset ) - & sshb ( i + i0 , j + j0 ) END DO END DO ! Fill halo regions CALL lbc_lnk ( 'distribute_state_pdaf' , ssh_iau_pdaf , 'T' , 1. ) ! *********************************************** ! Compute increment for state vector 3d variables ! *********************************************** ! T DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon t_iau_pdaf ( i + i0 , j + j0 , k ) = & state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + t_p_offset ) - & tsb ( i + i0 , j + j0 , k , jp_tem ) END DO END DO END DO ! S DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon s_iau_pdaf ( i + i0 , j + j0 , k ) = & state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + s_p_offset ) - & tsb ( i + i0 , j + j0 , k , jp_sal ) END DO END DO END DO ! Fill halo regions CALL lbc_lnk_multi ( 'distribute_state_pdaf' , t_iau_pdaf (:, :, :), 'T' , & 1. , s_iau_pdaf (:, :, :), 'T' , 1. ) ! U DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon u_iau_pdaf ( i + i0 , j + j0 , k ) = & state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + u_p_offset ) - & ub ( i + i0 , j + j0 , k ) END DO END DO END DO ! V DO k = 1 , mpi_subd_vert DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon v_iau_pdaf ( i + i0 , j + j0 , k ) = & state_p ( i + ( j - 1 ) * mpi_subd_lon + & ( k - 1 ) * mpi_subd_lat * mpi_subd_lon + v_p_offset ) - & vb ( i + i0 , j + j0 , k ) END DO END DO END DO ! Fill halo regions CALL lbc_lnk_multi ( 'distribute_state_pdaf' , u_iau_pdaf , 'U' , - 1. , & v_iau_pdaf , 'V' , - 1. ) END IF first END SUBROUTINE distribute_state_pdaf","tags":"","loc":"proc/distribute_state_pdaf.html"},{"title":"init_dim_l_pdaf – NEMO4 PDAFOMI","text":"subroutine init_dim_l_pdaf(step, domain_p, dim_l) Uses mod_kind_pdaf mod_assimilation_pdaf mod_statevector_pdaf mod_parallel_pdaf dom_oce Set dimension of local model state The routine is called during analysis step\nin PDAF_X_update in the loop over all local\nanalysis domains. It has to set the dimension\nof the local model state on the current analysis\ndomain. Called from: PDAFomi_assimilate_local / mod_assimilation_pdaf Arguments Type Intent Optional Attributes Name integer, intent(in) :: step Current time step integer, intent(in) :: domain_p Current local analysis domain integer, intent(out) :: dim_l Local state dimension Contents Variables cnt i i0 idx j j0 lat lon rad_conv Source Code init_dim_l_pdaf Variables Type Visibility Attributes Name Initial integer, public :: cnt Counters integer, public :: i Grid coordinates for local analysis domain integer, public :: i0 Halo offset for local PE integer, public :: idx Counters integer, public :: j Grid coordinates for local analysis domain integer, public :: j0 Halo offset for local PE real(kind=pwp), public :: lat Longitude, latitude for local analysis domain real(kind=pwp), public :: lon Longitude, latitude for local analysis domain real(kind=pwp), public :: rad_conv = 3.141592653589793/180.0 Degree to radian conversion Source Code SUBROUTINE init_dim_l_pdaf ( step , domain_p , dim_l ) USE mod_kind_pdaf USE mod_assimilation_pdaf , & ONLY : coords_l , indx_dom_l USE mod_statevector_pdaf , & ONLY : mpi_subd_vert , var2d_p_offset , var3d_p_offset USE mod_parallel_pdaf , & ONLY : abort_parallel USE dom_oce , & ONLY : nldj , nldi , tmask , glamt , gphit IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step !> Current local analysis domain INTEGER , INTENT ( in ) :: domain_p !> Local state dimension INTEGER , INTENT ( out ) :: dim_l !> Counters INTEGER :: idx , cnt !> Grid coordinates for local analysis domain INTEGER :: i , j !> Halo offset for local PE INTEGER :: i0 , j0 !> Longitude, latitude for local analysis domain REAL ( pwp ) :: lon , lat !> Degree to radian conversion REAL ( pwp ) :: rad_conv = 3.141592653589793 / 18 0.0 ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 ! Compute coordinates i = indx_dom_l ( 1 , domain_p ) j = indx_dom_l ( 2 , domain_p ) ! ********************************************** ! *** Initialize coordinates of local domain *** ! ********************************************** ! Use T-values to get local coordinates lat = gphit ( i + i0 , j + j0 ) lon = glamt ( i + i0 , j + j0 ) ! Convert local domain coordinates to radians (as required by PDAFOMI) coords_l ( 1 ) = lon * rad_conv coords_l ( 2 ) = lat * rad_conv ! **************************************** ! *** Initialize local state dimension *** ! **************************************** ! ************************************************************* ! dimension = (number of 2D state variables) + (number of 3D ! variables * number of ocean vertical points). ! ! We need to calculate the number of ocean vertical points ie ! we need to determine how many points in the vertical are ! ocean and how may are land (we do not include land points in ! our local state vector). ! ************************************************************* cnt = 0 DO idx = 1 , mpi_subd_vert IF ( tmask ( i + i0 , j + j0 , idx ) == 1.0_pwp ) cnt = cnt + 1 END DO ! on the local grid point (column), it has number of 2D variables + number of 3D variables dim_l = SIZE ( var2d_p_offset ) + ( SIZE ( var3d_p_offset ) * cnt ) END SUBROUTINE init_dim_l_pdaf","tags":"","loc":"proc/init_dim_l_pdaf.html"},{"title":"g2l_state_pdaf – NEMO4 PDAFOMI","text":"subroutine g2l_state_pdaf(step, domain_p, dim_p, state_p, dim_l, state_l) Uses mod_kind_pdaf mod_parallel_pdaf mod_assimilation_pdaf mod_statevector_pdaf dom_oce Restrict a model state to a local analysis domain The routine is called during the loop over all\nlocal analysis domains in PDAF_X_update before the analysis on a single local analysis\ndomain. It has to initialize elements of the\nstate vector for the local analysis domains from\nthe PE-local full state vector. Arguments Type Intent Optional Attributes Name integer, intent(in) :: step Current time step integer, intent(in) :: domain_p Current local analysis domain integer, intent(in) :: dim_p PE-local full state dimension real(kind=pwp), intent(in) :: state_p (dim_p) PE-local full state vector integer, intent(in) :: dim_l Local state dimension real(kind=pwp), intent(out) :: state_l (dim_l) State vector on local analysis domain Contents Variables a b c dim_vert_l i i0 idx j j0 loc_2dvar vcoord_conv Source Code g2l_state_pdaf Variables Type Visibility Attributes Name Initial integer, public :: a Variables for 3D state variable index integer, public :: b Variables for 3D state variable index integer, public :: c Variables for 3D state variable index integer, public :: dim_vert_l Number of vertical ocean points in local statevector integer, public :: i Grid coordinates for local analysis domain integer, public :: i0 Halo offset for local PE integer, public :: idx Counter integer, public :: j Grid coordinates for local analysis domain integer, public :: j0 Halo offset for local PE integer, public :: loc_2dvar 2D state variable coordinate in statevector integer, public :: vcoord_conv (100) Array for converting vertical coordinate in local statevector Source Code SUBROUTINE g2l_state_pdaf ( step , domain_p , dim_p , state_p , dim_l , state_l ) USE mod_kind_pdaf USE mod_parallel_pdaf , & ONLY : abort_parallel USE mod_assimilation_pdaf , & ONLY : indx_dom_l USE mod_statevector_pdaf , & ONLY : mpi_subd_vert , mpi_subd_lon , mpi_subd_lat , & var2d_p_offset , var3d_p_offset USE dom_oce , & ONLY : nldj , nldi , tmask IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step !> Current local analysis domain INTEGER , INTENT ( in ) :: domain_p !> PE-local full state dimension INTEGER , INTENT ( in ) :: dim_p !> Local state dimension INTEGER , INTENT ( in ) :: dim_l !> PE-local full state vector REAL ( pwp ), INTENT ( in ) :: state_p ( dim_p ) !> State vector on local analysis domain REAL ( pwp ), INTENT ( out ) :: state_l ( dim_l ) !> Counter INTEGER :: idx !> Halo offset for local PE INTEGER :: i0 , j0 !> Grid coordinates for local analysis domain INTEGER :: i , j !> Array for converting vertical coordinate in local statevector INTEGER :: vcoord_conv ( 100 ) !> Number of vertical ocean points in local statevector INTEGER :: dim_vert_l !> 2D state variable coordinate in statevector INTEGER :: loc_2dvar !> Variables for 3D state variable index INTEGER :: a , b , c ! ******************************************************* ! *** Initialise vertical coordinate conversion array *** ! ******************************************************* IF ( SIZE ( vcoord_conv ) < mpi_subd_vert ) THEN WRITE ( * , '(/1x,a59/)' ) & 'ERROR: automatic array v_coord in g2l_state_pdaf too small ' CALL abort_parallel () END IF vcoord_conv = 0 dim_vert_l = 0 ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 ! Compute coordinates i = indx_dom_l ( 1 , domain_p ) j = indx_dom_l ( 2 , domain_p ) ! Count number of ocean vertical points DO idx = 1 , mpi_subd_vert IF ( tmask ( i + i0 , j + j0 , idx ) == 1.0_pwp ) THEN dim_vert_l = dim_vert_l + 1 vcoord_conv ( dim_vert_l ) = idx END IF END DO ! ************************************* ! *** Initialize local state vector *** ! ************************************* ! ********************************************************** ! A local domain consists of all ocean points in a vertical ! column. Such a domain will have coordinates (x,y,:). ! The 2d state variables in the global statevector will be ! located at ( (y-1)*dim_longitude ) + x + 2d_offset. ! The 3d state variables in the global statevector will be ! located at ( (z-1)*dim_longitude*dim_latitude ) + ! ( (y-1)*dim_longitude ) + x + 3d_offset, where z can vary ! over all *ocean* points in the vertical column. ! ********************************************************** ! Compute location of 2d variables in statevector loc_2dvar = ( j - 1 ) * ( mpi_subd_lon ) + i DO idx = 1 , dim_l ! Compute 2d state variables IF ( idx <= SIZE ( var2d_p_offset )) THEN state_l ( idx ) = state_p ( loc_2dvar + var2d_p_offset ( idx )) ELSE ! Compute 3d state variables a = idx - SIZE ( var2d_p_offset ) b = MOD ( a - 1 , dim_vert_l ) + 1 c = ( a - b ) / dim_vert_l + 1 state_l ( idx ) = state_p ( mpi_subd_lon * mpi_subd_lat * ( vcoord_conv ( b ) - 1 ) & + loc_2dvar + var3d_p_offset ( c )) END IF END DO END SUBROUTINE g2l_state_pdaf","tags":"","loc":"proc/g2l_state_pdaf.html"},{"title":"init_n_domains_pdaf – NEMO4 PDAFOMI","text":"subroutine init_n_domains_pdaf(step, n_domains_p) Uses mod_kind_pdaf mod_assimilation_pdaf mod_statevector_pdaf mod_parallel_pdaf dom_oce Set number of local analysis domains The routine is called in PDAF_X_update at the beginning of the analysis step before\nthe loop through all local analysis domains.\nIt has to set the number of local analysis\ndomains for the PE-local domain. Called from: PDAFomi_assimilate_local / mod_assimilation_pdaf Arguments Type Intent Optional Attributes Name integer, intent(in) :: step Current time step integer, intent(out) :: n_domains_p PE-local number of analysis domains Contents Variables cnt i i0 j j0 Source Code init_n_domains_pdaf Variables Type Visibility Attributes Name Initial integer, public :: cnt Counters integer, public :: i Counters integer, public :: i0 Halo offset integer, public :: j Counters integer, public :: j0 Halo offset Source Code SUBROUTINE init_n_domains_pdaf ( step , n_domains_p ) USE mod_kind_pdaf USE mod_assimilation_pdaf , & ONLY : indx_dom_l USE mod_statevector_pdaf , & ONLY : mpi_subd_lon , mpi_subd_lat USE mod_parallel_pdaf , & ONLY : mype_filter USE dom_oce , & ONLY : nldi , nldj , tmask IMPLICIT NONE !> Current time step INTEGER , INTENT ( in ) :: step !> PE-local number of analysis domains INTEGER , INTENT ( out ) :: n_domains_p !> Counters INTEGER :: i , j , cnt !> Halo offset INTEGER :: i0 , j0 ! ************************************ ! *** Initialize number of domains *** ! ************************************ ! ******************************************* ! ! The number of local domains is defined as ! the number of grid points at the surface ! where tmask is 1 ie horizontal localization ! is used, and land points are ignored. ! ! ******************************************* n_domains_p = 0 ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon IF ( tmask ( i + i0 , j + j0 , 1 ) == 1.0_pwp ) THEN n_domains_p = n_domains_p + 1 END IF END DO END DO ! ******************************************* ! *** Store local domain i,j index values *** ! ******************************************* ! ****************************************** ! ! Important to consider case where there ! are no valid local domains. In this ! case we use a hack. We set the number ! of local domains (n_domains_p) to 1, ! and set the dimension of the observations ! for this local domain to zero (see ! init_dim_obs_l). This hack means no update ! will be performed on this local domain. ! ! ****************************************** IF ( n_domains_p > 0 ) THEN ! Deallocated in deallocate_obs_pdafomi IF (. NOT . ALLOCATED ( indx_dom_l )) ALLOCATE ( indx_dom_l ( 2 , n_domains_p )) cnt = 0 DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon IF ( tmask ( i + i0 , j + j0 , 1 ) == 1.0_pwp ) THEN cnt = cnt + 1 indx_dom_l ( 1 , cnt ) = i indx_dom_l ( 2 , cnt ) = j END IF END DO END DO ELSE WRITE ( * , '(8x,a,i3)' ) 'WARNING: No valid local domains, PE=' , mype_filter n_domains_p = 1 ! Deallocated in deallocate_obs_pdafomi IF (. NOT . ALLOCATED ( indx_dom_l )) ALLOCATE ( indx_dom_l ( 2 , 1 )) indx_dom_l ( 1 , 1 ) = 1 indx_dom_l ( 2 , 1 ) = 1 END IF END SUBROUTINE init_n_domains_pdaf","tags":"","loc":"proc/init_n_domains_pdaf.html"},{"title":"next_observation_pdaf – NEMO4 PDAFOMI","text":"subroutine next_observation_pdaf(stepnow, nsteps, doexit, time) Uses mod_kind_pdaf mod_assimilation_pdaf mod_parallel_pdaf in_out_manager Determining the Next Analysis Step The subroutine is called before each forecast phase\nby PDAF_get_state . It has to initialize the number\nof time steps until the next available observation\n( nsteps ). It indicates if the data assimilation process\nis completed such that the ensemble loop in the model\nroutine can be exited. The routine is called by all processes. Calling Sequence Called from: init_pdaf/PDAF_get_state (as U_next_obs) Arguments Type Intent Optional Attributes Name integer, intent(in) :: stepnow Number of the current time step integer, intent(out) :: nsteps Number of time steps until next obs integer, intent(out) :: doexit Whether to exit forecasting (1 for exit) real(kind=pwp), intent(out) :: time Current model (physical) time Contents Source Code next_observation_pdaf Source Code SUBROUTINE next_observation_pdaf ( stepnow , nsteps , doexit , time ) USE mod_kind_pdaf USE mod_assimilation_pdaf , & ONLY : delt_obs USE mod_parallel_pdaf , & ONLY : mype_ens USE in_out_manager , & ONLY : nitend IMPLICIT NONE !> Number of the current time step INTEGER , INTENT ( in ) :: stepnow !> Number of time steps until next obs INTEGER , INTENT ( out ) :: nsteps !> Whether to exit forecasting (1 for exit) INTEGER , INTENT ( out ) :: doexit !> Current model (physical) time REAL ( pwp ), INTENT ( out ) :: time ! ******************************************************* ! *** Set number of time steps until next observation *** ! ******************************************************* ! Not used in this implementation time = 0.0 IF ( stepnow + delt_obs <= nitend ) THEN ! *** During the assimilation process *** nsteps = delt_obs ! This assumes a constant time step interval doexit = 0 ! Not used in this implementation IF ( mype_ens == 0 ) WRITE ( * , '(i7, 3x, a, i7)' ) & stepnow , 'Next observation at time step' , stepnow + nsteps ELSE ! *** End of assimilation process *** nsteps = 0 ! No more steps doexit = 1 ! Not used in this implementation IF ( mype_ens == 0 ) WRITE ( * , '(i7, 3x, a)' ) & stepnow , 'No more observations - end assimilation' END IF END SUBROUTINE next_observation_pdaf","tags":"","loc":"proc/next_observation_pdaf.html"},{"title":"init_pdaf – NEMO4 PDAFOMI","text":"public subroutine init_pdaf() Uses mod_parallel_pdaf mod_assimilation_pdaf mod_iau_pdaf mod_statevector_pdaf mod_util_pdaf This routine collects the initialization of variables for PDAF. The initialization routine PDAF_init is called\n such that the internal initialization of PDAF is performed.\n The initialization is used to set-up local domain and filter options\n such as the filter type, inflation, and localization radius.\n This variant is for the online mode of PDAF. The ensemble is initialised in init_ens_pdaf , and is then\n distributed to the model in distribute_state_pdaf . The arrays\n for the incremental analysis update (IAU) are initialised in asm_inc_init_pdaf . The statevector dimension, and the offset and dimension of the\n statevector variables is calculated in calc_statevector_dim . Much of the initialisation is read from a PDAF-specific namelist.\n This is performed in read_config_pdaf . Calling Sequence Called from: nemogcm.F90 Calls: calc_statevector_dim read_config_pdaf init_pdaf_info PDAF_init asm_inc_init_pdaf PDAF_get_state Arguments None Contents Variables doexit filter_param_i filter_param_r status_pdaf steps timenow Source Code init_pdaf Variables Type Visibility Attributes Name Initial integer, public :: doexit Not used in this implementation integer, public :: filter_param_i (7) Integer parameter array for filter real(kind=pwp), public :: filter_param_r (2) Real parameter array for filter integer, public :: status_pdaf PDAF status flag integer, public :: steps Not used in this implementation real(kind=pwp), public :: timenow Not used in this implementation Source Code SUBROUTINE init_pdaf () USE mod_parallel_pdaf , & ONLY : n_modeltasks , task_id , COMM_model , COMM_filter , & COMM_couple , mype_ens , filterpe , abort_parallel USE mod_assimilation_pdaf , & ONLY : dim_state_p , screen , filtertype , subtype , dim_ens , & incremental , covartype , type_forget , forget , rank_analysis_enkf , & type_trans , type_sqrt , delt_obs , locweight , local_range , srange , & salfixmin USE mod_iau_pdaf , & ONLY : asm_inc_init_pdaf USE mod_statevector_pdaf , & ONLY : calc_statevector_dim USE mod_util_pdaf , & ONLY : init_info_pdaf , read_config_pdaf !> Integer parameter array for filter INTEGER :: filter_param_i ( 7 ) !> Real parameter array for filter REAL ( pwp ) :: filter_param_r ( 2 ) !> PDAF status flag INTEGER :: status_pdaf !> Not used in this implementation INTEGER :: doexit , steps !> Not used in this implementation REAL ( pwp ) :: timenow ! Ensemble initialization EXTERNAL :: init_ens_pdaf ! Determine how long until next observation EXTERNAL :: next_observation_pdaf ! Routine to distribute a state vector to model fields EXTERNAL :: distribute_state_pdaf ! User supplied pre/poststep routine EXTERNAL :: prepoststep_ens_pdaf ! *************************** ! ***   Initialize PDAF   *** ! *************************** IF ( mype_ens == 0 ) THEN WRITE ( * , '(/1x,a)' ) 'INITIALIZE PDAF - ONLINE MODE' END IF ! Compute dimension of local statevector. CALL calc_statevector_dim ( dim_state_p ) ! ********************************************************** ! ***   CONTROL OF PDAF - used in call to PDAF_init      *** ! ********************************************************** ! *** IO options *** screen = 2 ! Write screen output (1) for output, (2) add timings ! *** Filter specific variables filtertype = 5 ! Type of filter !   (1) SEIK !   (2) EnKF !   (3) LSEIK !   (4) ETKF !   (5) LETKF !   (6) ESTKF !   (7) LESTKF dim_ens = n_modeltasks ! Size of ensemble for all ensemble filters !   We use n_modeltasks here, initialized in init_parallel_pdaf subtype = 0 ! subtype of filter: !   ESTKF: !     (0) Standard form of ESTKF !   LESTKF: !     (0) Standard form of LESTKF type_trans = 0 ! Type of ensemble transformation !   SEIK/LSEIK and ESTKF/LESTKF: !     (0) use deterministic omega !     (1) use random orthonormal omega orthogonal to (1,...,1)&#94;T !     (2) use product of (0) with random orthonormal matrix with !         eigenvector (1,...,1)&#94;T !   ETKF/LETKF: !     (0) use deterministic symmetric transformation !     (2) use product of (0) with random orthonormal matrix with !         eigenvector (1,...,1)&#94;T type_forget = 0 ! Type of forgetting factor in SEIK/LSEIK/ETKF/LETKF/ESTKF/LESTKF !   (0) fixed !   (1) global adaptive !   (2) local adaptive for LSEIK/LETKF/LESTKF forget = 1.0 ! Forgetting factor type_sqrt = 0 ! Type of transform matrix square-root !   (0) symmetric square root, (1) Cholesky decomposition incremental = 0 ! (1) to perform incremental updating (only in SEIK/LSEIK!) covartype = 1 ! Definition of factor in covar. matrix used in SEIK !   (0) for dim_ens&#94;-1 (old SEIK) !   (1) for (dim_ens-1)&#94;-1 (real ensemble covariance matrix) !   This parameter has also to be set internally in PDAF_init. rank_analysis_enkf = 0 ! rank to be considered for inversion of HPH ! in analysis of EnKF; (0) for analysis w/o eigendecomposition ! ********************************************************************* ! ***   Settings for analysis steps  - used in call-back routines   *** ! ********************************************************************* ! *** Forecast length (time interval between analysis steps) *** delt_obs = 24 ! Number of time steps between analysis/assimilation steps ! *** Localization settings locweight = 0 ! Type of localizating weighting !   (0) constant weight of 1 !   (1) exponentially decreasing with SRANGE !   (2) use 5th-order polynomial !   (3) regulated localization of R with mean error variance !   (4) regulated localization of R with single-point error variance local_range = 0 ! Range in grid points for observation domain in local filters srange = local_range ! Support range for 5th-order polynomial ! or range for 1/e for exponential weighting ! *************************************************************** ! *** Settings for analysis increments - used in IAU routines *** ! *************************************************************** ! Minimum value for salinity to allow IAU update salfixmin = 0 ! ************************** ! Namelist and screen output ! ************************** ! Read namelist file for PDAF CALL read_config_pdaf () ! Screen output for PDAF parameters IF ( mype_ens == 0 ) CALL init_info_pdaf () ! ***************************************************** ! *** Call PDAF initialization routine on all PEs.  *** ! ***                                               *** ! *** Here, the full selection of filters is        *** ! *** implemented. In a real implementation, one    *** ! *** reduce this to selected filters.              *** ! ***                                               *** ! *** For all filters, first the arrays of integer  *** ! *** and real number parameters are initialized.   *** ! *** Subsequently, PDAF_init is called.            *** ! ***************************************************** whichinit : IF ( filtertype == 2 ) THEN ! *** EnKF with Monte Carlo init *** filter_param_i ( 1 ) = dim_state_p ! State dimension filter_param_i ( 2 ) = dim_ens ! Size of ensemble filter_param_i ( 3 ) = rank_analysis_enkf ! Rank of speudo-inverse in analysis filter_param_i ( 4 ) = incremental ! Whether to perform incremental analysis filter_param_i ( 5 ) = 0 ! Smoother lag (not implemented here) filter_param_r ( 1 ) = forget ! Forgetting factor CALL PDAF_init ( filtertype , subtype , 0 , & filter_param_i , 6 , & filter_param_r , 2 , & COMM_model , COMM_filter , COMM_couple , & task_id , n_modeltasks , filterpe , init_ens_pdaf , & screen , status_pdaf ) ELSE ! *** All other filters                       *** ! *** SEIK, LSEIK, ETKF, LETKF, ESTKF, LESTKF *** filter_param_i ( 1 ) = dim_state_p ! State dimension filter_param_i ( 2 ) = dim_ens ! Size of ensemble filter_param_i ( 3 ) = 0 ! Smoother lag (not implemented here) filter_param_i ( 4 ) = incremental ! Whether to perform incremental analysis filter_param_i ( 5 ) = type_forget ! Type of forgetting factor filter_param_i ( 6 ) = type_trans ! Type of ensemble transformation filter_param_i ( 7 ) = type_sqrt ! Type of transform square-root (SEIK-sub4/ESTKF) filter_param_r ( 1 ) = forget ! Forgetting factor CALL PDAF_init ( filtertype , subtype , 0 , & filter_param_i , 7 , & filter_param_r , 2 , & COMM_model , COMM_filter , COMM_couple , & task_id , n_modeltasks , filterpe , init_ens_pdaf , & screen , status_pdaf ) END IF whichinit ! *** Check whether initialization of PDAF was successful *** IF ( status_pdaf /= 0 ) THEN WRITE ( * , '(/1x,a6,i3,a43,i4,a1/)' ) & 'ERROR ' , status_pdaf , & ' in initialization of PDAF - stopping! (PE ' , mype_ens , ')' CALL abort_parallel () END IF ! ************************************** ! *** Initialise PDAF arrays for IAU *** ! ************************************** CALL asm_inc_init_PDAF () ! ********************************** ! *** Prepare ensemble forecasts *** ! ********************************** CALL PDAF_get_state ( steps , timenow , doexit , next_observation_pdaf , & distribute_state_pdaf , prepoststep_ens_pdaf , status_pdaf ) END SUBROUTINE init_pdaf","tags":"","loc":"proc/init_pdaf.html"},{"title":"finalize_pdaf – NEMO4 PDAFOMI","text":"public subroutine finalize_pdaf() Uses mod_parallel_pdaf mod_iau_pdaf Timing and clean-up of PDAF Calling Sequence Called from: nemogcm Calls: PDAF_deallocate Arguments None Contents Source Code finalize_pdaf Source Code SUBROUTINE finalize_pdaf () !>Timing and clean-up of PDAF !> !> **Calling Sequence** !> !> - Called from: `nemogcm` !> !> - Calls: `PDAF_deallocate` USE mod_parallel_pdaf , & ONLY : mype_ens USE mod_iau_pdaf , & ONLY : ssh_iau_pdaf , t_iau_pdaf , s_iau_pdaf , u_iau_pdaf , v_iau_pdaf ! Show allocated memory for PDAF ! DOES NOT CURRENTLY WORK WITH XIOS CONFIGURATION - TBD WITH LARS !IF (mype_ens==0) CALL PDAF_print_info(2) ! Print PDAF timings onto screen ! DOES NOT CURRENTLY WORK WITH XIOS CONFIGURATION - TBD WITH LARS !IF (mype_ens==0) CALL PDAF_print_info(1) ! Deallocate PDAF arrays CALL PDAF_deallocate () ! Deallocaite IAU arrays ! 2D variables DEALLOCATE ( ssh_iau_pdaf ) ! 3D variables DEALLOCATE ( t_iau_pdaf , s_iau_pdaf , u_iau_pdaf , v_iau_pdaf ) END SUBROUTINE finalize_pdaf","tags":"","loc":"proc/finalize_pdaf.html"},{"title":"init_info_pdaf – NEMO4 PDAFOMI","text":"public subroutine init_info_pdaf() Uses mod_assimilation_pdaf This routine performs a model-sided screen output about\n the coniguration of the data assimilation system. Calling Sequence Called from: init_pdaf Arguments None Contents Source Code init_info_pdaf Source Code SUBROUTINE init_info_pdaf () USE mod_assimilation_pdaf , & ! Variables for assimilation ONLY : filtertype , subtype , dim_ens , delt_obs , model_error , & model_err_amp , forget , rank_analysis_enkf , int_rediag ! ***************************** ! *** Initial Screen output *** ! ***************************** IF ( filtertype == 0 ) THEN WRITE ( * , '(/21x, a)' ) 'Filter: SEEK' IF ( subtype == 2 ) THEN WRITE ( * , '(6x, a)' ) '-- fixed basis filter with update of matrix U' WRITE ( * , '(6x, a)' ) '-- no re-diagonalization of VUV&#94;T' ELSE IF ( subtype == 3 ) THEN WRITE ( * , '(6x, a)' ) '-- fixed basis filter & no update of matrix U' WRITE ( * , '(6x, a)' ) '-- no re-diagonalization of VUV&#94;T' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(13x, a, i5)' ) 'number of EOFs:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( subtype /= 5 ) THEN IF (( int_rediag > 0 ) . AND . (( subtype /= 2 ) . OR . ( subtype /= 3 ))) & WRITE ( * , '(10x, a, i4, a)' ) & 'Re-diag each ' , int_rediag , '-th analysis step' ELSE IF ( int_rediag == 1 ) THEN WRITE ( * , '(10x, a)' ) 'Perform re-diagonalization' ELSE WRITE ( * , '(10x, a)' ) 'No re-diagonalization' END IF END IF ELSE IF ( filtertype == 1 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: SEIK' IF ( subtype == 2 ) THEN WRITE ( * , '(6x, a)' ) '-- fixed error-space basis' ELSE IF ( subtype == 3 ) THEN WRITE ( * , '(6x, a)' ) '-- fixed state covariance matrix' ELSE IF ( subtype == 4 ) THEN WRITE ( * , '(6x, a)' ) '-- use ensemble transformation' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF ELSE IF ( filtertype == 2 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: EnKF' IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF IF ( rank_analysis_enkf > 0 ) THEN WRITE ( * , '(6x, a, i5)' ) & 'analysis with pseudo-inverse of HPH, rank:' , rank_analysis_enkf END IF ELSE IF ( filtertype == 3 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: LSEIK' IF ( subtype == 2 ) THEN WRITE ( * , '(6x, a)' ) '-- fixed error-space basis' ELSE IF ( subtype == 3 ) THEN WRITE ( * , '(6x, a)' ) '-- fixed state covariance matrix' ELSE IF ( subtype == 4 ) THEN WRITE ( * , '(6x, a)' ) '-- use ensemble transformation' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF ELSE IF ( filtertype == 4 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: ETKF' IF ( subtype == 0 ) THEN WRITE ( * , '(6x, a)' ) '-- Variant using T-matrix' ELSE IF ( subtype == 1 ) THEN WRITE ( * , '(6x, a)' ) '-- Variant following Hunt et al. (2007)' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF ELSE IF ( filtertype == 5 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: LETKF' IF ( subtype == 0 ) THEN WRITE ( * , '(6x, a)' ) '-- Variant using T-matrix' ELSE IF ( subtype == 1 ) THEN WRITE ( * , '(6x, a)' ) '-- Variant following Hunt et al. (2007)' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF ELSE IF ( filtertype == 6 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: ESTKF' IF ( subtype == 0 ) THEN WRITE ( * , '(6x, a)' ) '-- Standard mode' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF ELSE IF ( filtertype == 7 ) THEN WRITE ( * , '(21x, a)' ) 'Filter: LESTKF' IF ( subtype == 0 ) THEN WRITE ( * , '(6x, a)' ) '-- Standard mode' ELSE IF ( subtype == 5 ) THEN WRITE ( * , '(6x, a)' ) '-- Offline mode' END IF WRITE ( * , '(14x, a, i5)' ) 'ensemble size:' , dim_ens IF ( subtype /= 5 ) WRITE ( * , '(6x, a, i5)' ) 'Assimilation interval:' , delt_obs WRITE ( * , '(10x, a, f5.2)' ) 'forgetting factor:' , forget IF ( model_error ) THEN WRITE ( * , '(6x, a, f5.2)' ) 'model error amplitude:' , model_err_amp END IF END IF END SUBROUTINE init_info_pdaf","tags":"","loc":"proc/init_info_pdaf.html"},{"title":"read_config_pdaf – NEMO4 PDAFOMI","text":"public subroutine read_config_pdaf() Uses mod_parallel_pdaf mod_assimilation_pdaf This routine reads the namelist file with parameters\n controlling data assimilation with PDAF and outputs to\n screen. Calling Sequence Called from: init_pdaf Arguments None Contents Variables nmlfile Source Code read_config_pdaf Variables Type Visibility Attributes Name Initial character(len=lc), public :: nmlfile Namelist file Source Code SUBROUTINE read_config_pdaf () USE mod_parallel_pdaf , & ONLY : mype_ens USE mod_assimilation_pdaf , & ONLY : filtertype , subtype , dim_ens , delt_obs , & screen , forget , local_range , locweight , srange , istate_t , & istate_s , istate_u , istate_v , istate_ssh , salfixmin !> Namelist file CHARACTER ( lc ) :: nmlfile NAMELIST / pdaf_nml / filtertype , subtype , dim_ens , & delt_obs , screen , forget , local_range , locweight , & srange , istate_s , istate_t , istate_u , istate_v , istate_ssh , & salfixmin ! **************************************************** ! ***   Initialize PDAF parameters from namelist   *** ! **************************************************** nmlfile = 'namelist.pdaf' OPEN ( 20 , file = nmlfile ) READ ( 20 , NML = pdaf_nml ) CLOSE ( 20 ) ! Print PDAF parameters to screen showconf : IF ( mype_ens == 0 ) THEN WRITE ( * , '(/1x,a)' ) '-- Overview of PDAF configuration --' WRITE ( * , '(3x,a)' ) 'PDAF [pdaf_nml]:' WRITE ( * , '(5x,a,i10)' ) 'filtertype   ' , filtertype WRITE ( * , '(5x,a,i10)' ) 'subtype      ' , subtype WRITE ( * , '(5x,a,i10)' ) 'dim_ens      ' , dim_ens WRITE ( * , '(5x,a,i10)' ) 'delt_obs     ' , delt_obs WRITE ( * , '(5x,a,i10)' ) 'screen       ' , screen WRITE ( * , '(5x,a,f10.2)' ) 'forget       ' , forget WRITE ( * , '(5x,a,es10.2)' ) 'local_range  ' , local_range WRITE ( * , '(5x,a,i10)' ) 'locweight    ' , locweight WRITE ( * , '(5x,a,es10.2)' ) 'srange       ' , srange WRITE ( * , '(5x,a,a)' ) 'istate_t   ' , istate_t WRITE ( * , '(5x,a,a)' ) 'istate_s   ' , istate_s WRITE ( * , '(5x,a,a)' ) 'istate_u   ' , istate_u WRITE ( * , '(5x,a,a)' ) 'istate_v   ' , istate_v WRITE ( * , '(5x,a,a)' ) 'istate_ssh ' , istate_ssh WRITE ( * , '(5x,a,es10.2)' ) 'salfixmin  ' , salfixmin WRITE ( * , '(1x,a)' ) '-- End of PDAF configuration overview --' END IF showconf END SUBROUTINE read_config_pdaf","tags":"","loc":"proc/read_config_pdaf.html"},{"title":"add_noise – NEMO4 PDAFOMI","text":"public subroutine add_noise(dim_obs_p, obs) Routine to add model error. Arguments Type Intent Optional Attributes Name integer, intent(in) :: dim_obs_p Number of process-local observations real, intent(inout) :: obs (dim_obs_p) Process-local observations Contents Variables firststep iseed noise Source Code add_noise Variables Type Visibility Attributes Name Initial logical, public, SAVE :: firststep = .TRUE. Flag for first call integer, public, SAVE :: iseed (4) Seed for random number generator real, public, ALLOCATABLE :: noise (:) Random noise Source Code SUBROUTINE add_noise ( dim_obs_p , obs ) !> Number of process-local observations INTEGER , INTENT ( in ) :: dim_obs_p !> Process-local observations REAL , INTENT ( inout ) :: obs ( dim_obs_p ) !> Random noise REAL , ALLOCATABLE :: noise (:) !> Seed for random number generator INTEGER , SAVE :: iseed ( 4 ) !> Flag for first call LOGICAL , SAVE :: firststep = . TRUE . ! Seeds taken from PDAF Lorenz96 routine IF ( firststep ) THEN WRITE ( * , '(9x, a)' ) '--- Initialize seed for ssh_mgrid noise' iseed ( 1 ) = 2 * 220 + 1 iseed ( 2 ) = 2 * 100 + 5 iseed ( 3 ) = 2 * 10 + 7 iseed ( 4 ) = 2 * 30 + 9 firststep = . FALSE . END IF ! Generate random Gaussian noise ALLOCATE ( noise ( dim_obs_p )) CALL dlarnv ( 3 , iseed , dim_obs_p , noise ) obs = obs + ( noise_amp_ssh_mgrid * noise ) DEALLOCATE ( noise ) END SUBROUTINE add_noise","tags":"","loc":"proc/add_noise.html"},{"title":"init_dim_obs_l_ssh_mgrid – NEMO4 PDAFOMI","text":"public subroutine init_dim_obs_l_ssh_mgrid(domain_p, step, dim_obs, dim_obs_l) Uses PDAFomi mod_assimilation_pdaf Initialize local information on the module-type observation The routine is called during the loop over all local\nanalysis domains. It has to initialize the information\nabout local ssh observations. This routine calls the routine PDAFomi_init_dim_obs_l for each observation type. The call allows to specify a\ndifferent localization radius and localization functions\nfor each observation type and local analysis domain. Arguments Type Intent Optional Attributes Name integer, intent(in) :: domain_p Index of current local analysis domain integer, intent(in) :: step Current time step integer, intent(in) :: dim_obs Full dimension of observation vector integer, intent(out) :: dim_obs_l Local dimension of observation vector Contents Source Code init_dim_obs_l_ssh_mgrid Source Code SUBROUTINE init_dim_obs_l_ssh_mgrid ( domain_p , step , dim_obs , dim_obs_l ) USE PDAFomi , & ONLY : PDAFomi_init_dim_obs_l USE mod_assimilation_pdaf , & ONLY : coords_l , local_range , locweight , srange !> Index of current local analysis domain INTEGER , INTENT ( in ) :: domain_p !> Current time step INTEGER , INTENT ( in ) :: step !> Full dimension of observation vector INTEGER , INTENT ( in ) :: dim_obs !> Local dimension of observation vector INTEGER , INTENT ( out ) :: dim_obs_l ! ********************************************** ! *** Initialize local observation dimension *** ! ********************************************** CALL PDAFomi_init_dim_obs_l ( thisobs_l , thisobs , coords_l , & locweight , local_range , srange , dim_obs_l ) END SUBROUTINE init_dim_obs_l_ssh_mgrid","tags":"","loc":"proc/init_dim_obs_l_ssh_mgrid.html"},{"title":"init_dim_obs_ssh_mgrid – NEMO4 PDAFOMI","text":"public subroutine init_dim_obs_ssh_mgrid(step, dim_obs) Uses PDAFomi mod_assimilation_pdaf mod_statevector_pdaf mod_parallel_pdaf par_oce dom_oce Initialize information on the observation The routine is called by each filter process.\n at the beginning of the analysis step before\n the loop through all local analysis domains. It has to count the number of process-local and full\n observations, initialize the vector of observations\n and their inverse variances, initialize the coordinate\n array and index array for indices of observed elements\n of the state vector. The following four variables have to be initialized in this routine: thisobs%doassim - Whether to assimilate ssh thisobs%disttype - type of distance computation for localization\n with ssh thisobs%ncoord - number of coordinates used for distance\n computation thisobs%id_obs_p - index of module-type observation in PE-local state\n vector Optional is the use of: thisobs%icoeff_p - Interpolation coefficients for obs. operator\n (only if interpolation is used) thisobs%domainsize - Size of domain for periodicity for disttype=1 (<0 for no periodicity) thisobs%obs_err_type - Type of observation errors for particle filter\n and NETF thisobs%use_global_obs - Whether to use global observations or\n restrict the observations to the relevant ones (default: .true. i.e use\n global full observations) The following variables are set in the routine gather_obs: thisobs%dim_obs_p - PE-local number of ssh observations thisobs%dim_obs - full number of ssh observations thisobs%obs_f - full vector of ssh observations thisobs%ocoord_f - coordinates of observations in OBS_MOD_F thisobs%ivar_obs_f - full vector of inverse obs. error variances of\n module-type thisobs%dim_obs_g - Number of global observations (only if use_global_obs=.false ) thisobs%id_obs_f_lim - Ids of full observations in global observations\n (if use_global_obs=.false ) Arguments Type Intent Optional Attributes Name integer, intent(in) :: step Current time step integer, intent(inout) :: dim_obs Dimension of full observation vector Contents Variables cnt cnt0_p cnt_p dim_obs_p i i0 i_obs id_var ivar_obs_p j j0 j_obs nc_step ncid_in obs obs_p ocoord_p pos rad_conv s stat Source Code init_dim_obs_ssh_mgrid Variables Type Visibility Attributes Name Initial integer, public :: cnt (3) NetCDF position arrays for 3D field integer, public :: cnt0_p Counters integer, public :: cnt_p Counters integer, public :: dim_obs_p Number of process-local observations integer, public :: i Counters integer, public :: i0 Halo offset for local PE integer, public :: i_obs Global gridbox coordinates of observations integer, public :: id_var IDs for fields real(kind=pwp), public, ALLOCATABLE :: ivar_obs_p (:) PE-local inverse observation error variance integer, public :: j Counters integer, public :: j0 Halo offset for local PE integer, public :: j_obs Global gridbox coordinates of observations integer, public :: nc_step = 0 Step for observations in NetCDF file integer, public :: ncid_in ID for NetCDF file real(kind=pwp), public, ALLOCATABLE :: obs (:,:,:) Global observation field real(kind=pwp), public, ALLOCATABLE :: obs_p (:) PE-local observation vector real(kind=pwp), public, ALLOCATABLE :: ocoord_p (:,:) PE-local observation coordinates integer, public :: pos (3) NetCDF position arrays for 3D field real(kind=pwp), public :: rad_conv = 3.141592653589793/180.0 Degree to radian conversion integer, public :: s Counters integer, public :: stat (50) Status array for NetCDF operations Source Code SUBROUTINE init_dim_obs_ssh_mgrid ( step , dim_obs ) USE PDAFomi , & ONLY : PDAFomi_gather_obs USE mod_assimilation_pdaf , & ONLY : filtertype , local_range , delt_obs USE mod_statevector_pdaf , & ONLY : mpi_subd_lon , mpi_subd_lat USE mod_parallel_pdaf , & ONLY : COMM_filter USE par_oce , & ONLY : jpiglo , jpjglo USE dom_oce , & ONLY : nldj , nldi , glamt , gphit , nimpp , njmpp , ndastp !> Current time step INTEGER , INTENT ( in ) :: step !> Dimension of full observation vector INTEGER , INTENT ( inout ) :: dim_obs !> Counters INTEGER :: i , j , s !> Step for observations in NetCDF file INTEGER :: nc_step = 0 !> Status array for NetCDF operations INTEGER :: stat ( 50 ) !> ID for NetCDF file INTEGER :: ncid_in !> IDs for fields INTEGER :: id_var !> NetCDF position arrays for 3D field INTEGER :: pos ( 3 ), cnt ( 3 ) !> Global observation field REAL ( pwp ), ALLOCATABLE :: obs (:, :, :) !> Number of process-local observations INTEGER :: dim_obs_p !> Counters INTEGER :: cnt_p , cnt0_p !> Global gridbox coordinates of observations INTEGER :: i_obs , j_obs !> Halo offset for local PE INTEGER :: i0 , j0 !> PE-local observation vector REAL ( pwp ), ALLOCATABLE :: obs_p (:) !> PE-local inverse observation error variance REAL ( pwp ), ALLOCATABLE :: ivar_obs_p (:) !> PE-local observation coordinates REAL ( pwp ), ALLOCATABLE :: ocoord_p (:, :) !> Degree to radian conversion REAL ( pwp ) :: rad_conv = 3.141592653589793 / 18 0.0 ! ***************************** ! *** Global setting config *** ! ***************************** IF ( mype_filter == 0 ) & WRITE ( * , '(8x,a)' ) 'Assimilate observations - obs_ssh_mgrid' ! Store whether to assimilate this observation type (used in routines ! below) IF ( assim_ssh_mgrid ) thisobs % doassim = 1 ! Specify type of distance computation thisobs % disttype = 3 ! 3=Haversine ! Number of coordinates used for distance computation. ! The distance compution starts from the first row thisobs % ncoord = 2 ! ********************************** ! *** Read PE-local observations *** ! ********************************** ! Format of ndastp is YYYYMMDD IF ( mype_filter == 0 ) WRITE ( * , '(/9x, a, i8)' ) & 'obs_ssh_mgrid current date:' , ndastp s = 1 stat ( s ) = NF90_OPEN ( file_ssh_mgrid , NF90_NOWRITE , ncid_in ) s = s + 1 stat ( s ) = NF90_INQ_VARID ( ncid_in , 'sossheig' , id_var ) s = s + 1 ALLOCATE ( obs ( jpiglo , jpjglo , 1 )) ! Increment time in NetCDF file so correct obs read nc_step = nc_step + delt_obs pos = ( / 1 , 1 , nc_step / ) cnt = ( / jpiglo , jpjglo , 1 / ) stat ( s ) = NF90_GET_VAR ( ncid_in , id_var , obs , start = pos , count = cnt ) s = s + 1 stat ( s ) = NF90_CLOSE ( ncid_in ) s = s + 1 DO j = 1 , s - 1 IF ( stat ( j ) . NE . NF90_NOERR ) THEN WRITE ( * , '(/9x, a, 3x, a, 3x, a, i2)' ) & 'NetCDF error in reading obs file:' , file_ssh_mgrid , & 'status array, j=' , j CALL abort_parallel () END IF END DO ! *********************************************************** ! *** Count available observations for the process domain *** ! *** and initialize index and coordinate arrays.         *** ! *********************************************************** ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 cnt_p = 0 DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon ! Convert to global coordinates i_obs = nimpp + i0 + i - 1 j_obs = njmpp + j0 + j - 1 cnt_p = cnt_p + 1 END DO END DO ! Set number of local observations dim_obs_p = cnt_p IF ( cnt_p == 0 ) WRITE ( * , '(/9x, a, i3, 3x, a, i4)' ) & 'WARNING: No ssh_mgrid observations on PE:' , mype_filter , & 'NetCDF file step=' , nc_step obs_nonzero : IF ( dim_obs_p > 0 ) THEN ! Vector of observations on the process sub-domain ALLOCATE ( obs_p ( dim_obs_p )) ! Coordinate array of observations on the process sub-domain ALLOCATE ( ocoord_p ( 2 , dim_obs_p )) ! Coordinate array for observation operator ALLOCATE ( thisobs % id_obs_p ( 1 , dim_obs_p )) ALLOCATE ( ivar_obs_p ( dim_obs_p )) ! Compute halo offset i0 = nldi - 1 j0 = nldj - 1 cnt_p = 0 cnt0_p = 0 DO j = 1 , mpi_subd_lat DO i = 1 , mpi_subd_lon ! State vector index counter for observation operator. cnt0_p = cnt0_p + 1 ! Convert to global coordinates. i_obs = nimpp + i0 + i - 1 j_obs = njmpp + j0 + j - 1 cnt_p = cnt_p + 1 obs_p ( cnt_p ) = obs ( i_obs , j_obs , 1 ) ! Observation coordinates - must be in radians for PDAFOMI ocoord_p ( 1 , cnt_p ) = glamt ( i + i0 , j + j0 ) * rad_conv ocoord_p ( 2 , cnt_p ) = gphit ( i + i0 , j + j0 ) * rad_conv ! Coordinates for observation operator (gridpoint) thisobs % id_obs_p ( 1 , cnt_p ) = cnt0_p END DO END DO ELSE ! No observations on PE, create dummy arrays to pass to PDAFOMI ALLOCATE ( obs_p ( 1 )) ALLOCATE ( ivar_obs_p ( 1 )) ALLOCATE ( ocoord_p ( 2 , 1 )) ALLOCATE ( thisobs % id_obs_p ( 1 , 1 )) obs_p = - 99999 9.0 ivar_obs_p = EPSILON ( ivar_obs_p ) ocoord_p = 0 thisobs % id_obs_p = 1 END IF obs_nonzero ! **************************************************************** ! *** Define observation errors for process-local observations *** ! **************************************************************** ! Set inverse observation error variances ivar_obs_p (:) = 1.0 / ( rms_ssh_mgrid * rms_ssh_mgrid ) ! ********************************************************* ! *** For twin experiment: Read synthetic observations  *** ! ********************************************************* IF ( twin_exp_ssh_mgrid ) THEN IF ( dim_obs_p > 0 ) CALL add_noise ( dim_obs_p , obs_p ) END IF ! **************************************** ! *** Gather global observation arrays *** ! **************************************** CALL PDAFomi_gather_obs ( thisobs , dim_obs_p , obs_p , ivar_obs_p , ocoord_p , & thisobs % ncoord , local_range , dim_obs ) ! ******************** ! *** Finishing up *** ! ******************** ! Deallocate all local arrays DEALLOCATE ( obs ) DEALLOCATE ( obs_p , ocoord_p , ivar_obs_p ) ! Arrays in THISOBS have to be deallocated after the analysis step ! by a call to deallocate_obs() in prepoststep_pdaf. END SUBROUTINE init_dim_obs_ssh_mgrid","tags":"","loc":"proc/init_dim_obs_ssh_mgrid.html"},{"title":"obs_op_ssh_mgrid – NEMO4 PDAFOMI","text":"public subroutine obs_op_ssh_mgrid(dim_p, dim_obs, state_p, ostate) Uses PDAFomi Implementation of observation operator This routine applies the full observation operator\nfor the ssh observations. The routine is called by all filter processes. Arguments Type Intent Optional Attributes Name integer, intent(in) :: dim_p PE-local state dimension integer, intent(in) :: dim_obs Dimension of full observed state (all observed fields) real(kind=pwp), intent(in) :: state_p (dim_p) PE-local model state real(kind=pwp), intent(inout) :: ostate (dim_obs) Full observed state Contents Source Code obs_op_ssh_mgrid Source Code SUBROUTINE obs_op_ssh_mgrid ( dim_p , dim_obs , state_p , ostate ) USE PDAFomi , & ONLY : PDAFomi_obs_op_gridpoint !> PE-local state dimension INTEGER , INTENT ( in ) :: dim_p !> Dimension of full observed state (all observed fields) INTEGER , INTENT ( in ) :: dim_obs !> PE-local model state REAL ( pwp ), INTENT ( in ) :: state_p ( dim_p ) !> Full observed state REAL ( pwp ), INTENT ( inout ) :: ostate ( dim_obs ) ! ****************************************************** ! *** Apply observation operator H on a state vector *** ! ****************************************************** IF ( thisobs % doassim == 1 ) THEN CALL PDAFomi_obs_op_gridpoint ( thisobs , state_p , ostate ) END IF END SUBROUTINE obs_op_ssh_mgrid","tags":"","loc":"proc/obs_op_ssh_mgrid.html"},{"title":"assimilate_pdaf – NEMO4 PDAFOMI","text":"public subroutine assimilate_pdaf() Uses pdaf_interfaces_module mod_parallel_pdaf Performing the Assimilation Step This routine is called during the model integrations at each timestep.\nIt calls PDAF to check whether the forecast phase is completed and if\nso, PDAF will perform the analysis step. Calling Sequence Called from: `step.F90’ Calls: PDAFomi_assimilate_local Arguments None Contents Variables localfilter status_pdaf Source Code assimilate_pdaf Variables Type Visibility Attributes Name Initial integer, public :: localfilter Flag for domain-localized filter (1=true) integer, public :: status_pdaf PDAF status flag Source Code SUBROUTINE assimilate_pdaf () USE pdaf_interfaces_module , & ONLY : PDAFomi_assimilate_local , PDAF_get_localfilter USE mod_parallel_pdaf , & ONLY : mype_ens , abort_parallel !> PDAF status flag INTEGER :: status_pdaf !> Flag for domain-localized filter (1=true) INTEGER :: localfilter ! Collect a state vector from model fields EXTERNAL :: collect_state_pdaf ! Distribute a state vector to model fields EXTERNAL :: distribute_state_pdaf ! Provide time step of next observation EXTERNAL :: next_observation_pdaf ! User supplied pre/poststep routine EXTERNAL :: prepoststep_ens_pdaf ! Provide number of local analysis domains EXTERNAL :: init_n_domains_pdaf ! Initialize state dimension for local analysis domain EXTERNAL :: init_dim_l_pdaf ! Get state on local analysis domain from global state EXTERNAL :: g2l_state_pdaf ! Update global state from state on local analysis domain EXTERNAL :: l2g_state_pdaf ! Get dimension of full obs. vector for PE-local domain EXTERNAL :: init_dim_obs_pdafomi ! Obs. operator for full obs. vector for PE-local domain EXTERNAL :: obs_op_pdafomi ! Get dimension of obs. vector for local analysis domain EXTERNAL :: init_dim_obs_l_pdafomi ! ********************************* ! *** Call assimilation routine *** ! ********************************* ! Check  whether the filter is domain-localized CALL PDAF_get_localfilter ( localfilter ) IF ( localfilter == 1 ) THEN CALL PDAFomi_assimilate_local ( collect_state_pdaf , & distribute_state_pdaf , init_dim_obs_pdafomi , obs_op_pdafomi , & prepoststep_ens_pdaf , init_n_domains_pdaf , init_dim_l_pdaf , & init_dim_obs_l_pdafomi , g2l_state_pdaf , l2g_state_pdaf , & next_observation_pdaf , status_pdaf ) ELSE WRITE ( * , '(a)' ) 'ERROR - global filter not implemented, stopping.' CALL abort_parallel () END IF ! Check for errors during execution of PDAF IF ( status_pdaf /= 0 ) THEN WRITE ( * , '(/1x,a6,i3,a43,i4,a1/)' ) & 'ERROR ' , status_pdaf , & ' in PDAF_put_state - stopping! (PE ' , mype_ens , ')' CALL abort_parallel () END IF END SUBROUTINE assimilate_pdaf","tags":"","loc":"proc/assimilate_pdaf.html"},{"title":"asm_inc_init_pdaf – NEMO4 PDAFOMI","text":"public subroutine asm_inc_init_pdaf() This routine initialises the arrays for the IAU. The arrays\n* must be initialised to zero, as PDAF does not compute an\nanalysis update for all gridpoints. (In this case, no increment\nshould be added to the model at this gridpoint.) The arrays are deallocated in finalise_pdaf . Arguments None Contents Source Code asm_inc_init_pdaf Source Code SUBROUTINE asm_inc_init_pdaf () ! 2D variables ALLOCATE ( ssh_iau_pdaf ( jpi , jpj )) ssh_iau_pdaf = 0._pwp ! 3D variables ALLOCATE ( t_iau_pdaf ( jpi , jpj , jpk ), s_iau_pdaf ( jpi , jpj , jpk )) t_iau_pdaf = 0._pwp s_iau_pdaf = 0._pwp ALLOCATE ( u_iau_pdaf ( jpi , jpj , jpk ), v_iau_pdaf ( jpi , jpj , jpk )) u_iau_pdaf = 0._pwp v_iau_pdaf = 0._pwp END SUBROUTINE asm_inc_init_pdaf","tags":"","loc":"proc/asm_inc_init_pdaf.html"},{"title":"dyn_asm_inc_pdaf – NEMO4 PDAFOMI","text":"public subroutine dyn_asm_inc_pdaf(kt) Uses mod_assimilation_pdaf in_out_manager oce This routine is almost identical to a similar routine\nfrom NEMOVAR. It applies the IAU to the dynamical fields. Called from: step.F90 Arguments Type Intent Optional Attributes Name integer, intent(in) :: kt Current time step Contents Variables jk Source Code dyn_asm_inc_pdaf Variables Type Visibility Attributes Name Initial integer, public :: jk Counter Source Code SUBROUTINE dyn_asm_inc_pdaf ( kt ) USE mod_assimilation_pdaf , & ONLY : delt_obs USE in_out_manager , & ONLY : nit000 USE oce , & ONLY : ua , va !> Current time step INTEGER , INTENT ( IN ) :: kt !> Counter INTEGER :: jk ! Check whether to update the dynamic tendencies IF ( MOD ( kt - nit000 , delt_obs ) == 0 . AND . kt > nit000 ) THEN DO jk = 1 , jpkm1 ua (:, :, jk ) = ua (:, :, jk ) + u_iau_pdaf (:, :, jk ) va (:, :, jk ) = va (:, :, jk ) + v_iau_pdaf (:, :, jk ) END DO END IF END SUBROUTINE dyn_asm_inc_pdaf","tags":"","loc":"proc/dyn_asm_inc_pdaf.html"},{"title":"ssh_asm_div_pdaf – NEMO4 PDAFOMI","text":"public subroutine ssh_asm_div_pdaf(kt, phdivn) Uses mod_assimilation_pdaf mod_parallel_pdaf dom_oce in_out_manager This routine is almost identical to a similar routine\nfrom NEMOVAR. It applies the IAU to the ssh divergence term. Currently, the implementation is only valid when using the\nlinear free surface assumption. Called from: divhor.F90 Arguments Type Intent Optional Attributes Name integer, intent(in) :: kt Current time step real(kind=pwp), intent(inout), DIMENSION(:, :, :) :: phdivn Horizontal divergence Contents Source Code ssh_asm_div_pdaf Source Code SUBROUTINE ssh_asm_div_pdaf ( kt , phdivn ) USE mod_assimilation_pdaf , & ONLY : delt_obs USE mod_parallel_pdaf , & ONLY : abort_parallel USE dom_oce , & ONLY : e3t_n , ln_linssh , tmask USE in_out_manager , & ONLY : nit000 !> Current time step INTEGER , INTENT ( IN ) :: kt !> Horizontal divergence REAL ( pwp ), DIMENSION (:, :, :), INTENT ( inout ) :: phdivn ! Check whether to update the tracer tendencies IF ( MOD ( kt - nit000 , delt_obs ) == 0 . AND . kt > nit000 ) THEN ! NEMO-PDAF currently only implemented for linear free surface. IF ( ln_linssh ) THEN phdivn (:, :, 1 ) = phdivn (:, :, 1 ) - & ssh_iau_pdaf (:, :) / e3t_n (:, :, 1 ) * tmask (:, :, 1 ) ELSE WRITE ( * , '(/9x, a)' ) & 'NEMO-PDAF has not yet been implemented for nonlinear free & &surface (see ssh_asm_div_pdaf).' CALL abort_parallel () END IF END IF END SUBROUTINE ssh_asm_div_pdaf","tags":"","loc":"proc/ssh_asm_div_pdaf.html"},{"title":"tra_asm_inc_pdaf – NEMO4 PDAFOMI","text":"public subroutine tra_asm_inc_pdaf(kt) Uses mod_assimilation_pdaf eosbn2 dom_oce in_out_manager oce This routine is almost identical to a similar routine\nfrom NEMOVAR. It applies the IAU to the tracer fields. Called from: step.F90 Arguments Type Intent Optional Attributes Name integer, intent(in) :: kt Current time step Contents Variables fzptnz jk Source Code tra_asm_inc_pdaf Variables Type Visibility Attributes Name Initial real(kind=pwp), public, DIMENSION(jpi, jpj, jpk) :: fzptnz ! 3d freezing point values\n Nick: Taken from NEMOVAR. Will this lead to stack overflow? integer, public :: jk Counter Source Code SUBROUTINE tra_asm_inc_pdaf ( kt ) USE mod_assimilation_pdaf , & ONLY : delt_obs , salfixmin USE eosbn2 , & ONLY : eos_fzp USE dom_oce , & ONLY : gdept_n USE in_out_manager , & ONLY : nit000 USE oce , & ONLY : tsn , tsa !> Current time step INTEGER , INTENT ( IN ) :: kt !> Counter INTEGER :: jk !> ! 3d freezing point values !> Nick: Taken from NEMOVAR. Will this lead to stack overflow? REAL ( pwp ), DIMENSION ( jpi , jpj , jpk ) :: fzptnz ! Freezing point calculation taken from oc_fz_pt (but calculated for ! all depths). Used to prevent the applied increments taking the ! temperature below the local freezing point. DO jk = 1 , jpkm1 CALL eos_fzp ( tsn (:, :, jk , jp_sal ), fzptnz (:, :, jk ), gdept_n (:, :, jk )) END DO ! Check whether to update the tracer tendencies IF ( MOD ( kt - nit000 , delt_obs ) == 0 . AND . kt > nit000 ) THEN ! Do not apply nonnegative increments. ! Do not apply increments if the temperature will fall below freezing ! or if the salinity will fall below a specified minimum value. DO jk = 1 , jpkm1 WHERE ( t_iau_pdaf (:, :, jk ) > 0.0_pwp . OR . & tsn (:, :, jk , jp_tem ) + tsa (:, :, jk , jp_tem ) + t_iau_pdaf (:, :, jk ) & > fzptnz (:, :, jk )) tsa (:, :, jk , jp_tem ) = tsa (:, :, jk , jp_tem ) + t_iau_pdaf (:, :, jk ) END WHERE WHERE ( s_iau_pdaf (:, :, jk ) > 0.0_pwp . OR . & tsn (:, :, jk , jp_sal ) + tsa (:, :, jk , jp_sal ) + s_iau_pdaf (:, :, jk ) & > salfixmin ) tsa (:, :, jk , jp_sal ) = tsa (:, :, jk , jp_sal ) + s_iau_pdaf (:, :, jk ) END WHERE END DO END IF END SUBROUTINE tra_asm_inc_pdaf","tags":"","loc":"proc/tra_asm_inc_pdaf.html"},{"title":"calc_mpi_dim – NEMO4 PDAFOMI","text":"public subroutine calc_mpi_dim() Uses par_oce dom_oce This routine calculates the dimensions of the MPI subdomain that is used to fill the local statevector. Calling Sequence Called from: calc_statevar_dim Arguments None Contents Source Code calc_mpi_dim Source Code SUBROUTINE calc_mpi_dim () USE par_oce , ONLY : jpk USE dom_oce , ONLY : nldi , nldj , nlei , nlej mpi_subd_lon = nlei - nldi + 1 mpi_subd_lat = nlej - nldj + 1 mpi_subd_vert = jpk END SUBROUTINE calc_mpi_dim","tags":"","loc":"proc/calc_mpi_dim.html"},{"title":"calc_offset – NEMO4 PDAFOMI","text":"public subroutine calc_offset() This routine calculates the offset values for each of the local statevector variables. It then stores the 2d/3d offset values in separate arrays. Calling Sequence Called from: calc_statevector_dim Calls: calc_statevar_dim Arguments None Contents Source Code calc_offset Source Code SUBROUTINE calc_offset () ! Compute local statevector dimensions CALL calc_statevar_dim () ssh_p_offset = 0 t_p_offset = ssh_p_offset + ssh_p_dim s_p_offset = t_p_offset + t_p_dim u_p_offset = s_p_offset + s_p_dim v_p_offset = u_p_offset + u_p_dim ! Fill array of 2D state variable offsets for local PE var2d_p_offset ( 1 ) = ssh_p_offset ! Fill array of 3D state variable offsets for local PE var3d_p_offset ( 1 ) = t_p_offset var3d_p_offset ( 2 ) = s_p_offset var3d_p_offset ( 3 ) = u_p_offset var3d_p_offset ( 4 ) = v_p_offset END SUBROUTINE calc_offset","tags":"","loc":"proc/calc_offset.html"},{"title":"calc_statevar_dim – NEMO4 PDAFOMI","text":"public subroutine calc_statevar_dim() This routine calculates the dimension of each of the local statevector variables. Calling Sequence Called from: calc_offset Calls: calc_mpi_dim Arguments None Contents Source Code calc_statevar_dim Source Code SUBROUTINE calc_statevar_dim () ! Compute MPI subdomain dimensions CALL calc_mpi_dim () ssh_p_dim = mpi_subd_lat * mpi_subd_lon t_p_dim = mpi_subd_lat * mpi_subd_lon * mpi_subd_vert s_p_dim = mpi_subd_lat * mpi_subd_lon * mpi_subd_vert u_p_dim = mpi_subd_lat * mpi_subd_lon * mpi_subd_vert v_p_dim = mpi_subd_lat * mpi_subd_lon * mpi_subd_vert END SUBROUTINE calc_statevar_dim","tags":"","loc":"proc/calc_statevar_dim.html"},{"title":"calc_statevector_dim – NEMO4 PDAFOMI","text":"public subroutine calc_statevector_dim(dim_p) This routine calculates the dimension of the local statevector. Calling Sequence Called from: init_pdaf Calls: calc_offset Arguments Type Intent Optional Attributes Name integer, intent(inout) :: dim_p Local statevector dimension Contents Source Code calc_statevector_dim Source Code SUBROUTINE calc_statevector_dim ( dim_p ) !> Local statevector dimension INTEGER , INTENT ( inout ) :: dim_p ! Calculate statevector variable offset and dimension. ! *DO NOT REMOVE* as offset is not calculated anywhere else. CALL calc_offset () dim_p = ssh_p_dim + t_p_dim + s_p_dim + u_p_dim + v_p_dim END SUBROUTINE calc_statevector_dim","tags":"","loc":"proc/calc_statevector_dim.html"},{"title":"fill2d_ensarray – NEMO4 PDAFOMI","text":"public subroutine fill2d_ensarray(fname, ens_p) Uses netcdf Fill local ensemble array with 2d state variables from initial state file. Todo User should give values to the statevector\n for the DA. Arguments Type Intent Optional Attributes Name character(len=lc), intent(in) :: fname Name of netCDF file real(kind=pwp), intent(inout) :: ens_p (:,:) PE-local state ensemble Contents Source Code fill2d_ensarray Source Code SUBROUTINE fill2d_ensarray ( fname , ens_p ) USE netcdf !> Name of netCDF file CHARACTER ( lc ), INTENT ( in ) :: fname !> PE-local state ensemble REAL ( pwp ), INTENT ( inout ) :: ens_p (:, :) WRITE ( * , '(/1x,a,a)' ) '2D initial state file =' , TRIM ( fname ) WRITE ( * , '(/1x,a)' ) 'Insert routine for reading 2D initial state here.' END SUBROUTINE fill2d_ensarray","tags":"","loc":"proc/fill2d_ensarray.html"},{"title":"fill3d_ensarray – NEMO4 PDAFOMI","text":"public subroutine fill3d_ensarray(fname, statevar, ens_p) Uses netcdf Fill local ensemble array with 3d state variables from initial state file. Todo User should give values to the statevector\n for the DA. Arguments Type Intent Optional Attributes Name character(len=lc), intent(in) :: fname Name of netCDF file character(len=1), intent(in) :: statevar Name of state variable real(kind=pwp), intent(inout) :: ens_p (:,:) PE-local state ensemble Contents Source Code fill3d_ensarray Source Code SUBROUTINE fill3d_ensarray ( fname , statevar , ens_p ) USE netcdf !> Name of netCDF file CHARACTER ( lc ), INTENT ( in ) :: fname !> Name of state variable CHARACTER ( len = 1 ), INTENT ( in ) :: statevar !> PE-local state ensemble REAL ( pwp ), INTENT ( inout ) :: ens_p (:, :) SELECT CASE ( statevar ) CASE ( 'T' ) WRITE ( * , '(/1x,a,a)' ) 'T initial state file =' , TRIM ( fname ) WRITE ( * , '(/1x,a)' ) 'Insert routine for reading T initial state here.' CASE ( 'S' ) WRITE ( * , '(/1x,a,a)' ) 'S initial state file =' , TRIM ( fname ) WRITE ( * , '(/1x,a)' ) 'Insert routine for reading S initial state here.' CASE ( 'U' ) WRITE ( * , '(/1x,a,a)' ) 'U initial state file =' , TRIM ( fname ) WRITE ( * , '(/1x,a)' ) 'Insert routine for reading U initial state here.' CASE ( 'V' ) WRITE ( * , '(/1x,a,a)' ) 'V initial state file =' , TRIM ( fname ) WRITE ( * , '(/1x,a)' ) 'Insert routine for reading V initial state here.' END SELECT END SUBROUTINE fill3d_ensarray","tags":"","loc":"proc/fill3d_ensarray.html"},{"title":"abort_parallel – NEMO4 PDAFOMI","text":"public subroutine abort_parallel() Terminate the MPI execution environment. Arguments None Contents Source Code abort_parallel Source Code SUBROUTINE abort_parallel () CALL MPI_Abort ( MPI_COMM_WORLD , 1 , MPIerr ) END SUBROUTINE abort_parallel","tags":"","loc":"proc/abort_parallel.html"},{"title":"init_parallel_pdaf – NEMO4 PDAFOMI","text":"public subroutine init_parallel_pdaf(screen, mpi_comm) Split the MPI communicator initialised by XIOS into MODEL, FILTER and COUPLE communicators, return MODEL communicator. Calling Sequence Called from lib_mpp.F90 Calls: MPI_Comm_size , MPI_Comm_rank MPI_Comm_split , MPI_Barrier Arguments Type Intent Optional Attributes Name integer, intent(in) :: screen Whether screen information is shown integer, intent(inout) :: mpi_comm Communicator after XIOS splitting Contents Variables color_couple i j my_color nmlfile pe_index tasks Source Code init_parallel_pdaf Variables Type Visibility Attributes Name Initial integer, public :: color_couple Variables for communicator-splitting integer, public :: i Counters integer, public :: j Counters integer, public :: my_color Variables for communicator-splitting character(len=lc), public :: nmlfile namelist file integer, public :: pe_index Index of PE integer, public :: tasks Number of model tasks Source Code SUBROUTINE init_parallel_pdaf ( screen , mpi_comm ) !> Whether screen information is shown INTEGER , INTENT ( in ) :: screen !> Communicator after XIOS splitting INTEGER , INTENT ( inout ) :: mpi_comm !> Counters INTEGER :: i , j !> Index of PE INTEGER :: pe_index !> Variables for communicator-splitting INTEGER :: my_color , color_couple !> Number of model tasks INTEGER :: tasks !> namelist file CHARACTER ( lc ) :: nmlfile ! Number of ensemble members, supplied by PDAF namelist NAMELIST / tasks_nml / tasks ! Read namelist for number of model tasks nmlfile = 'namelist.pdaf' OPEN ( 20 , file = nmlfile ) READ ( 20 , NML = tasks_nml ) CLOSE ( 20 ) n_modeltasks = tasks ! ***              COMM_ENSEMBLE                *** ! *** Generate communicator for ensemble runs   *** ! *** only used to generate model communicators *** COMM_ensemble = mpi_comm CALL MPI_Comm_Size ( COMM_ensemble , npes_ens , MPIerr ) CALL MPI_Comm_Rank ( COMM_ensemble , mype_ens , MPIerr ) ! Initialize communicators for ensemble evaluations IF ( mype_ens == 0 ) THEN WRITE ( * , '(/1x, a)' ) 'Initialize communicators for assimilation with PDAF' END IF ! Store # PEs per ensemble member. Used for info on PE 0 and for ! generation of model communicators on other PEs ALLOCATE ( local_npes_model ( n_modeltasks )) local_npes_model = FLOOR ( REAL ( npes_ens ) / REAL ( n_modeltasks )) DO i = 1 , ( npes_ens - n_modeltasks * local_npes_model ( 1 )) local_npes_model ( i ) = local_npes_model ( i ) + 1 END DO ! ***              COMM_MODEL               *** ! *** Generate communicators for model runs *** pe_index = 0 doens1 : DO i = 1 , n_modeltasks DO j = 1 , local_npes_model ( i ) IF ( mype_ens == pe_index ) THEN task_id = i EXIT doens1 END IF pe_index = pe_index + 1 END DO END DO doens1 CALL MPI_Comm_split ( COMM_ensemble , task_id , mype_ens , & COMM_model , MPIerr ) ! Re-initialize PE information according to model communicator CALL MPI_Comm_Size ( COMM_model , npes_model , MPIerr ) CALL MPI_Comm_Rank ( COMM_model , mype_model , MPIerr ) IF ( screen > 1 ) then WRITE ( * , * ) 'MODEL: mype(w)= ' , mype_ens , '; model task: ' , task_id , & '; mype(m)= ' , mype_model , '; npes(m)= ' , npes_model END IF ! Init flag FILTERPE (all PEs of model task 1) IF ( task_id == 1 ) THEN filterpe = . TRUE . ELSE filterpe = . FALSE . END IF ! ***         COMM_FILTER                 *** ! *** Generate communicator for filter    *** IF ( filterpe ) THEN my_color = task_id ELSE my_color = MPI_UNDEFINED END IF CALL MPI_Comm_split ( COMM_ensemble , my_color , mype_ens , & COMM_filter , MPIerr ) ! Initialize PE information according to filter communicator IF ( filterpe ) THEN CALL MPI_Comm_Size ( COMM_filter , npes_filter , MPIerr ) CALL MPI_Comm_Rank ( COMM_filter , mype_filter , MPIerr ) END IF ! ***              COMM_COUPLE                 *** ! *** Generate communicators for communication *** ! *** between model and filter PEs             *** color_couple = mype_model + 1 CALL MPI_Comm_split ( COMM_ensemble , color_couple , mype_ens , & COMM_couple , MPIerr ) ! Initialize PE information according to coupling communicator CALL MPI_Comm_Size ( COMM_couple , npes_couple , MPIerr ) CALL MPI_Comm_Rank ( COMM_couple , mype_couple , MPIerr ) IF ( screen > 0 ) THEN IF ( mype_ens == 0 ) THEN WRITE ( * , '(/18x, a)' ) 'PE configuration:' WRITE ( * , '(2x, a6, a9, a10, a14, a13, /2x, a5, a9, a7, a7, a7, a7, a7, /2x, a)' ) & 'world' , 'filter' , 'model' , 'couple' , 'filterPE' , & 'rank' , 'rank' , 'task' , 'rank' , 'task' , 'rank' , 'T/F' , & '----------------------------------------------------------' END IF CALL MPI_Barrier ( COMM_ensemble , MPIerr ) IF ( task_id == 1 ) THEN WRITE ( * , '(2x, i4, 4x, i4, 4x, i3, 4x, i3, 4x, i3, 4x, i3, 5x, l3)' ) & mype_ens , mype_filter , task_id , mype_model , color_couple , & mype_couple , filterpe END IF IF ( task_id > 1 ) THEN WRITE ( * , '(2x, i4, 12x, i3, 4x, i3, 4x, i3, 4x, i3, 5x, l3)' ) & mype_ens , task_id , mype_model , color_couple , mype_couple , filterpe END IF CALL MPI_Barrier ( COMM_ensemble , MPIerr ) IF ( mype_ens == 0 ) WRITE ( * , '(/a)' ) '' END IF ! **************************************************** ! *** Re-initialize model equivalent to COMM_model *** ! **************************************************** mpi_comm = COMM_model END SUBROUTINE init_parallel_pdaf","tags":"","loc":"proc/init_parallel_pdaf.html"},{"title":"mod_init_pdaf – NEMO4 PDAFOMI","text":"Initialise PDAF This modules contains the initialisation routine for PDAF init_pdaf . Here the ensemble is initialised and distributed\nand the statevector and state variable information is computed. Uses mod_kind_pdaf Contents Subroutines init_pdaf Subroutines public subroutine init_pdaf () The initialization routine PDAF_init is called\n such that the internal initialization of PDAF is performed.\n The initialization is used to set-up local domain and filter options\n such as the filter type, inflation, and localization radius.\n This variant is for the online mode of PDAF. Read more… Arguments None","tags":"","loc":"module/mod_init_pdaf.html"},{"title":"mod_kind_pdaf – NEMO4 PDAFOMI","text":"Define real precision This module defines the kind of real and length of character strings\nfor the PDAF call-back routines and interfaces. It is based on the NEMO\nmodule par_kind.F90 . Contents Variables lc pdp pwp Variables Type Visibility Attributes Name Initial integer, public, parameter :: lc = 256 integer, public, parameter :: pdp = SELECTED_REAL_KIND(12, 307) double precision integer, public, parameter :: pwp = pdp double precision","tags":"","loc":"module/mod_kind_pdaf.html"},{"title":"mod_util_pdaf – NEMO4 PDAFOMI","text":"Utility Routines This module contains several routines useful for common\nmodel tasks. The initial routines included output configuration\ninformation about the PDAF library, and configuration information\nabout the assimilation parameters. Uses mod_kind_pdaf Contents Subroutines finalize_pdaf init_info_pdaf read_config_pdaf Subroutines public subroutine finalize_pdaf () Timing and clean-up of PDAF Read more… Arguments None public subroutine init_info_pdaf () This routine performs a model-sided screen output about\n the coniguration of the data assimilation system. Read more… Arguments None public subroutine read_config_pdaf () This routine reads the namelist file with parameters\n controlling data assimilation with PDAF and outputs to\n screen. Read more… Arguments None","tags":"","loc":"module/mod_util_pdaf.html"},{"title":"mod_obs_ssh_mgrid_pdafomi – NEMO4 PDAFOMI","text":"PDAF-OMI observation module for ssh observations (on model grid) The subroutines in this module are for the particular handling of\nssh observations available on the model grid. The routines are called by the different call-back routines of PDAF.\nMost of the routines are generic so that in practice only 2 routines\nneed to be adapted for a particular data type. These are the routines\nfor the initialization of the observation information ( init_dim_obs )\nand for the observation operator ( obs_op ). The module uses two derived data type (obs_f and obs_l), which contain\nall information about the full and local observations. Only variables\nof the type obs_f need to be initialized in this module. The variables\nin the type obs_l are initialized by the generic routines from PDAFomi . Uses mod_kind_pdaf mod_parallel_pdaf PDAFomi netcdf Contents Variables assim_ssh_mgrid file_ssh_mgrid noise_amp_ssh_mgrid rms_ssh_mgrid thisobs thisobs_l twin_exp_ssh_mgrid Subroutines add_noise init_dim_obs_l_ssh_mgrid init_dim_obs_ssh_mgrid obs_op_ssh_mgrid Variables Type Visibility Attributes Name Initial logical, public :: assim_ssh_mgrid = .TRUE. Whether to assimilate this data type character(len=lc), public :: file_ssh_mgrid = 'my_nemo_ssh_file.nc' real(kind=pwp), public :: noise_amp_ssh_mgrid = 1 Standard deviation for Gaussian noise in twin experiment real(kind=pwp), public :: rms_ssh_mgrid = 1 Observation error standard deviation (for constant errors) type(obs_f), public, TARGET :: thisobs Instance of full observation data type - see PDAFomi for details. type(obs_l), public, TARGET :: thisobs_l Instance of local observation data type - see PDAFomi for details. logical, public :: twin_exp_ssh_mgrid = .FALSE. Whether to perform an identical twin experiment Subroutines public subroutine add_noise (dim_obs_p, obs) Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: dim_obs_p Number of process-local observations real, intent(inout) :: obs (dim_obs_p) Process-local observations public subroutine init_dim_obs_l_ssh_mgrid (domain_p, step, dim_obs, dim_obs_l) The routine is called during the loop over all local\nanalysis domains. It has to initialize the information\nabout local ssh observations. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: domain_p Index of current local analysis domain integer, intent(in) :: step Current time step integer, intent(in) :: dim_obs Full dimension of observation vector integer, intent(out) :: dim_obs_l Local dimension of observation vector public subroutine init_dim_obs_ssh_mgrid (step, dim_obs) The routine is called by each filter process.\n at the beginning of the analysis step before\n the loop through all local analysis domains. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: step Current time step integer, intent(inout) :: dim_obs Dimension of full observation vector public subroutine obs_op_ssh_mgrid (dim_p, dim_obs, state_p, ostate) This routine applies the full observation operator\nfor the ssh observations. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: dim_p PE-local state dimension integer, intent(in) :: dim_obs Dimension of full observed state (all observed fields) real(kind=pwp), intent(in) :: state_p (dim_p) PE-local model state real(kind=pwp), intent(inout) :: ostate (dim_obs) Full observed state","tags":"","loc":"module/mod_obs_ssh_mgrid_pdafomi.html"},{"title":"mod_assimilation_pdaf – NEMO4 PDAFOMI","text":"Assimilation Parameters This module provides variables needed for the\nassimilation. See mod_init_pdaf for where many of these\nvariables are initialised. Uses mod_kind_pdaf Contents Variables coords_l coords_obs_f covartype delt_obs dim_bias dim_ens dim_lag dim_obs dim_obs_p dim_state dim_state_p distance_l epsilon filtertype forget incremental indx_dom_l int_rediag istate_s istate_ssh istate_t istate_u istate_v local_range locweight model_err_amp model_error obs_f obs_index_l obs_index_p obs_p rank_analysis_enkf salfixmin screen srange subtype time type_forget type_sqrt type_trans Subroutines assimilate_pdaf Variables Type Visibility Attributes Name Initial real(kind=pwp), public :: coords_l (2) Coordinates of local analysis domain real(kind=pwp), public, ALLOCATABLE :: coords_obs_f (:,:) Array for full observation coordinates integer, public :: covartype For SEIK: Definition of ensemble covar matrix\n (0): Factor (r+1)&#94;-1 (or N&#94;-1)\n (1): Factor r&#94;-1 (or (N-1)&#94;-1) - real ensemble covar.\n This setting is only for the model part; The definition\n of P has also to be specified in PDAF_filter_init.\n Only for upward-compatibility of PDAF integer, public :: delt_obs time step interval between assimilation steps integer, public :: dim_bias dimension of bias vector integer, public :: dim_ens Size of ensemble for SEIK/LSEIK/EnKF/ETKF\n Number of EOFs to be used for SEEK integer, public :: dim_lag Number of time instances for smoother integer, public :: dim_obs Number of observations integer, public :: dim_obs_p Process-local number of observations integer, public :: dim_state Global model state dimension integer, public :: dim_state_p Model state dimension for PE-local domain real(kind=pwp), public, ALLOCATABLE :: distance_l (:) Vector holding distances of local observations real(kind=pwp), public :: epsilon Epsilon for gradient approx. in SEEK forecast integer, public :: filtertype Select filter algorithm:\n SEEK (0), SEIK (1), EnKF (2), LSEIK (3), ETKF (4)\n LETKF (5), ESTKF (6), LESTKF (7), NETF (9), LNETF (10) real(kind=pwp), public :: forget Forgetting factor for filter analysis integer, public :: incremental Perform incremental updating in LSEIK real(kind=pwp), public, ALLOCATABLE :: indx_dom_l (:,:) Indices of local analysis domain integer, public :: int_rediag Interval to perform re-diagonalization in SEEK character(len=lc), public :: istate_s file for t initial state estimate character(len=lc), public :: istate_ssh file for ssh initial state estimate character(len=lc), public :: istate_t file for t initial state estimate character(len=lc), public :: istate_u file for u initial state estimate character(len=lc), public :: istate_v file for v initial state estimate real(kind=pwp), public :: local_range Range for local observation domain - NEMO grid integer, public :: locweight Type of localizing weighting of observations\n   (0) constant weight of 1\n   (1) exponentially decreasing with SRANGE\n   (2) use 5th-order polynomial\n   (3) regulated localization of R with mean error variance\n   (4) regulated localization of R with single-point error variance real(kind=pwp), public :: model_err_amp Amplitude for model error logical, public :: model_error Control application of model error real(kind=pwp), public, ALLOCATABLE :: obs_f (:) Vector holding full vector of observations integer, public, ALLOCATABLE :: obs_index_l (:) Vector holding local state-vector indices of observations integer, public, ALLOCATABLE :: obs_index_p (:) Vector holding state-vector indices of observations real(kind=pwp), public, ALLOCATABLE :: obs_p (:) Vector holding process-local observations integer, public :: rank_analysis_enkf Rank to be considered for inversion of HPH real(kind=pwp), public :: salfixmin Ensure that the salinity is larger than this value integer, public :: screen Control verbosity of PDAF\n (0) no outputs, (1) progess info, (2) add timings\n (3) debugging output real(kind=pwp), public :: srange Support range for 5th order polynomial - NEMO grid\n   or radius for 1/e for exponential weighting\n    ! SEIK-subtype4/LSEIK-subtype4/ESTKF/LESTKF integer, public :: subtype Subtype of filter algorithm\n   SEEK:\n     (0) evolve normalized modes\n     (1) evolve scaled modes with unit U\n     (2) fixed basis (V); variable U matrix\n     (3) fixed covar matrix (V,U kept static)\n   SEIK:\n     (0) ensemble forecast; new formulation\n     (1) ensemble forecast; old formulation\n     (2) fixed error space basis\n     (3) fixed state covariance matrix\n     (4) SEIK with ensemble transformation\n   EnKF:\n     (0) analysis for large observation dimension\n     (1) analysis for small observation dimension\n   LSEIK:\n     (0) ensemble forecast;\n     (2) fixed error space basis\n     (3) fixed state covariance matrix\n     (4) LSEIK with ensemble transformation\n   ETKF:\n     (0) ETKF using T-matrix like SEIK\n     (1) ETKF following Hunt et al. (2007)\n       There are no fixed basis/covariance cases, as\n       these are equivalent to SEIK subtypes 2/3\n   LETKF:\n     (0) LETKF using T-matrix like SEIK\n     (1) LETKF following Hunt et al. (2007)\n       There are no fixed basis/covariance cases, as\n       these are equivalent to LSEIK subtypes 2/3\n   ESTKF:\n     (0) standard ESTKF\n       There are no fixed basis/covariance cases, as\n       these are equivalent to SEIK subtypes 2/3\n   LESTKF:\n     (0) standard LESTKF\n       There are no fixed basis/covariance cases, as\n       these are equivalent to LSEIK subtypes 2/3\n   NETF:\n     (0) standard NETF\n   LNETF:\n     (0) standard LNETF real(kind=pwp), public :: time model time integer, public :: type_forget Type of forgetting factor integer, public :: type_sqrt Type of the transform matrix square-root\n (0) symmetric square root, (1) Cholesky decomposition integer, public :: type_trans Type of ensemble transformation\n SEIK/LSEIK:\n (0) use deterministic omega\n (1) use random orthonormal omega orthogonal to (1,…,1)&#94;T\n (2) use product of (0) with random orthonormal matrix with\n     eigenvector (1,…,1)&#94;T\n ETKF/LETKF with subtype=4:\n (0) use deterministic symmetric transformation\n (2) use product of (0) with random orthonormal matrix with\n     eigenvector (1,…,1)&#94;T\n ESTKF/LESTKF:\n (0) use deterministic omega\n (1) use random orthonormal omega orthogonal to (1,…,1)&#94;T\n (2) use product of (0) with random orthonormal matrix with\n     eigenvector (1,…,1)&#94;T\n NETF/LNETF:\n (0) use random orthonormal transformation orthogonal to (1,…,1)&#94;T\n (1) use identity transformation\n    ! LSEIK/LETKF/LESTKF Subroutines public subroutine assimilate_pdaf () This routine is called during the model integrations at each timestep.\nIt calls PDAF to check whether the forecast phase is completed and if\nso, PDAF will perform the analysis step. Read more… Arguments None","tags":"","loc":"module/mod_assimilation_pdaf.html"},{"title":"mod_iau_pdaf – NEMO4 PDAFOMI","text":"Using the Incremental Analysis Method The material in this module is heavily based on a similar\nimplementation for the NEMOVAR data assimilation system. Please\nrefer to the ASM subdirectory in the OCE source code for\nprecise details. Uses mod_kind_pdaf par_oce Contents Variables s_iau_pdaf ssh_iau_pdaf t_iau_pdaf u_iau_pdaf v_iau_pdaf Subroutines asm_inc_init_pdaf dyn_asm_inc_pdaf ssh_asm_div_pdaf tra_asm_inc_pdaf Variables Type Visibility Attributes Name Initial real(kind=pwp), public, DIMENSION(:, :, :), ALLOCATABLE :: s_iau_pdaf Array to store the T, S IAU real(kind=pwp), public, DIMENSION(:, :), ALLOCATABLE :: ssh_iau_pdaf Array to store the ssh IAU real(kind=pwp), public, DIMENSION(:, :, :), ALLOCATABLE :: t_iau_pdaf Array to store the T, S IAU real(kind=pwp), public, DIMENSION(:, :, :), ALLOCATABLE :: u_iau_pdaf Array to store the U, V IAU real(kind=pwp), public, DIMENSION(:, :, :), ALLOCATABLE :: v_iau_pdaf Array to store the U, V IAU Subroutines public subroutine asm_inc_init_pdaf () This routine initialises the arrays for the IAU. The arrays\n* must be initialised to zero, as PDAF does not compute an\nanalysis update for all gridpoints. (In this case, no increment\nshould be added to the model at this gridpoint.) Read more… Arguments None public subroutine dyn_asm_inc_pdaf (kt) This routine is almost identical to a similar routine\nfrom NEMOVAR. It applies the IAU to the dynamical fields. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: kt Current time step public subroutine ssh_asm_div_pdaf (kt, phdivn) This routine is almost identical to a similar routine\nfrom NEMOVAR. It applies the IAU to the ssh divergence term. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: kt Current time step real(kind=pwp), intent(inout), DIMENSION(:, :, :) :: phdivn Horizontal divergence public subroutine tra_asm_inc_pdaf (kt) This routine is almost identical to a similar routine\nfrom NEMOVAR. It applies the IAU to the tracer fields. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: kt Current time step","tags":"","loc":"module/mod_iau_pdaf.html"},{"title":"mod_statevector_pdaf – NEMO4 PDAFOMI","text":"Building the Statevector This module provides variables & routines for\nbuilding the state vector. Uses mod_kind_pdaf Contents Variables mpi_subd_lat mpi_subd_lon mpi_subd_vert s_p_dim s_p_offset ssh_p_dim ssh_p_offset t_p_dim t_p_offset u_p_dim u_p_offset v_p_dim v_p_offset var2d_p_offset var3d_p_offset Subroutines calc_mpi_dim calc_offset calc_statevar_dim calc_statevector_dim fill2d_ensarray fill3d_ensarray Variables Type Visibility Attributes Name Initial integer, public :: mpi_subd_lat Dimensions for MPI subdomain that is included\n in local statevector. Necessary so that halo\n regions are not included in multiple local\n statevectors\n size of local lat domain excluding halo region integer, public :: mpi_subd_lon size of local lon domain excluding halo region integer, public :: mpi_subd_vert size of local vertical domain integer, public :: s_p_dim 3d statevector variables (s) - dimension size integer, public :: s_p_offset 3d statevector variable - start index for s integer, public :: ssh_p_dim 2d statevector variables - dimension size integer, public :: ssh_p_offset 2d statevector variables - start index integer, public :: t_p_dim 3d statevector variables (t) - dimension size integer, public :: t_p_offset 3d statevector variables - start index for t integer, public :: u_p_dim 3d statevector variables (u) - dimension size integer, public :: u_p_offset 3d statevector variable - start index for u integer, public :: v_p_dim 3d statevector variables (v) - dimension size integer, public :: v_p_offset 3d statevector variable - start index for v integer, public :: var2d_p_offset (1) Array holding 2d state variable offsets integer, public :: var3d_p_offset (4) Array holding 3d state variable offsets\n index i is the i-th variable Subroutines public subroutine calc_mpi_dim () that is used to fill the local statevector. Read more… Arguments None public subroutine calc_offset () local statevector variables. Read more… Arguments None public subroutine calc_statevar_dim () statevector variables. Read more… Arguments None public subroutine calc_statevector_dim (dim_p) Calling Sequence Read more… Arguments Type Intent Optional Attributes Name integer, intent(inout) :: dim_p Local statevector dimension public subroutine fill2d_ensarray (fname, ens_p) initial state file. Read more… Arguments Type Intent Optional Attributes Name character(len=lc), intent(in) :: fname Name of netCDF file real(kind=pwp), intent(inout) :: ens_p (:,:) PE-local state ensemble public subroutine fill3d_ensarray (fname, statevar, ens_p) initial state file. Read more… Arguments Type Intent Optional Attributes Name character(len=lc), intent(in) :: fname Name of netCDF file character(len=1), intent(in) :: statevar Name of state variable real(kind=pwp), intent(inout) :: ens_p (:,:) PE-local state ensemble","tags":"","loc":"module/mod_statevector_pdaf.html"},{"title":"mod_parallel_pdaf – NEMO4 PDAFOMI","text":"Setup parallelisation This modules provides variables for the MPI parallelization\nto be shared between model-related routines. There are variables\nthat are used in the model even without PDAF, and additional variables\nthat are only used if data assimilaion with PDAF is performed.\nThe initialization of communicators for execution with PDAF is\nperformed in init_parallel_pdaf . Uses mod_kind_pdaf Contents Variables COMM_couple COMM_ensemble COMM_filter COMM_model MPIerr MPIstatus filterpe local_npes_model mype_couple mype_ens mype_filter mype_model n_modeltasks npes_couple npes_ens npes_filter npes_model screen_parallel task_id Subroutines abort_parallel init_parallel_pdaf Variables Type Visibility Attributes Name Initial integer, public :: COMM_couple MPI communicator for coupling filter and model integer, public :: COMM_ensemble Communicator for entire ensemble integer, public :: COMM_filter MPI communicator for filter PEs integer, public :: COMM_model MPI communicator for model tasks integer, public :: MPIerr Error flag for MPI integer, public :: MPIstatus (MPI_STATUS_SIZE) Status array for MPI logical, public :: filterpe Whether we are on a PE in a COMM_filter integer, public, ALLOCATABLE :: local_npes_model (:) # PEs per ensemble integer, public :: mype_couple rank and size in COMM_couple integer, public :: mype_ens Rank and size in COMM_ensemble integer, public :: mype_filter rank and size in COMM_filter integer, public :: mype_model Rank and size in COMM_model integer, public :: n_modeltasks = 1 Number of parallel model tasks integer, public :: npes_couple rank and size in COMM_couple integer, public :: npes_ens Rank and size in COMM_ensemble integer, public :: npes_filter rank and size in COMM_filter integer, public :: npes_model Rank and size in COMM_model integer, public :: screen_parallel = 1 Option for screen output integer, public :: task_id Index of my model task (1,…,n_modeltasks) Subroutines public subroutine abort_parallel () Read more… Arguments None public subroutine init_parallel_pdaf (screen, mpi_comm) FILTER and COUPLE communicators, return MODEL communicator. Read more… Arguments Type Intent Optional Attributes Name integer, intent(in) :: screen Whether screen information is shown integer, intent(inout) :: mpi_comm Communicator after XIOS splitting","tags":"","loc":"module/mod_parallel_pdaf.html"}]}